{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.vits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789a053",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41357cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.util as librosa_util\n",
    "from librosa.util import normalize, pad_center, tiny\n",
    "from scipy.signal import get_window\n",
    "from scipy.io.wavfile import read\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "\n",
    "MAX_WAV_VALUE = 32768.0\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def dynamic_range_decompression_torch(x, C=1):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor used to compress\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / C\n",
    "\n",
    "\n",
    "def spectral_normalize_torch(magnitudes):\n",
    "    output = dynamic_range_compression_torch(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "def spectral_de_normalize_torch(magnitudes):\n",
    "    output = dynamic_range_decompression_torch(magnitudes)\n",
    "    return output\n",
    "\n",
    "\n",
    "mel_basis = {}\n",
    "hann_window = {}\n",
    "\n",
    "\n",
    "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n",
    "    global mel_basis\n",
    "    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n",
    "            dtype=spec.dtype, device=spec.device\n",
    "        )\n",
    "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    spec = spectral_normalize_torch(spec)\n",
    "    return spec\n",
    "\n",
    "\n",
    "def mel_spectrogram_torch(\n",
    "    y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False\n",
    "):\n",
    "    if torch.min(y) < -1.0:\n",
    "        print(\"min value is \", torch.min(y))\n",
    "    if torch.max(y) > 1.0:\n",
    "        print(\"max value is \", torch.max(y))\n",
    "\n",
    "    global mel_basis, hann_window\n",
    "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n",
    "    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n",
    "    if fmax_dtype_device not in mel_basis:\n",
    "        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n",
    "        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "    if wnsize_dtype_device not in hann_window:\n",
    "        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n",
    "            dtype=y.dtype, device=y.device\n",
    "        )\n",
    "\n",
    "    y = torch.nn.functional.pad(\n",
    "        y.unsqueeze(1),\n",
    "        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "    spec = torch.stft(\n",
    "        y,\n",
    "        n_fft,\n",
    "        hop_length=hop_size,\n",
    "        win_length=win_size,\n",
    "        window=hann_window[wnsize_dtype_device],\n",
    "        center=center,\n",
    "        pad_mode=\"reflect\",\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "    )\n",
    "\n",
    "    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "\n",
    "    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n",
    "    spec = spectral_normalize_torch(spec)\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d4f95",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47500540",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Couldn't build proto file into descriptor pool!\nInvalid proto descriptor for file \"tensorboard/compat/proto/tensor_shape.proto\":\n  tensorboard.TensorShapeProto.dim: \"tensorboard.TensorShapeProto.dim\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.unknown_rank: \"tensorboard.TensorShapeProto.unknown_rank\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim.size: \"tensorboard.TensorShapeProto.Dim.size\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim.name: \"tensorboard.TensorShapeProto.Dim.name\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim: \"tensorboard.TensorShapeProto.Dim\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto: \"tensorboard.TensorShapeProto\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.dim: \"tensorboard.TensorShapeProto.Dim\" seems to be defined in \"tensorboardX/src/tensor_shape.proto\", which is not imported by \"tensorboard/compat/proto/tensor_shape.proto\".  To use it here, please add the necessary import.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-84a394e826b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muberduck_ml_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muberduck_ml_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbols_with_ipa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0muberduck_ml_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTTSTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from uberduck_ml_dev.models.vits import (\n",
      "\u001b[0;32m~/code/uberduck-ml-dev/uberduck_ml_dev/trainer/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummaryWriter\u001b[0m  \u001b[0;31m# noqa F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordWriter\u001b[0m  \u001b[0;31m# noqa F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionLog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_pb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/tensorboard/compat/proto/event_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_summary__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/tensorboard/compat/proto/summary_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/tensorboard/compat/proto/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/tensorboard/compat/proto/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorboard_dot_compat_dot_proto_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/tensorboard/compat/proto/tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0msyntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'proto3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\030org.tensorflow.frameworkB\\021TensorShapeProtosP\\001ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\370\\001\\001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mserialized_pb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n+tensorboard/compat/proto/tensor_shape.proto\\x12\\x0btensorboard\\\"{\\n\\x10TensorShapeProto\\x12.\\n\\x03\\x64im\\x18\\x02 \\x03(\\x0b\\x32!.tensorboard.TensorShapeProto.Dim\\x12\\x14\\n\\x0cunknown_rank\\x18\\x03 \\x01(\\x08\\x1a!\\n\\x03\\x44im\\x12\\x0c\\n\\x04size\\x18\\x01 \\x01(\\x03\\x12\\x0c\\n\\x04name\\x18\\x02 \\x01(\\tB\\x87\\x01\\n\\x18org.tensorflow.frameworkB\\x11TensorShapeProtosP\\x01ZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\\xf8\\x01\\x01\\x62\\x06proto3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/uberduck/lib/python3.6/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, package, options, serialized_options, serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)\u001b[0m\n\u001b[1;32m    963\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please link in cpp generated lib for %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mserialized_pb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddSerializedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_pb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFileDescriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Couldn't build proto file into descriptor pool!\nInvalid proto descriptor for file \"tensorboard/compat/proto/tensor_shape.proto\":\n  tensorboard.TensorShapeProto.dim: \"tensorboard.TensorShapeProto.dim\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.unknown_rank: \"tensorboard.TensorShapeProto.unknown_rank\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim.size: \"tensorboard.TensorShapeProto.Dim.size\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim.name: \"tensorboard.TensorShapeProto.Dim.name\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.Dim: \"tensorboard.TensorShapeProto.Dim\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto: \"tensorboard.TensorShapeProto\" is already defined in file \"tensorboardX/src/tensor_shape.proto\".\n  tensorboard.TensorShapeProto.dim: \"tensorboard.TensorShapeProto.Dim\" seems to be defined in \"tensorboardX/src/tensor_shape.proto\", which is not imported by \"tensorboard/compat/proto/tensor_shape.proto\".  To use it here, please add the necessary import.\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.text.symbols import symbols_with_ipa\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "\n",
    "from uberduck_ml_dev.models.vits import (\n",
    "    DEFAULTS,\n",
    "    MultiPeriodDiscriminator,\n",
    "    SynthesizerTrn,\n",
    ")\n",
    "from uberduck_ml_dev.data_loader import (\n",
    "    TextAudioSpeakerLoader,\n",
    "    TextAudioSpeakerCollate,\n",
    "    DistributedBucketSampler,\n",
    ")\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy, plot_spectrogram\n",
    "from uberduck_ml_dev.utils.utils import slice_segments, clip_grad_value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54d4d6",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc21ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def feature_loss(fmap_r, fmap_g):\n",
    "    loss = 0\n",
    "    for dr, dg in zip(fmap_r, fmap_g):\n",
    "        for rl, gl in zip(dr, dg):\n",
    "            rl = rl.float().detach()\n",
    "            gl = gl.float()\n",
    "            loss += torch.mean(torch.abs(rl - gl))\n",
    "\n",
    "    return loss * 2\n",
    "\n",
    "\n",
    "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
    "    loss = 0\n",
    "    r_losses = []\n",
    "    g_losses = []\n",
    "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "        dr = dr.float()\n",
    "        dg = dg.float()\n",
    "        r_loss = torch.mean((1 - dr) ** 2)\n",
    "        g_loss = torch.mean(dg**2)\n",
    "        loss += r_loss + g_loss\n",
    "        r_losses.append(r_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "    return loss, r_losses, g_losses\n",
    "\n",
    "\n",
    "def generator_loss(disc_outputs):\n",
    "    loss = 0\n",
    "    gen_losses = []\n",
    "    for dg in disc_outputs:\n",
    "        dg = dg.float()\n",
    "        l = torch.mean((1 - dg) ** 2)\n",
    "        gen_losses.append(l)\n",
    "        loss += l\n",
    "\n",
    "    return loss, gen_losses\n",
    "\n",
    "\n",
    "def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n",
    "    \"\"\"\n",
    "    z_p, logs_q: [b, h, t_t]\n",
    "    m_p, logs_p: [b, h, t_t]\n",
    "    \"\"\"\n",
    "    z_p = z_p.float()\n",
    "    logs_q = logs_q.float()\n",
    "    m_p = m_p.float()\n",
    "    logs_p = logs_p.float()\n",
    "    z_mask = z_mask.float()\n",
    "\n",
    "    kl = logs_p - logs_q - 0.5\n",
    "    kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n",
    "    kl = torch.sum(kl * z_mask)\n",
    "    l = kl / torch.sum(z_mask)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34fa71",
   "metadata": {},
   "source": [
    "# VITS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e61d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class VITSTrainer(TTSTrainer):\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"betas\",\n",
    "        \"c_kl\",\n",
    "        \"c_mel\",\n",
    "        \"eps\",\n",
    "        \"lr_decay\",\n",
    "        \"segment_size\",\n",
    "        \"training_audiopaths_and_text\",\n",
    "        \"val_audiopaths_and_text\",\n",
    "        \"warm_start_name_g\",\n",
    "        \"warm_start_name_d\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.log_interval = 10\n",
    "        for param in self.REQUIRED_HPARAMS:\n",
    "            if not hasattr(self, param):\n",
    "                raise Exception(f\"VITSTrainer missing a required param: {param}\")\n",
    "        self.mel_stft = MelSTFT(\n",
    "            device=self.device,\n",
    "            rank=self.rank,\n",
    "            padding=(self.filter_length - self.hop_length) // 2,\n",
    "        )\n",
    "\n",
    "    def init_distributed(self):\n",
    "        if not self.distributed_run:\n",
    "            return\n",
    "        if self.rank is None or self.world_size is None:\n",
    "            raise Exception(\n",
    "                \"Rank and wrld size must be provided when distributed training\"\n",
    "            )\n",
    "        dist.init_process_group(\n",
    "            \"nccl\",\n",
    "            init_method=\"tcp://localhost:54321\",\n",
    "            rank=self.rank,\n",
    "            world_size=self.world_size,\n",
    "        )\n",
    "        torch.cuda.set_device(self.rank)\n",
    "\n",
    "    def _log_training(self, scalars, images):\n",
    "        print(\"log training placeholder...\")\n",
    "        if self.rank != 0 or self.global_step % self.log_interval != 0:\n",
    "            return\n",
    "        for k, v in scalars.items():\n",
    "            pieces = k.split(\"_\")\n",
    "            key = \"/\".join(pieces)\n",
    "            self.log(key, self.global_step, scalar=v)\n",
    "        for k, v in images.items():\n",
    "            pieces = k.split(\"_\")\n",
    "            key = \"/\".join(pieces)\n",
    "            self.log(key, self.global_step, image=v)\n",
    "\n",
    "    def _log_validation(self):\n",
    "        print(\"log validation...\")\n",
    "        pass\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_name, model, optimizer, learning_rate, epoch):\n",
    "        if self.rank != 0:\n",
    "            return\n",
    "        if hasattr(model, \"module\"):\n",
    "            state_dict = model.module.state_dict()\n",
    "        else:\n",
    "            state_dict = model.state_dict()\n",
    "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": state_dict,\n",
    "                \"global_step\": self.global_step,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            os.path.join(self.checkpoint_path, f\"{checkpoint_name}.pt\"),\n",
    "        )\n",
    "\n",
    "    def warm_start(self, net_g, net_d, optim_g, optim_d):\n",
    "        if not (self.warm_start_name_g and self.warm_start_name_d):\n",
    "            return net_g, net_d, optim_g, optim_d, 0\n",
    "        if self.warm_start_name_g:\n",
    "            checkpoint = torch.load(self.warm_start_name_g)\n",
    "            net_g.load_state_dict(checkpoint[\"model\"])\n",
    "            optim_g.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        if self.warm_start_name_d:\n",
    "            checkpoint = torch.load(self.warm_start_name_d)\n",
    "            net_d.load_state_dict(checkpoint[\"model\"])\n",
    "            optim_d.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        self.global_step = checkpoint[\"global_step\"]\n",
    "        self.learning_rate = checkpoint[\"learning_rate\"]\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        return net_g, net_d, optim_g, optim_d, start_epoch\n",
    "\n",
    "    def _batch_to_device(self, *args):\n",
    "        ret = []\n",
    "        if self.device == \"cuda\":\n",
    "            for arg in args:\n",
    "                arg = arg.cuda(self.rank, non_blocking=True)\n",
    "                ret.append(arg)\n",
    "            return ret\n",
    "        else:\n",
    "            return args\n",
    "\n",
    "    def _evaluate(self, generator, val_loader):\n",
    "        print(\"Validation ...\")\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                (\n",
    "                    x,\n",
    "                    x_lengths,\n",
    "                    spec,\n",
    "                    spec_lengths,\n",
    "                    y,\n",
    "                    y_lengths,\n",
    "                    speakers,\n",
    "                ) = self._batch_to_device(*batch)\n",
    "                x = x[:1]\n",
    "                x_lengths = x_lengths[:1]\n",
    "                spec = spec[:1]\n",
    "                spec_lengths[:1]\n",
    "                y = y[:1]\n",
    "                y_lengths = y_lengths[:1]\n",
    "                speakers = speakers[:1]\n",
    "                break\n",
    "            if self.distributed_run:\n",
    "                y_hat, attn, mask, *_ = generator.module.infer(\n",
    "                    x, x_lengths, speakers, max_len=1000\n",
    "                )\n",
    "            else:\n",
    "                y_hat, attn, mask, *_ = generator.infer(\n",
    "                    x, x_lengths, speakers, max_len=1000\n",
    "                )\n",
    "            y_hat_lengths = mask.sum([1, 2]).long() * self.hparams.hop_length\n",
    "            mel = self.mel_stft.spec_to_mel(spec)\n",
    "            y_hat_mel = self.mel_stft.mel_spectrogram(y_hat.squeeze(1).float())\n",
    "        self.log(\n",
    "            \"Val/mel_gen\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(plot_spectrogram(y_hat_mel[0].data.cpu())),\n",
    "        )\n",
    "        self.log(\n",
    "            \"Val/mel_gt\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(plot_spectrogram(mel[0].data.cpu())),\n",
    "        )\n",
    "        self.log(\n",
    "            \"Val/audio_gen\", self.global_step, audio=y_hat[0, :, : y_hat_lengths[0]]\n",
    "        )\n",
    "        self.log(\"Val/audio_gt\", self.global_step, audio=y[0, :, : y_lengths[0]])\n",
    "        generator.train()\n",
    "\n",
    "    def _train_and_evaluate(\n",
    "        self, epoch, nets, optims, schedulers, scaler: GradScaler, loaders\n",
    "    ):\n",
    "        net_g, net_d = nets\n",
    "        optim_g, optim_d = optims\n",
    "        scheduler_g, scheduler_d = schedulers\n",
    "        train_loader, val_loader = loaders\n",
    "        train_loader.batch_sampler.set_epoch(epoch)\n",
    "        net_g.train()\n",
    "        net_d.train()\n",
    "        # TODO (zach): remove when you want to.\n",
    "        # self._evaluate(net_g, val_loader)\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            print(f\"global step: {self.global_step}\")\n",
    "            print(f\"batch idx: {batch_idx}\")\n",
    "            (\n",
    "                x,\n",
    "                x_lengths,\n",
    "                spec,\n",
    "                spec_lengths,\n",
    "                y,\n",
    "                y_lengths,\n",
    "                speakers,\n",
    "            ) = self._batch_to_device(*batch)\n",
    "\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                (\n",
    "                    y_hat,\n",
    "                    l_length,\n",
    "                    attn,\n",
    "                    ids_slice,\n",
    "                    x_mask,\n",
    "                    z_mask,\n",
    "                    (z, z_p, m_p, logs_p, m_q, logs_q),\n",
    "                ) = net_g(x, x_lengths, spec, spec_lengths, speakers)\n",
    "                mel = self.mel_stft.spec_to_mel(spec)\n",
    "                # NOTE(zach): slight difference from the original VITS\n",
    "                # implementation due to padding differences in the spectrograms\n",
    "                y_mel = slice_segments(\n",
    "                    mel, ids_slice, self.segment_size // self.hop_length\n",
    "                )\n",
    "                y_hat_mel = self.mel_stft.mel_spectrogram(y_hat.squeeze(1))\n",
    "                y = slice_segments(y, ids_slice * self.hop_length, self.segment_size)\n",
    "\n",
    "                # Discriminator\n",
    "                y_d_hat_r, y_d_hat_g, _, _ = net_d(y, y_hat.detach())\n",
    "                with autocast(enabled=False):\n",
    "                    loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(\n",
    "                        y_d_hat_r, y_d_hat_g\n",
    "                    )\n",
    "                    loss_disc_all = loss_disc\n",
    "            optim_d.zero_grad()\n",
    "            scaler.scale(loss_disc_all).backward()\n",
    "            scaler.unscale_(optim_d)\n",
    "            scaler.step(optim_d)\n",
    "\n",
    "            with autocast(enabled=self.fp16_run):\n",
    "                # Generator\n",
    "                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(y, y_hat)\n",
    "                with autocast(enabled=False):\n",
    "                    loss_dur = torch.sum(l_length.float())\n",
    "                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * self.c_mel\n",
    "                    loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * self.c_kl\n",
    "\n",
    "                    loss_fm = feature_loss(fmap_r, fmap_g)\n",
    "                    loss_gen, losses_gen = generator_loss(y_d_hat_g)\n",
    "                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl\n",
    "            optim_g.zero_grad()\n",
    "            scaler.scale(loss_gen_all).backward()\n",
    "            scaler.unscale_(optim_g)\n",
    "            scaler.step(optim_g)\n",
    "            scaler.update()\n",
    "\n",
    "            if self.rank == 0 and self.global_step % self.log_interval == 0:\n",
    "                grad_norm_g = clip_grad_value_(net_g.parameters(), None)\n",
    "                grad_norm_d = clip_grad_value_(net_d.parameters(), None)\n",
    "                self._log_training(\n",
    "                    scalars=dict(\n",
    "                        loss_g_total=loss_gen_all,\n",
    "                        loss_d_total=loss_disc_all,\n",
    "                        gradnorm_d=grad_norm_d,\n",
    "                        gradnorm_g=grad_norm_g,\n",
    "                        loss_g_fm=loss_fm,\n",
    "                        loss_g_dur=loss_dur,\n",
    "                        loss_g_mel=loss_mel,\n",
    "                        loss_g_kl=loss_kl,\n",
    "                    ),\n",
    "                    images=dict(\n",
    "                        slice_mel_org=save_figure_to_numpy(\n",
    "                            plot_spectrogram(y_mel[0].data.cpu())\n",
    "                        ),\n",
    "                        slice_mel_gen=save_figure_to_numpy(\n",
    "                            plot_spectrogram(y_hat_mel[0].data.cpu())\n",
    "                        ),\n",
    "                        all_mel=save_figure_to_numpy(\n",
    "                            plot_spectrogram(mel[0].data.cpu())\n",
    "                        ),\n",
    "                        all_attn=save_figure_to_numpy(\n",
    "                            plot_attention(attn[0, 0].data.cpu())\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            self.global_step += 1\n",
    "        if self.rank == 0:\n",
    "            self._evaluate(net_g, val_loader)\n",
    "\n",
    "    def train(self):\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "        train_dataset = TextAudioSpeakerLoader(\n",
    "            self.training_audiopaths_and_text,\n",
    "            self.hparams,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.debug_dataset_size,\n",
    "        )\n",
    "        train_sampler = DistributedBucketSampler(\n",
    "            train_dataset,\n",
    "            self.batch_size,\n",
    "            [32, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        collate_fn = TextAudioSpeakerCollate()\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=collate_fn,\n",
    "            batch_sampler=train_sampler,\n",
    "        )\n",
    "        val_dataset, val_loader = None, None\n",
    "        if self.rank == 0:\n",
    "            val_dataset = TextAudioSpeakerLoader(\n",
    "                self.val_audiopaths_and_text,\n",
    "                self.hparams,\n",
    "                debug=self.debug,\n",
    "                debug_dataset_size=self.debug_dataset_size,\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                num_workers=0,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                pin_memory=True,\n",
    "                drop_last=False,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "\n",
    "        model_kwargs = {k: v for k, v in DEFAULTS.values().items() if hasattr(self, k)}\n",
    "        net_g = SynthesizerTrn(\n",
    "            len(symbols_with_ipa),\n",
    "            self.filter_length // 2 + 1,\n",
    "            self.segment_size // self.hop_length,\n",
    "            n_speakers=self.n_speakers,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        net_d = MultiPeriodDiscriminator(self.use_spectral_norm)\n",
    "        if self.device == \"cuda\":\n",
    "            net_g = net_g.cuda(self.rank)\n",
    "            net_d = net_d.cuda(self.rank)\n",
    "\n",
    "        optim_g = torch.optim.AdamW(\n",
    "            net_g.parameters(),\n",
    "            self.learning_rate,\n",
    "            betas=self.betas,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "        optim_d = torch.optim.AdamW(\n",
    "            net_d.parameters(), self.learning_rate, betas=self.betas, eps=self.eps\n",
    "        )\n",
    "        start_epoch = 0\n",
    "        net_g, net_d, optim_g, optim_d, start_epoch = self.warm_start(\n",
    "            net_g,\n",
    "            net_d,\n",
    "            optim_g,\n",
    "            optim_d,\n",
    "        )\n",
    "\n",
    "        if self.distributed_run:\n",
    "            net_g = DDP(net_g, device_ids=[self.rank])\n",
    "            net_d = DDP(net_d, device_ids=[self.rank])\n",
    "\n",
    "        scheduler_g = ExponentialLR(\n",
    "            optim_g, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scheduler_d = ExponentialLR(\n",
    "            optim_d, gamma=self.lr_decay, last_epoch=start_epoch - 1\n",
    "        )\n",
    "        scaler = GradScaler(enabled=self.fp16_run)\n",
    "\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            self._train_and_evaluate(\n",
    "                epoch,\n",
    "                [net_g, net_d],\n",
    "                [optim_g, optim_d],\n",
    "                [scheduler_g, scheduler_d],\n",
    "                scaler,\n",
    "                [train_loader, val_loader],\n",
    "            )\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(\n",
    "                    f\"{self.checkpoint_name}_G_{self.global_step}\",\n",
    "                    net_g,\n",
    "                    optim_g,\n",
    "                    self.learning_rate,\n",
    "                    epoch,\n",
    "                )\n",
    "                self.save_checkpoint(\n",
    "                    f\"{self.checkpoint_name}_D_{self.global_step}\",\n",
    "                    net_d,\n",
    "                    optim_d,\n",
    "                    self.learning_rate,\n",
    "                    epoch,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d5dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
