{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0499d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.torchmoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d06b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import uuid\n",
    "import codecs\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, OrderedDict\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import pickle\n",
    "from time import sleep\n",
    "import unicodedata\n",
    "from text_unidecode import unidecode\n",
    "import string\n",
    "import sys\n",
    "from os.path import abspath, dirname, exists\n",
    "\n",
    "from io import open\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21411e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/global_variables.py\n",
    "\n",
    "\"\"\" Global variables.\n",
    "\"\"\"\n",
    "\n",
    "# The ordering of these special tokens matter\n",
    "# blank tokens can be used for new purposes\n",
    "# Tokenizer should be updated if special token prefix is changed\n",
    "SPECIAL_PREFIX = \"CUSTOM_\"\n",
    "SPECIAL_TOKENS = [\n",
    "    \"CUSTOM_MASK\",\n",
    "    \"CUSTOM_UNKNOWN\",\n",
    "    \"CUSTOM_AT\",\n",
    "    \"CUSTOM_URL\",\n",
    "    \"CUSTOM_NUMBER\",\n",
    "    \"CUSTOM_BREAK\",\n",
    "]\n",
    "SPECIAL_TOKENS.extend([f\"{SPECIAL_PREFIX}BLANK_{i}\" for i in range(6, 10)])\n",
    "\n",
    "NB_TOKENS = 50000\n",
    "NB_EMOJI_CLASSES = 64\n",
    "FINETUNING_METHODS = [\"last\", \"full\", \"new\", \"chain-thaw\"]\n",
    "FINETUNING_METRICS = [\"acc\", \"weighted\"]\n",
    "\n",
    "# Emoji map in emoji_overview.png\n",
    "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: \\\n",
    ":pensive: :ok_hand: :blush: :heart: :smirk: \\\n",
    ":grin: :notes: :flushed: :100: :sleeping: \\\n",
    ":relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n",
    ":sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n",
    ":neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n",
    ":v: :sunglasses: :rage: :thumbsup: :cry: \\\n",
    ":sleepy: :yum: :triumph: :hand: :mask: \\\n",
    ":clap: :eyes: :gun: :persevere: :smiling_imp: \\\n",
    ":sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n",
    ":wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n",
    ":angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n",
    ":sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(\n",
    "    \" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/lstm.py\n",
    "\n",
    "\"\"\" Implement a pyTorch LSTM with hard sigmoid reccurent activation functions.\n",
    "    Adapted from the non-cuda variant of pyTorch LSTM at\n",
    "    https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/rnn.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LSTMHardSigmoid(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        bias=True,\n",
    "        batch_first=False,\n",
    "        dropout=0,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        super(LSTMHardSigmoid, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.dropout_state = {}\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        gate_size = 4 * hidden_size\n",
    "\n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            for direction in range(num_directions):\n",
    "                layer_input_size = (\n",
    "                    input_size if layer == 0 else hidden_size * num_directions\n",
    "                )\n",
    "\n",
    "                w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "                w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "                b_ih = Parameter(torch.Tensor(gate_size))\n",
    "                b_hh = Parameter(torch.Tensor(gate_size))\n",
    "                layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "\n",
    "                suffix = \"_reverse\" if direction == 1 else \"\"\n",
    "                param_names = [\"weight_ih_l{}{}\", \"weight_hh_l{}{}\"]\n",
    "                if bias:\n",
    "                    param_names += [\"bias_ih_l{}{}\", \"bias_hh_l{}{}\"]\n",
    "                param_names = [x.format(layer, suffix) for x in param_names]\n",
    "\n",
    "                for name, param in zip(param_names, layer_params):\n",
    "                    setattr(self, name, param)\n",
    "                self._all_weights.append(param_names)\n",
    "\n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"Resets parameter data pointer so that they can use faster code paths.\n",
    "\n",
    "        Right now, this is a no-op wince we don't use CUDA acceleration.\n",
    "        \"\"\"\n",
    "        self._data_ptrs = []\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(LSTMHardSigmoid, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, hx=None):\n",
    "        is_packed = isinstance(input, PackedSequence)\n",
    "        if is_packed:\n",
    "            input, batch_sizes, _, _ = input\n",
    "            max_batch_size = batch_sizes[0]\n",
    "        else:\n",
    "            batch_sizes = None\n",
    "            max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        if hx is None:\n",
    "            num_directions = 2 if self.bidirectional else 1\n",
    "            hx = torch.autograd.Variable(\n",
    "                input.data.new(\n",
    "                    self.num_layers * num_directions, max_batch_size, self.hidden_size\n",
    "                ).zero_(),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "            hx = (hx, hx)\n",
    "\n",
    "        has_flat_weights = (\n",
    "            list(p.data.data_ptr() for p in self.parameters()) == self._data_ptrs\n",
    "        )\n",
    "        if has_flat_weights:\n",
    "            first_data = next(self.parameters()).data\n",
    "            assert first_data.storage().size() == self._param_buf_size\n",
    "            flat_weight = first_data.new().set_(\n",
    "                first_data.storage(), 0, torch.Size([self._param_buf_size])\n",
    "            )\n",
    "        else:\n",
    "            flat_weight = None\n",
    "        func = AutogradRNN(\n",
    "            self.input_size,\n",
    "            self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=self.batch_first,\n",
    "            dropout=self.dropout,\n",
    "            train=self.training,\n",
    "            bidirectional=self.bidirectional,\n",
    "            batch_sizes=batch_sizes,\n",
    "            dropout_state=self.dropout_state,\n",
    "            flat_weight=flat_weight,\n",
    "        )\n",
    "        output, hidden = func(input, self.all_weights, hx)\n",
    "        if is_packed:\n",
    "            output = PackedSequence(output, batch_sizes)\n",
    "        return output, hidden\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"{name}({input_size}, {hidden_size}\"\n",
    "        if self.num_layers != 1:\n",
    "            s += \", num_layers={num_layers}\"\n",
    "        if self.bias is not True:\n",
    "            s += \", bias={bias}\"\n",
    "        if self.batch_first is not False:\n",
    "            s += \", batch_first={batch_first}\"\n",
    "        if self.dropout != 0:\n",
    "            s += \", dropout={dropout}\"\n",
    "        if self.bidirectional is not False:\n",
    "            s += \", bidirectional={bidirectional}\"\n",
    "        s += \")\"\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        super(LSTMHardSigmoid, self).__setstate__(d)\n",
    "        self.__dict__.setdefault(\"_data_ptrs\", [])\n",
    "        if \"all_weights\" in d:\n",
    "            self._all_weights = d[\"all_weights\"]\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            for direction in range(num_directions):\n",
    "                suffix = \"_reverse\" if direction == 1 else \"\"\n",
    "                weights = [\n",
    "                    \"weight_ih_l{}{}\",\n",
    "                    \"weight_hh_l{}{}\",\n",
    "                    \"bias_ih_l{}{}\",\n",
    "                    \"bias_hh_l{}{}\",\n",
    "                ]\n",
    "                weights = [x.format(layer, suffix) for x in weights]\n",
    "                if self.bias:\n",
    "                    self._all_weights += [weights]\n",
    "                else:\n",
    "                    self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [\n",
    "            [getattr(self, weight) for weight in weights]\n",
    "            for weights in self._all_weights\n",
    "        ]\n",
    "\n",
    "\n",
    "def AutogradRNN(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    num_layers=1,\n",
    "    batch_first=False,\n",
    "    dropout=0,\n",
    "    train=True,\n",
    "    bidirectional=False,\n",
    "    batch_sizes=None,\n",
    "    dropout_state=None,\n",
    "    flat_weight=None,\n",
    "):\n",
    "\n",
    "    cell = LSTMCell\n",
    "\n",
    "    if batch_sizes is None:\n",
    "        rec_factory = Recurrent\n",
    "    else:\n",
    "        rec_factory = variable_recurrent_factory(batch_sizes)\n",
    "\n",
    "    if bidirectional:\n",
    "        layer = (rec_factory(cell), rec_factory(cell, reverse=True))\n",
    "    else:\n",
    "        layer = (rec_factory(cell),)\n",
    "\n",
    "    func = StackedRNN(layer, num_layers, True, dropout=dropout, train=train)\n",
    "\n",
    "    def forward(input, weight, hidden):\n",
    "        if batch_first and batch_sizes is None:\n",
    "            input = input.transpose(0, 1)\n",
    "\n",
    "        nexth, output = func(input, hidden, weight)\n",
    "\n",
    "        if batch_first and batch_sizes is None:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        return output, nexth\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def Recurrent(inner, reverse=False):\n",
    "    def forward(input, hidden, weight):\n",
    "        output = []\n",
    "        steps = range(input.size(0) - 1, -1, -1) if reverse else range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = inner(input[i], hidden, *weight)\n",
    "            # hack to handle LSTM\n",
    "            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n",
    "\n",
    "        if reverse:\n",
    "            output.reverse()\n",
    "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
    "\n",
    "        return hidden, output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def variable_recurrent_factory(batch_sizes):\n",
    "    def fac(inner, reverse=False):\n",
    "        if reverse:\n",
    "            return VariableRecurrentReverse(batch_sizes, inner)\n",
    "        else:\n",
    "            return VariableRecurrent(batch_sizes, inner)\n",
    "\n",
    "    return fac\n",
    "\n",
    "\n",
    "def VariableRecurrent(batch_sizes, inner):\n",
    "    def forward(input, hidden, weight):\n",
    "        output = []\n",
    "        input_offset = 0\n",
    "        last_batch_size = batch_sizes[0]\n",
    "        hiddens = []\n",
    "        flat_hidden = not isinstance(hidden, tuple)\n",
    "        if flat_hidden:\n",
    "            hidden = (hidden,)\n",
    "        for batch_size in batch_sizes:\n",
    "            step_input = input[input_offset : input_offset + batch_size]\n",
    "            input_offset += batch_size\n",
    "\n",
    "            dec = last_batch_size - batch_size\n",
    "            if dec > 0:\n",
    "                hiddens.append(tuple(h[-dec:] for h in hidden))\n",
    "                hidden = tuple(h[:-dec] for h in hidden)\n",
    "            last_batch_size = batch_size\n",
    "\n",
    "            if flat_hidden:\n",
    "                hidden = (inner(step_input, hidden[0], *weight),)\n",
    "            else:\n",
    "                hidden = inner(step_input, hidden, *weight)\n",
    "\n",
    "            output.append(hidden[0])\n",
    "        hiddens.append(hidden)\n",
    "        hiddens.reverse()\n",
    "\n",
    "        hidden = tuple(torch.cat(h, 0) for h in zip(*hiddens))\n",
    "        assert hidden[0].size(0) == batch_sizes[0]\n",
    "        if flat_hidden:\n",
    "            hidden = hidden[0]\n",
    "        output = torch.cat(output, 0)\n",
    "\n",
    "        return hidden, output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def VariableRecurrentReverse(batch_sizes, inner):\n",
    "    def forward(input, hidden, weight):\n",
    "        output = []\n",
    "        input_offset = input.size(0)\n",
    "        last_batch_size = batch_sizes[-1]\n",
    "        initial_hidden = hidden\n",
    "        flat_hidden = not isinstance(hidden, tuple)\n",
    "        if flat_hidden:\n",
    "            hidden = (hidden,)\n",
    "            initial_hidden = (initial_hidden,)\n",
    "        hidden = tuple(h[: batch_sizes[-1]] for h in hidden)\n",
    "        for batch_size in reversed(batch_sizes):\n",
    "            inc = batch_size - last_batch_size\n",
    "            if inc > 0:\n",
    "                hidden = tuple(\n",
    "                    torch.cat((h, ih[last_batch_size:batch_size]), 0)\n",
    "                    for h, ih in zip(hidden, initial_hidden)\n",
    "                )\n",
    "            last_batch_size = batch_size\n",
    "            step_input = input[input_offset - batch_size : input_offset]\n",
    "            input_offset -= batch_size\n",
    "\n",
    "            if flat_hidden:\n",
    "                hidden = (inner(step_input, hidden[0], *weight),)\n",
    "            else:\n",
    "                hidden = inner(step_input, hidden, *weight)\n",
    "            output.append(hidden[0])\n",
    "\n",
    "        output.reverse()\n",
    "        output = torch.cat(output, 0)\n",
    "        if flat_hidden:\n",
    "            hidden = hidden[0]\n",
    "        return hidden, output\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def StackedRNN(inners, num_layers, lstm=False, dropout=0, train=True):\n",
    "\n",
    "    num_directions = len(inners)\n",
    "    total_layers = num_layers * num_directions\n",
    "\n",
    "    def forward(input, hidden, weight):\n",
    "        assert len(weight) == total_layers\n",
    "        next_hidden = []\n",
    "\n",
    "        if lstm:\n",
    "            hidden = list(zip(*hidden))\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            all_output = []\n",
    "            for j, inner in enumerate(inners):\n",
    "                l = i * num_directions + j\n",
    "\n",
    "                hy, output = inner(input, hidden[l], weight[l])\n",
    "                next_hidden.append(hy)\n",
    "                all_output.append(output)\n",
    "\n",
    "            input = torch.cat(all_output, input.dim() - 1)\n",
    "\n",
    "            if dropout != 0 and i < num_layers - 1:\n",
    "                input = F.dropout(input, p=dropout, training=train, inplace=False)\n",
    "\n",
    "        if lstm:\n",
    "            next_h, next_c = zip(*next_hidden)\n",
    "            next_hidden = (\n",
    "                torch.cat(next_h, 0).view(total_layers, *next_h[0].size()),\n",
    "                torch.cat(next_c, 0).view(total_layers, *next_c[0].size()),\n",
    "            )\n",
    "        else:\n",
    "            next_hidden = torch.cat(next_hidden, 0).view(\n",
    "                total_layers, *next_hidden[0].size()\n",
    "            )\n",
    "\n",
    "        return next_hidden, input\n",
    "\n",
    "    return forward\n",
    "\n",
    "\n",
    "def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n",
    "    \"\"\"\n",
    "    A modified LSTM cell with hard sigmoid activation on the input, forget and output gates.\n",
    "    \"\"\"\n",
    "    hx, cx = hidden\n",
    "    gates = F.linear(input, w_ih, b_ih) + F.linear(hx, w_hh, b_hh)\n",
    "\n",
    "    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "    ingate = hard_sigmoid(ingate)\n",
    "    forgetgate = hard_sigmoid(forgetgate)\n",
    "    cellgate = F.tanh(cellgate)\n",
    "    outgate = hard_sigmoid(outgate)\n",
    "\n",
    "    cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "    hy = outgate * F.tanh(cy)\n",
    "\n",
    "    return hy, cy\n",
    "\n",
    "\n",
    "def hard_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes element-wise hard sigmoid of x.\n",
    "    See e.g. https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/sigm.py#L279\n",
    "    \"\"\"\n",
    "    x = (0.2 * x) + 0.5\n",
    "    x = F.threshold(-x, -1, -1)\n",
    "    x = F.threshold(-x, 0, 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:140: DeprecationWarning: Flags not at the start of the expression '\\\\s+|((?:https?://|ww' (truncated)\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "# torchmoji/tokenizer.py\n",
    "\n",
    "\"\"\"\n",
    "Splits up a Unicode string into a list of tokens.\n",
    "Recognises:\n",
    "- Abbreviations\n",
    "- URLs\n",
    "- Emails\n",
    "- #hashtags\n",
    "- @mentions\n",
    "- emojis\n",
    "- emoticons (limited support)\n",
    "\n",
    "Multiple consecutive symbols are also treated as a single token.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Basic patterns.\n",
    "RE_NUM = r\"[0-9]+\"\n",
    "RE_WORD = r\"[a-zA-Z]+\"\n",
    "RE_WHITESPACE = r\"\\s+\"\n",
    "RE_ANY = r\".\"\n",
    "\n",
    "# Combined words such as 'red-haired' or 'CUSTOM_TOKEN'\n",
    "RE_COMB = r\"[a-zA-Z]+[-_][a-zA-Z]+\"\n",
    "\n",
    "# English-specific patterns\n",
    "RE_CONTRACTIONS = RE_WORD + r\"\\'\" + RE_WORD\n",
    "\n",
    "TITLES = [\n",
    "    r\"Mr\\.\",\n",
    "    r\"Ms\\.\",\n",
    "    r\"Mrs\\.\",\n",
    "    r\"Dr\\.\",\n",
    "    r\"Prof\\.\",\n",
    "]\n",
    "# Ensure case insensitivity\n",
    "RE_TITLES = r\"|\".join([r\"(?i)\" + t for t in TITLES])\n",
    "\n",
    "# Symbols have to be created as separate patterns in order to match consecutive\n",
    "# identical symbols.\n",
    "SYMBOLS = r\"()<!?.,/\\'\\\"-_=\\\\§|´ˇ°[]<>{}~$^&*;:%+\\xa3€`\"\n",
    "RE_SYMBOL = r\"|\".join([re.escape(s) + r\"+\" for s in SYMBOLS])\n",
    "\n",
    "# Hash symbols and at symbols have to be defined separately in order to not\n",
    "# clash with hashtags and mentions if there are multiple - i.e.\n",
    "# ##hello -> ['#', '#hello'] instead of ['##', 'hello']\n",
    "SPECIAL_SYMBOLS = r\"|#+(?=#[a-zA-Z0-9_]+)|@+(?=@[a-zA-Z0-9_]+)|#+|@+\"\n",
    "RE_SYMBOL += SPECIAL_SYMBOLS\n",
    "\n",
    "RE_ABBREVIATIONS = r\"\\b(?<!\\.)(?:[A-Za-z]\\.){2,}\"\n",
    "\n",
    "# Twitter-specific patterns\n",
    "RE_HASHTAG = r\"#[a-zA-Z0-9_]+\"\n",
    "RE_MENTION = r\"@[a-zA-Z0-9_]+\"\n",
    "\n",
    "RE_URL = r\"(?:https?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\n",
    "RE_EMAIL = r\"\\b[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\\b\"\n",
    "\n",
    "# Emoticons and emojis\n",
    "RE_HEART = r\"(?:<+/?3+)+\"\n",
    "EMOTICONS_START = [\n",
    "    r\">:\",\n",
    "    r\":\",\n",
    "    r\"=\",\n",
    "    r\";\",\n",
    "]\n",
    "EMOTICONS_MID = [\n",
    "    r\"-\",\n",
    "    r\",\",\n",
    "    r\"^\",\n",
    "    \"'\",\n",
    "    '\"',\n",
    "]\n",
    "EMOTICONS_END = [\n",
    "    r\"D\",\n",
    "    r\"d\",\n",
    "    r\"p\",\n",
    "    r\"P\",\n",
    "    r\"v\",\n",
    "    r\")\",\n",
    "    r\"o\",\n",
    "    r\"O\",\n",
    "    r\"(\",\n",
    "    r\"3\",\n",
    "    r\"/\",\n",
    "    r\"|\",\n",
    "    \"\\\\\",\n",
    "]\n",
    "EMOTICONS_EXTRA = [\n",
    "    r\"-_-\",\n",
    "    r\"x_x\",\n",
    "    r\"^_^\",\n",
    "    r\"o.o\",\n",
    "    r\"o_o\",\n",
    "    r\"(:\",\n",
    "    r\"):\",\n",
    "    r\");\",\n",
    "    r\"(;\",\n",
    "]\n",
    "\n",
    "RE_EMOTICON = r\"|\".join([re.escape(s) for s in EMOTICONS_EXTRA])\n",
    "for s in EMOTICONS_START:\n",
    "    for m in EMOTICONS_MID:\n",
    "        for e in EMOTICONS_END:\n",
    "            RE_EMOTICON += \"|{0}{1}?{2}+\".format(\n",
    "                re.escape(s), re.escape(m), re.escape(e)\n",
    "            )\n",
    "\n",
    "# requires ucs4 in python2.7 or python3+\n",
    "# RE_EMOJI = r\"\"\"[\\U0001F300-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]\"\"\"\n",
    "# safe for all python\n",
    "RE_EMOJI = r\"\"\"\\ud83c[\\udf00-\\udfff]|\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|[\\u2600-\\u26FF\\u2700-\\u27BF]\"\"\"\n",
    "\n",
    "# List of matched token patterns, ordered from most specific to least specific.\n",
    "TOKENS = [\n",
    "    RE_URL,\n",
    "    RE_EMAIL,\n",
    "    RE_COMB,\n",
    "    RE_HASHTAG,\n",
    "    RE_MENTION,\n",
    "    RE_HEART,\n",
    "    RE_EMOTICON,\n",
    "    RE_CONTRACTIONS,\n",
    "    RE_TITLES,\n",
    "    RE_ABBREVIATIONS,\n",
    "    RE_NUM,\n",
    "    RE_WORD,\n",
    "    RE_SYMBOL,\n",
    "    RE_EMOJI,\n",
    "    RE_ANY,\n",
    "]\n",
    "\n",
    "# List of ignored token patterns\n",
    "IGNORED = [RE_WHITESPACE]\n",
    "\n",
    "# Final pattern\n",
    "RE_PATTERN = re.compile(\n",
    "    r\"|\".join(IGNORED) + r\"|(\" + r\"|\".join(TOKENS) + r\")\", re.UNICODE\n",
    ")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Splits given input string into a list of tokens.\n",
    "\n",
    "    # Arguments:\n",
    "        text: Input string to be tokenized.\n",
    "\n",
    "    # Returns:\n",
    "        List of strings (tokens).\n",
    "    \"\"\"\n",
    "    result = RE_PATTERN.findall(text)\n",
    "\n",
    "    # Remove empty strings\n",
    "    result = [t for t in result if t.strip()]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f77bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/attlayer.py\n",
    "\n",
    "\"\"\" Define the Attention Layer of the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TorchmojiAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_size, return_attention=False):\n",
    "        \"\"\"Initialize the attention layer\n",
    "\n",
    "        # Arguments:\n",
    "            attention_size: Size of the attention vector.\n",
    "            return_attention: If true, output will include the weight for each input token\n",
    "                              used for the prediction\n",
    "\n",
    "        \"\"\"\n",
    "        super(TorchmojiAttention, self).__init__()\n",
    "        self.return_attention = return_attention\n",
    "        self.attention_size = attention_size\n",
    "        self.attention_vector = Parameter(torch.FloatTensor(attention_size))\n",
    "        self.attention_vector.data.normal_(std=0.05)  # Initialize attention vector\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"{name}({attention_size}, return attention={return_attention})\"\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
    "\n",
    "    def forward(self, inputs, input_lengths):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        # Arguments:\n",
    "            inputs (Torch.Variable): Tensor of input sequences\n",
    "            input_lengths (torch.LongTensor): Lengths of the sequences\n",
    "\n",
    "        # Return:\n",
    "            Tuple with (representations and attentions if self.return_attention else None).\n",
    "        \"\"\"\n",
    "        logits = inputs.matmul(self.attention_vector)\n",
    "        unnorm_ai = (logits - logits.max()).exp()\n",
    "\n",
    "        # Compute a mask for the attention on the padded sequences\n",
    "        # See e.g. https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/5\n",
    "        max_len = unnorm_ai.size(1)\n",
    "        idxes = torch.arange(0, max_len, out=torch.LongTensor(max_len)).unsqueeze(0)\n",
    "        #         if torch.cuda.is_available():\n",
    "        #             idxes = idxes.cuda()\n",
    "\n",
    "        mask = Variable((idxes < input_lengths.unsqueeze(1)).float())\n",
    "\n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked_weights = unnorm_ai * mask\n",
    "        att_sums = masked_weights.sum(dim=1, keepdim=True)  # sums per sequence\n",
    "        attentions = masked_weights.div(att_sums)\n",
    "\n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(dim=1)\n",
    "\n",
    "        return (representations, attentions if self.return_attention else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609056de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/create_vocab.py\n",
    "\n",
    "\n",
    "class VocabBuilder:\n",
    "    \"\"\"Create vocabulary with words extracted from sentences as fed from a\n",
    "    word generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, word_gen):\n",
    "        # initialize any new key with value of 0\n",
    "        self.word_counts = defaultdict(lambda: 0, {})\n",
    "        self.word_length_limit = 30\n",
    "\n",
    "        for token in SPECIAL_TOKENS:\n",
    "            assert len(token) < self.word_length_limit\n",
    "            self.word_counts[token] = 0\n",
    "        self.word_gen = word_gen\n",
    "\n",
    "    def count_words_in_sentence(self, words):\n",
    "        \"\"\"Generates word counts for all tokens in the given sentence.\n",
    "\n",
    "        # Arguments:\n",
    "            words: Tokenized sentence whose words should be counted.\n",
    "        \"\"\"\n",
    "        for word in words:\n",
    "            if 0 < len(word) and len(word) <= self.word_length_limit:\n",
    "                try:\n",
    "                    self.word_counts[word] += 1\n",
    "                except KeyError:\n",
    "                    self.word_counts[word] = 1\n",
    "\n",
    "    def save_vocab(self, path=None):\n",
    "        \"\"\"Saves the vocabulary into a file.\n",
    "\n",
    "        # Arguments:\n",
    "            path: Where the vocabulary should be saved. If not specified, a\n",
    "                  randomly generated filename is used instead.\n",
    "        \"\"\"\n",
    "        dtype = [(\"word\", \"|S{}\".format(self.word_length_limit)), (\"count\", \"int\")]\n",
    "        np_dict = np.array(self.word_counts.items(), dtype=dtype)\n",
    "\n",
    "        # sort from highest to lowest frequency\n",
    "        np_dict[::-1].sort(order=\"count\")\n",
    "        data = np_dict\n",
    "\n",
    "        if path is None:\n",
    "            path = str(uuid.uuid4())\n",
    "\n",
    "        np.savez_compressed(path, data=data)\n",
    "        print(\"Saved dict to {}\".format(path))\n",
    "\n",
    "    def get_next_word(self):\n",
    "        \"\"\"Returns next tokenized sentence from the word geneerator.\n",
    "\n",
    "        # Returns:\n",
    "            List of strings, representing the next tokenized sentence.\n",
    "        \"\"\"\n",
    "        return self.word_gen.__iter__().next()\n",
    "\n",
    "    def count_all_words(self):\n",
    "        \"\"\"Generates word counts for all words in all sentences of the word\n",
    "        generator.\n",
    "        \"\"\"\n",
    "        for words, _ in self.word_gen:\n",
    "            self.count_words_in_sentence(words)\n",
    "\n",
    "\n",
    "class MasterVocab:\n",
    "    \"\"\"Combines vocabularies.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # initialize custom tokens\n",
    "        self.master_vocab = {}\n",
    "\n",
    "    def populate_master_vocab(self, vocab_path, min_words=1, force_appearance=None):\n",
    "        \"\"\"Populates the master vocabulary using all vocabularies found in the\n",
    "            given path. Vocabularies should be named *.npz. Expects the\n",
    "            vocabularies to be numpy arrays with counts. Normalizes the counts\n",
    "            and combines them.\n",
    "\n",
    "        # Arguments:\n",
    "            vocab_path: Path containing vocabularies to be combined.\n",
    "            min_words: Minimum amount of occurences a word must have in order\n",
    "                to be included in the master vocabulary.\n",
    "            force_appearance: Optional vocabulary filename that will be added\n",
    "                to the master vocabulary no matter what. This vocabulary must\n",
    "                be present in vocab_path.\n",
    "        \"\"\"\n",
    "\n",
    "        paths = glob.glob(vocab_path + \"*.npz\")\n",
    "        sizes = {path: 0 for path in paths}\n",
    "        dicts = {path: {} for path in paths}\n",
    "\n",
    "        # set up and get sizes of individual dictionaries\n",
    "        for path in paths:\n",
    "            np_data = np.load(path)[\"data\"]\n",
    "\n",
    "            for entry in np_data:\n",
    "                word, count = entry\n",
    "                if count < min_words:\n",
    "                    continue\n",
    "                if is_special_token(word):\n",
    "                    continue\n",
    "                dicts[path][word] = count\n",
    "\n",
    "            sizes[path] = sum(dicts[path].values())\n",
    "            print(\"Overall word count for {} -> {}\".format(path, sizes[path]))\n",
    "            print(\"Overall word number for {} -> {}\".format(path, len(dicts[path])))\n",
    "\n",
    "        vocab_of_max_size = max(sizes, key=sizes.get)\n",
    "        max_size = sizes[vocab_of_max_size]\n",
    "        print(\"Min: {}, {}, {}\".format(sizes, vocab_of_max_size, max_size))\n",
    "\n",
    "        # can force one vocabulary to always be present\n",
    "        if force_appearance is not None:\n",
    "            force_appearance_path = [p for p in paths if force_appearance in p][0]\n",
    "            force_appearance_vocab = deepcopy(dicts[force_appearance_path])\n",
    "            print(force_appearance_path)\n",
    "        else:\n",
    "            force_appearance_path, force_appearance_vocab = None, None\n",
    "\n",
    "        # normalize word counts before inserting into master dict\n",
    "        for path in paths:\n",
    "            normalization_factor = max_size / sizes[path]\n",
    "            print(\"Norm factor for path {} -> {}\".format(path, normalization_factor))\n",
    "\n",
    "            for word in dicts[path]:\n",
    "                if is_special_token(word):\n",
    "                    print(\"SPECIAL - \", word)\n",
    "                    continue\n",
    "                normalized_count = dicts[path][word] * normalization_factor\n",
    "\n",
    "                # can force one vocabulary to always be present\n",
    "                if force_appearance_vocab is not None:\n",
    "                    try:\n",
    "                        force_word_count = force_appearance_vocab[word]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    # if force_word_count < 5:\n",
    "                    # continue\n",
    "\n",
    "                if word in self.master_vocab:\n",
    "                    self.master_vocab[word] += normalized_count\n",
    "                else:\n",
    "                    self.master_vocab[word] = normalized_count\n",
    "\n",
    "        print(\"Size of master_dict {}\".format(len(self.master_vocab)))\n",
    "        print(\n",
    "            \"Hashes for master dict: {}\".format(\n",
    "                len([w for w in self.master_vocab if \"#\" in w[0]])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def save_vocab(self, path_count, path_vocab, word_limit=100000):\n",
    "        \"\"\"Saves the master vocabulary into a file.\"\"\"\n",
    "\n",
    "        # reserve space for 10 special tokens\n",
    "        words = OrderedDict()\n",
    "        for token in SPECIAL_TOKENS:\n",
    "            # store -1 instead of np.inf, which can overflow\n",
    "            words[token] = -1\n",
    "\n",
    "        # sort words by frequency\n",
    "        desc_order = OrderedDict(\n",
    "            sorted(self.master_vocab.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        )\n",
    "        words.update(desc_order)\n",
    "\n",
    "        # use encoding of up to 30 characters (no token conversions)\n",
    "        # use float to store large numbers (we don't care about precision loss)\n",
    "        np_vocab = np.array(\n",
    "            words.items(), dtype=([(\"word\", \"|S30\"), (\"count\", \"float\")])\n",
    "        )\n",
    "\n",
    "        # output count for debugging\n",
    "        counts = np_vocab[:word_limit]\n",
    "        np.savez_compressed(path_count, counts=counts)\n",
    "\n",
    "        # output the index of each word for easy lookup\n",
    "        final_words = OrderedDict()\n",
    "        for i, w in enumerate(words.keys()[:word_limit]):\n",
    "            final_words.update({w: i})\n",
    "        with open(path_vocab, \"w\") as f:\n",
    "            f.write(json.dumps(final_words, indent=4, separators=(\",\", \": \")))\n",
    "\n",
    "\n",
    "def all_words_in_sentences(sentences):\n",
    "    \"\"\"Extracts all unique words from a given list of sentences.\n",
    "\n",
    "    # Arguments:\n",
    "        sentences: List or word generator of sentences to be processed.\n",
    "\n",
    "    # Returns:\n",
    "        List of all unique words contained in the given sentences.\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "    if isinstance(sentences, WordGenerator):\n",
    "        sentences = [s for s, _ in sentences]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def extend_vocab_in_file(\n",
    "    vocab, max_tokens=10000, vocab_path=\"../models/vocabulary.json\"\n",
    "):\n",
    "    \"\"\"Extends JSON-formatted vocabulary with words from vocab that are not\n",
    "        present in the current vocabulary. Adds up to max_tokens words.\n",
    "        Overwrites file in vocab_path.\n",
    "\n",
    "    # Arguments:\n",
    "        new_vocab: Vocabulary to be added. MUST have word_counts populated, i.e.\n",
    "            must have run count_all_words() previously.\n",
    "        max_tokens: Maximum number of words to be added.\n",
    "        vocab_path: Path to the vocabulary json which is to be extended.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(vocab_path, \"r\") as f:\n",
    "            current_vocab = json.load(f)\n",
    "    except IOError:\n",
    "        print(\"Vocabulary file not found, expected at \" + vocab_path)\n",
    "        return\n",
    "\n",
    "    extend_vocab(current_vocab, vocab, max_tokens)\n",
    "\n",
    "    # Save back to file\n",
    "    with open(vocab_path, \"w\") as f:\n",
    "        json.dump(current_vocab, f, sort_keys=True, indent=4, separators=(\",\", \": \"))\n",
    "\n",
    "\n",
    "def extend_vocab(current_vocab, new_vocab, max_tokens=10000):\n",
    "    \"\"\"Extends current vocabulary with words from vocab that are not\n",
    "        present in the current vocabulary. Adds up to max_tokens words.\n",
    "\n",
    "    # Arguments:\n",
    "        current_vocab: Current dictionary of tokens.\n",
    "        new_vocab: Vocabulary to be added. MUST have word_counts populated, i.e.\n",
    "            must have run count_all_words() previously.\n",
    "        max_tokens: Maximum number of words to be added.\n",
    "\n",
    "    # Returns:\n",
    "        How many new tokens have been added.\n",
    "    \"\"\"\n",
    "    if max_tokens < 0:\n",
    "        max_tokens = 10000\n",
    "\n",
    "    words = OrderedDict()\n",
    "\n",
    "    # sort words by frequency\n",
    "    desc_order = OrderedDict(\n",
    "        sorted(new_vocab.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    )\n",
    "    words.update(desc_order)\n",
    "\n",
    "    base_index = len(current_vocab.keys())\n",
    "    added = 0\n",
    "    for word in words:\n",
    "        if added >= max_tokens:\n",
    "            break\n",
    "        if word not in current_vocab.keys():\n",
    "            current_vocab[word] = base_index + added\n",
    "            added += 1\n",
    "\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c9a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/sentence_tokenizer.py\n",
    "\n",
    "\"\"\"\n",
    "Provides functionality for converting a given list of tokens (words) into\n",
    "numbers, according to the given vocabulary.\n",
    "\"\"\"\n",
    "from __future__ import print_function, division, unicode_literals\n",
    "\n",
    "import numbers\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class SentenceTokenizer:\n",
    "    \"\"\"Create numpy array of tokens corresponding to input sentences.\n",
    "    The vocabulary can include Unicode tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary,\n",
    "        fixed_length,\n",
    "        custom_wordgen=None,\n",
    "        ignore_sentences_with_only_custom=False,\n",
    "        masking_value=0,\n",
    "        unknown_value=1,\n",
    "    ):\n",
    "        \"\"\"Needs a dictionary as input for the vocabulary.\"\"\"\n",
    "\n",
    "        if len(vocabulary) > np.iinfo(\"uint16\").max:\n",
    "            raise ValueError(\n",
    "                \"Dictionary is too big ({} tokens) for the numpy \"\n",
    "                \"datatypes used (max limit={}). Reduce vocabulary\"\n",
    "                \" or adjust code accordingly!\".format(\n",
    "                    len(vocabulary), np.iinfo(\"uint16\").max\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Shouldn't be able to modify the given vocabulary\n",
    "        self.vocabulary = deepcopy(vocabulary)\n",
    "        self.fixed_length = fixed_length\n",
    "        self.ignore_sentences_with_only_custom = ignore_sentences_with_only_custom\n",
    "        self.masking_value = masking_value\n",
    "        self.unknown_value = unknown_value\n",
    "\n",
    "        # Initialized with an empty stream of sentences that must then be fed\n",
    "        # to the generator at a later point for reusability.\n",
    "        # A custom word generator can be used for domain-specific filtering etc\n",
    "        if custom_wordgen is not None:\n",
    "            assert custom_wordgen.stream is None\n",
    "            self.wordgen = custom_wordgen\n",
    "            self.uses_custom_wordgen = True\n",
    "        else:\n",
    "            self.wordgen = WordGenerator(\n",
    "                None,\n",
    "                allow_unicode_text=True,\n",
    "                ignore_emojis=False,\n",
    "                remove_variation_selectors=True,\n",
    "                break_replacement=True,\n",
    "            )\n",
    "            self.uses_custom_wordgen = False\n",
    "\n",
    "    def tokenize_sentences(self, sentences, reset_stats=True, max_sentences=None):\n",
    "        \"\"\"Converts a given list of sentences into a numpy array according to\n",
    "            its vocabulary.\n",
    "\n",
    "        # Arguments:\n",
    "            sentences: List of sentences to be tokenized.\n",
    "            reset_stats: Whether the word generator's stats should be reset.\n",
    "            max_sentences: Maximum length of sentences. Must be set if the\n",
    "                length cannot be inferred from the input.\n",
    "\n",
    "        # Returns:\n",
    "            Numpy array of the tokenization sentences with masking,\n",
    "            infos,\n",
    "            stats\n",
    "\n",
    "        # Raises:\n",
    "            ValueError: When maximum length is not set and cannot be inferred.\n",
    "        \"\"\"\n",
    "\n",
    "        if max_sentences is None and not hasattr(sentences, \"__len__\"):\n",
    "            raise ValueError(\n",
    "                \"Either you must provide an array with a length\"\n",
    "                \"attribute (e.g. a list) or specify the maximum \"\n",
    "                \"length yourself using `max_sentences`!\"\n",
    "            )\n",
    "        n_sentences = max_sentences if max_sentences is not None else len(sentences)\n",
    "\n",
    "        if self.masking_value == 0:\n",
    "            tokens = np.zeros((n_sentences, self.fixed_length), dtype=\"uint16\")\n",
    "        else:\n",
    "            tokens = (\n",
    "                np.ones((n_sentences, self.fixed_length), dtype=\"uint16\")\n",
    "                * self.masking_value\n",
    "            )\n",
    "\n",
    "        if reset_stats:\n",
    "            self.wordgen.reset_stats()\n",
    "\n",
    "        # With a custom word generator info can be extracted from each\n",
    "        # sentence (e.g. labels)\n",
    "        infos = []\n",
    "\n",
    "        # Returns words as strings and then map them to vocabulary\n",
    "        self.wordgen.stream = sentences\n",
    "        next_insert = 0\n",
    "        n_ignored_unknowns = 0\n",
    "        for s_words, s_info in self.wordgen:\n",
    "            s_tokens = self.find_tokens(s_words)\n",
    "\n",
    "            if self.ignore_sentences_with_only_custom and np.all(\n",
    "                [True if t < len(SPECIAL_TOKENS) else False for t in s_tokens]\n",
    "            ):\n",
    "                n_ignored_unknowns += 1\n",
    "                continue\n",
    "            if len(s_tokens) > self.fixed_length:\n",
    "                s_tokens = s_tokens[: self.fixed_length]\n",
    "            tokens[next_insert, : len(s_tokens)] = s_tokens\n",
    "            infos.append(s_info)\n",
    "            next_insert += 1\n",
    "\n",
    "        # For standard word generators all sentences should be tokenized\n",
    "        # this is not necessarily the case for custom wordgenerators as they\n",
    "        # may filter the sentences etc.\n",
    "        if not self.uses_custom_wordgen and not self.ignore_sentences_with_only_custom:\n",
    "            assert len(sentences) == next_insert\n",
    "        else:\n",
    "            # adjust based on actual tokens received\n",
    "            tokens = tokens[:next_insert]\n",
    "            infos = infos[:next_insert]\n",
    "\n",
    "        return tokens, infos, self.wordgen.stats\n",
    "\n",
    "    def find_tokens(self, words):\n",
    "        assert len(words) > 0\n",
    "        tokens = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                tokens.append(self.vocabulary[w])\n",
    "            except KeyError:\n",
    "                tokens.append(self.unknown_value)\n",
    "        return tokens\n",
    "\n",
    "    def split_train_val_test(\n",
    "        self, sentences, info_dicts, split_parameter=[0.7, 0.1, 0.2], extend_with=0\n",
    "    ):\n",
    "        \"\"\"Splits given sentences into three different datasets: training,\n",
    "            validation and testing.\n",
    "\n",
    "        # Arguments:\n",
    "            sentences: The sentences to be tokenized.\n",
    "            info_dicts: A list of dicts that contain information about each\n",
    "                sentence (e.g. a label).\n",
    "            split_parameter: A parameter for deciding the splits between the\n",
    "                three different datasets. If instead of being passed three\n",
    "                values, three lists are passed, then these will be used to\n",
    "                specify which observation belong to which dataset.\n",
    "            extend_with: An optional parameter. If > 0 then this is the number\n",
    "                of tokens added to the vocabulary from this dataset. The\n",
    "                expanded vocab will be generated using only the training set,\n",
    "                but is applied to all three sets.\n",
    "\n",
    "        # Returns:\n",
    "            List of three lists of tokenized sentences,\n",
    "\n",
    "            List of three corresponding dictionaries with information,\n",
    "\n",
    "            How many tokens have been added to the vocab. Make sure to extend\n",
    "            the embedding layer of the model accordingly.\n",
    "        \"\"\"\n",
    "\n",
    "        # If passed three lists, use those directly\n",
    "        if (\n",
    "            isinstance(split_parameter, list)\n",
    "            and all(isinstance(x, list) for x in split_parameter)\n",
    "            and len(split_parameter) == 3\n",
    "        ):\n",
    "\n",
    "            # Helper function to verify provided indices are numbers in range\n",
    "            def verify_indices(inds):\n",
    "                return list(\n",
    "                    filter(\n",
    "                        lambda i: isinstance(i, numbers.Number) and i < len(sentences),\n",
    "                        inds,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            ind_train = verify_indices(split_parameter[0])\n",
    "            ind_val = verify_indices(split_parameter[1])\n",
    "            ind_test = verify_indices(split_parameter[2])\n",
    "        else:\n",
    "            # Split sentences and dicts\n",
    "            ind = list(range(len(sentences)))\n",
    "            ind_train, ind_test = train_test_split(ind, test_size=split_parameter[2])\n",
    "            ind_train, ind_val = train_test_split(\n",
    "                ind_train, test_size=split_parameter[1]\n",
    "            )\n",
    "\n",
    "        # Map indices to data\n",
    "        train = np.array([sentences[x] for x in ind_train])\n",
    "        test = np.array([sentences[x] for x in ind_test])\n",
    "        val = np.array([sentences[x] for x in ind_val])\n",
    "\n",
    "        info_train = np.array([info_dicts[x] for x in ind_train])\n",
    "        info_test = np.array([info_dicts[x] for x in ind_test])\n",
    "        info_val = np.array([info_dicts[x] for x in ind_val])\n",
    "\n",
    "        added = 0\n",
    "        # Extend vocabulary with training set tokens\n",
    "        if extend_with > 0:\n",
    "            wg = WordGenerator(train)\n",
    "            vb = VocabBuilder(wg)\n",
    "            vb.count_all_words()\n",
    "            added = extend_vocab(self.vocabulary, vb, max_tokens=extend_with)\n",
    "\n",
    "        # Wrap results\n",
    "        result = [self.tokenize_sentences(s)[0] for s in [train, val, test]]\n",
    "        result_infos = [info_train, info_val, info_test]\n",
    "        # if type(result_infos[0][0]) in [np.double, np.float, np.int64, np.int32, np.uint8]:\n",
    "        #     result_infos = [torch.from_numpy(label).long() for label in result_infos]\n",
    "\n",
    "        return result, result_infos, added\n",
    "\n",
    "    def to_sentence(self, sentence_idx):\n",
    "        \"\"\"Converts a tokenized sentence back to a list of words.\n",
    "\n",
    "        # Arguments:\n",
    "            sentence_idx: List of numbers, representing a tokenized sentence\n",
    "                given the current vocabulary.\n",
    "\n",
    "        # Returns:\n",
    "            String created by converting all numbers back to words and joined\n",
    "            together with spaces.\n",
    "        \"\"\"\n",
    "        # Have to recalculate the mappings in case the vocab was extended.\n",
    "        ind_to_word = {ind: word for word, ind in self.vocabulary.items()}\n",
    "\n",
    "        sentence_as_list = [ind_to_word[x] for x in sentence_idx]\n",
    "        cleaned_list = [x for x in sentence_as_list if x != \"CUSTOM_MASK\"]\n",
    "        return \" \".join(cleaned_list)\n",
    "\n",
    "\n",
    "def coverage(dataset, verbose=False):\n",
    "    \"\"\"Computes the percentage of words in a given dataset that are unknown.\n",
    "\n",
    "    # Arguments:\n",
    "        dataset: Tokenized dataset to be checked.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Percentage of unknown tokens.\n",
    "    \"\"\"\n",
    "    n_total = np.count_nonzero(dataset)\n",
    "    n_unknown = np.sum(dataset == 1)\n",
    "    coverage = 1.0 - float(n_unknown) / n_total\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Unknown words: {}\".format(n_unknown))\n",
    "        print(\"Total words: {}\".format(n_total))\n",
    "        print(\"Coverage: {}\".format(coverage))\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b322386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/model_def.py\n",
    "\"\"\" Model definition functions and weight loading.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def torchmoji_feature_encoding(weight_path, return_attention=False, verbose=False):\n",
    "    \"\"\"Loads the pretrained torchMoji model for extracting features\n",
    "        from the penultimate feature layer. In this way, it transforms\n",
    "        the text into its emotional encoding.\n",
    "\n",
    "    # Arguments:\n",
    "        weight_path: Path to model weights to be loaded.\n",
    "        return_attention: If true, output will include weight of each input token\n",
    "            used for the prediction\n",
    "\n",
    "    # Returns:\n",
    "        Pretrained model for encoding text into feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    model = TorchMoji(\n",
    "        nb_classes=None,\n",
    "        nb_tokens=NB_TOKENS,\n",
    "        feature_output=True,\n",
    "        return_attention=return_attention,\n",
    "    )\n",
    "    load_specific_weights(\n",
    "        model, weight_path, exclude_names=[\"output_layer\"], verbose=verbose\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def torchmoji_emojis(weight_path, return_attention=False):\n",
    "    \"\"\"Loads the pretrained torchMoji model for extracting features\n",
    "        from the penultimate feature layer. In this way, it transforms\n",
    "        the text into its emotional encoding.\n",
    "\n",
    "    # Arguments:\n",
    "        weight_path: Path to model weights to be loaded.\n",
    "        return_attention: If true, output will include weight of each input token\n",
    "            used for the prediction\n",
    "\n",
    "    # Returns:\n",
    "        Pretrained model for encoding text into feature vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    model = TorchMoji(\n",
    "        nb_classes=NB_EMOJI_CLASSES,\n",
    "        nb_tokens=NB_TOKENS,\n",
    "        return_attention=return_attention,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def torchmoji_transfer(\n",
    "    nb_classes,\n",
    "    weight_path=None,\n",
    "    extend_embedding=0,\n",
    "    embed_dropout_rate=0.1,\n",
    "    final_dropout_rate=0.5,\n",
    "):\n",
    "    \"\"\"Loads the pretrained torchMoji model for finetuning/transfer learning.\n",
    "        Does not load weights for the softmax layer.\n",
    "\n",
    "        Note that if you are planning to use class average F1 for evaluation,\n",
    "        nb_classes should be set to 2 instead of the actual number of classes\n",
    "        in the dataset, since binary classification will be performed on each\n",
    "        class individually.\n",
    "\n",
    "        Note that for the 'new' method, weight_path should be left as None.\n",
    "\n",
    "    # Arguments:\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        weight_path: Path to model weights to be loaded.\n",
    "        extend_embedding: Number of tokens that have been added to the\n",
    "            vocabulary on top of NB_TOKENS. If this number is larger than 0,\n",
    "            the embedding layer's dimensions are adjusted accordingly, with the\n",
    "            additional weights being set to random values.\n",
    "        embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "        final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "\n",
    "    # Returns:\n",
    "        Model with the given parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    model = TorchMoji(\n",
    "        nb_classes=nb_classes,\n",
    "        nb_tokens=NB_TOKENS + extend_embedding,\n",
    "        embed_dropout_rate=embed_dropout_rate,\n",
    "        final_dropout_rate=final_dropout_rate,\n",
    "        output_logits=True,\n",
    "    )\n",
    "    if weight_path is not None:\n",
    "        load_specific_weights(\n",
    "            model,\n",
    "            weight_path,\n",
    "            exclude_names=[\"output_layer\"],\n",
    "            extend_embedding=extend_embedding,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "class TorchMoji(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nb_classes,\n",
    "        nb_tokens,\n",
    "        feature_output=False,\n",
    "        output_logits=False,\n",
    "        embed_dropout_rate=0,\n",
    "        final_dropout_rate=0,\n",
    "        return_attention=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        torchMoji model.\n",
    "        IMPORTANT: The model is loaded in evaluation mode by default (self.eval())\n",
    "\n",
    "        # Arguments:\n",
    "            nb_classes: Number of classes in the dataset.\n",
    "            nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n",
    "            feature_output: If True the model returns the penultimate\n",
    "                            feature vector rather than Softmax probabilities\n",
    "                            (defaults to False).\n",
    "            output_logits:  If True the model returns logits rather than probabilities\n",
    "                            (defaults to False).\n",
    "            embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "            final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "            return_attention: If True the model also returns attention weights over the sentence\n",
    "                              (defaults to False).\n",
    "        \"\"\"\n",
    "        super(TorchMoji, self).__init__()\n",
    "\n",
    "        embedding_dim = 256\n",
    "        hidden_size = 512\n",
    "        attention_size = 4 * hidden_size + embedding_dim\n",
    "\n",
    "        self.feature_output = feature_output\n",
    "        self.embed_dropout_rate = embed_dropout_rate\n",
    "        self.final_dropout_rate = final_dropout_rate\n",
    "        self.return_attention = return_attention\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_logits = output_logits\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        self.add_module(\"embed\", nn.Embedding(nb_tokens, embedding_dim))\n",
    "        # dropout2D: embedding channels are dropped out instead of words\n",
    "        # many exampels in the datasets contain few words that losing one or more words can alter the emotions completely\n",
    "        self.add_module(\"embed_dropout\", nn.Dropout2d(embed_dropout_rate))\n",
    "        self.add_module(\n",
    "            \"lstm_0\",\n",
    "            LSTMHardSigmoid(\n",
    "                embedding_dim, hidden_size, batch_first=True, bidirectional=True\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\n",
    "            \"lstm_1\",\n",
    "            LSTMHardSigmoid(\n",
    "                hidden_size * 2, hidden_size, batch_first=True, bidirectional=True\n",
    "            ),\n",
    "        )\n",
    "        self.add_module(\n",
    "            \"attention_layer\",\n",
    "            TorchmojiAttention(\n",
    "                attention_size=attention_size, return_attention=return_attention\n",
    "            ),\n",
    "        )\n",
    "        if not feature_output:\n",
    "            self.add_module(\"final_dropout\", nn.Dropout(final_dropout_rate))\n",
    "            if output_logits:\n",
    "                self.add_module(\n",
    "                    \"output_layer\",\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(\n",
    "                            attention_size, nb_classes if self.nb_classes > 2 else 1\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                self.add_module(\n",
    "                    \"output_layer\",\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(\n",
    "                            attention_size, nb_classes if self.nb_classes > 2 else 1\n",
    "                        ),\n",
    "                        nn.Softmax() if self.nb_classes > 2 else nn.Sigmoid(),\n",
    "                    ),\n",
    "                )\n",
    "        self.init_weights()\n",
    "        # Put model in evaluation mode by default\n",
    "        self.eval()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Here we reproduce Keras default initialization weights for consistency with Keras version\n",
    "        \"\"\"\n",
    "        ih = (\n",
    "            param.data for name, param in self.named_parameters() if \"weight_ih\" in name\n",
    "        )\n",
    "        hh = (\n",
    "            param.data for name, param in self.named_parameters() if \"weight_hh\" in name\n",
    "        )\n",
    "        b = (param.data for name, param in self.named_parameters() if \"bias\" in name)\n",
    "        nn.init.uniform(self.embed.weight.data, a=-0.5, b=0.5)\n",
    "        for t in ih:\n",
    "            nn.init.xavier_uniform(t)\n",
    "        for t in hh:\n",
    "            nn.init.orthogonal(t)\n",
    "        for t in b:\n",
    "            nn.init.constant(t, 0)\n",
    "        if not self.feature_output:\n",
    "            nn.init.xavier_uniform(self.output_layer[0].weight.data)\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        # Arguments:\n",
    "            input_seqs: Can be one of Numpy array, Torch.LongTensor, Torch.Variable, Torch.PackedSequence.\n",
    "\n",
    "        # Return:\n",
    "            Same format as input format (except for PackedSequence returned as Variable).\n",
    "        \"\"\"\n",
    "        # Check if we have Torch.LongTensor inputs or not Torch.Variable (assume Numpy array in this case), take note to return same format\n",
    "        return_numpy = False\n",
    "        return_tensor = False\n",
    "        if isinstance(input_seqs, (torch.LongTensor, torch.cuda.LongTensor)):\n",
    "            input_seqs = Variable(input_seqs)\n",
    "            return_tensor = True\n",
    "        elif not isinstance(input_seqs, Variable):\n",
    "            input_seqs = Variable(torch.from_numpy(input_seqs.astype(\"int64\")).long())\n",
    "            return_numpy = True\n",
    "\n",
    "        # If we don't have a packed inputs, let's pack it\n",
    "        reorder_output = False\n",
    "        if not isinstance(input_seqs, PackedSequence):\n",
    "            ho = self.lstm_0.weight_hh_l0.data.new(\n",
    "                2, input_seqs.size()[0], self.hidden_size\n",
    "            ).zero_()\n",
    "            co = self.lstm_0.weight_hh_l0.data.new(\n",
    "                2, input_seqs.size()[0], self.hidden_size\n",
    "            ).zero_()\n",
    "\n",
    "            # Reorder batch by sequence length\n",
    "            input_lengths = torch.LongTensor(\n",
    "                [\n",
    "                    torch.max(input_seqs[i, :].data.nonzero()) + 1\n",
    "                    for i in range(input_seqs.size()[0])\n",
    "                ]\n",
    "            )\n",
    "            input_lengths, perm_idx = input_lengths.sort(0, descending=True)\n",
    "            input_seqs = input_seqs[perm_idx][:, : input_lengths.max()]\n",
    "\n",
    "            # Pack sequence and work on data tensor to reduce embeddings/dropout computations\n",
    "            packed_input = pack_padded_sequence(\n",
    "                input_seqs, input_lengths.cpu().numpy(), batch_first=True\n",
    "            )\n",
    "            reorder_output = True\n",
    "        else:\n",
    "            ho = self.lstm_0.weight_hh_l0.data.data.new(\n",
    "                2, input_seqs.size()[0], self.hidden_size\n",
    "            ).zero_()\n",
    "            co = self.lstm_0.weight_hh_l0.data.data.new(\n",
    "                2, input_seqs.size()[0], self.hidden_size\n",
    "            ).zero_()\n",
    "            input_lengths = input_seqs.batch_sizes\n",
    "            packed_input = input_seqs\n",
    "\n",
    "        hidden = (Variable(ho, requires_grad=False), Variable(co, requires_grad=False))\n",
    "\n",
    "        # Embed with an activation function to bound the values of the embeddings\n",
    "        x = self.embed(packed_input.data)\n",
    "        x = nn.Tanh()(x)\n",
    "\n",
    "        # pyTorch 2D dropout2d operate on axis 1 which is fine for us\n",
    "        x = self.embed_dropout(x)\n",
    "\n",
    "        # Update packed sequence data for RNN\n",
    "        packed_input = PackedSequence(x, packed_input.batch_sizes)\n",
    "\n",
    "        # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "        # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "        lstm_0_output, _ = self.lstm_0(packed_input, hidden)\n",
    "        lstm_1_output, _ = self.lstm_1(lstm_0_output, hidden)\n",
    "\n",
    "        # Update packed sequence data for attention layer\n",
    "        packed_input = PackedSequence(\n",
    "            torch.cat(\n",
    "                (lstm_1_output.data, lstm_0_output.data, packed_input.data), dim=1\n",
    "            ),\n",
    "            packed_input.batch_sizes,\n",
    "        )\n",
    "\n",
    "        input_seqs, _ = pad_packed_sequence(packed_input, batch_first=True)\n",
    "\n",
    "        x, att_weights = self.attention_layer(input_seqs, input_lengths)\n",
    "\n",
    "        # output class probabilities or penultimate feature vector\n",
    "        if not self.feature_output:\n",
    "            x = self.final_dropout(x)\n",
    "            outputs = self.output_layer(x)\n",
    "        else:\n",
    "            outputs = x\n",
    "\n",
    "        # Reorder output if needed\n",
    "        if reorder_output:\n",
    "            reorered = Variable(outputs.data.new(outputs.size()))\n",
    "            reorered[perm_idx] = outputs\n",
    "            outputs = reorered\n",
    "\n",
    "        # Adapt return format if needed\n",
    "        if return_tensor:\n",
    "            outputs = outputs.data\n",
    "        if return_numpy:\n",
    "            outputs = outputs.data.numpy()\n",
    "\n",
    "        if self.return_attention:\n",
    "            return outputs, att_weights\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "\n",
    "def load_specific_weights(\n",
    "    model, weight_path, exclude_names=[], extend_embedding=0, verbose=True\n",
    "):\n",
    "    \"\"\"Loads model weights from the given file path, excluding any\n",
    "        given layers.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model whose weights should be loaded.\n",
    "        weight_path: Path to file containing model weights.\n",
    "        exclude_names: List of layer names whose weights should not be loaded.\n",
    "        extend_embedding: Number of new words being added to vocabulary.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Raises:\n",
    "        ValueError if the file at weight_path does not exist.\n",
    "    \"\"\"\n",
    "    if not exists(weight_path):\n",
    "        raise ValueError(\n",
    "            \"ERROR (load_weights): The weights file at {} does \"\n",
    "            \"not exist. Refer to the README for instructions.\".format(weight_path)\n",
    "        )\n",
    "\n",
    "    if extend_embedding and \"embed\" in exclude_names:\n",
    "        raise ValueError(\n",
    "            \"ERROR (load_weights): Cannot extend a vocabulary \"\n",
    "            \"without loading the embedding weights.\"\n",
    "        )\n",
    "\n",
    "    # Copy only weights from the temporary model that are wanted\n",
    "    # for the specific task (e.g. the Softmax is often ignored)\n",
    "    weights = torch.load(weight_path)\n",
    "    for key, weight in weights.items():\n",
    "        if any(excluded in key for excluded in exclude_names):\n",
    "            if verbose:\n",
    "                print(\"Ignoring weights for {}\".format(key))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            model_w = model.state_dict()[key]\n",
    "        except KeyError:\n",
    "            raise KeyError(\n",
    "                \"Weights had parameters {},\".format(key)\n",
    "                + \" but could not find this parameters in model.\"\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Loading weights for {}\".format(key))\n",
    "\n",
    "        # extend embedding layer to allow new randomly initialized words\n",
    "        # if requested. Otherwise, just load the weights for the layer.\n",
    "        if \"embed\" in key and extend_embedding > 0:\n",
    "            weight = torch.cat((weight, model_w[NB_TOKENS:, :]), dim=0)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Extended vocabulary for embedding layer \"\n",
    "                    + \"from {} to {} tokens.\".format(\n",
    "                        NB_TOKENS, NB_TOKENS + extend_embedding\n",
    "                    )\n",
    "                )\n",
    "        try:\n",
    "            model_w.copy_(weight)\n",
    "        except:\n",
    "            print(\n",
    "                \"While copying the weigths named {}, whose dimensions in the model are\"\n",
    "                \" {} and whose dimensions in the saved file are {}, ...\".format(\n",
    "                    key, model_w.size(), weight.size()\n",
    "                )\n",
    "            )\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/finetuning.py\n",
    "\n",
    "\"\"\" Finetuning functions for doing transfer learning to new datasets.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    unicode\n",
    "    IS_PYTHON2 = True\n",
    "except NameError:\n",
    "    unicode = str\n",
    "    IS_PYTHON2 = False\n",
    "\n",
    "\n",
    "def load_benchmark(path, vocab, extend_with=0):\n",
    "    \"\"\"Loads the given benchmark dataset.\n",
    "\n",
    "        Tokenizes the texts using the provided vocabulary, extending it with\n",
    "        words from the training dataset if extend_with > 0. Splits them into\n",
    "        three lists: training, validation and testing (in that order).\n",
    "\n",
    "        Also calculates the maximum length of the texts and the\n",
    "        suggested batch_size.\n",
    "\n",
    "    # Arguments:\n",
    "        path: Path to the dataset to be loaded.\n",
    "        vocab: Vocabulary to be used for tokenizing texts.\n",
    "        extend_with: If > 0, the vocabulary will be extended with up to\n",
    "            extend_with tokens from the training set before tokenizing.\n",
    "\n",
    "    # Returns:\n",
    "        A dictionary with the following fields:\n",
    "            texts: List of three lists, containing tokenized inputs for\n",
    "                training, validation and testing (in that order).\n",
    "            labels: List of three lists, containing labels for training,\n",
    "                validation and testing (in that order).\n",
    "            added: Number of tokens added to the vocabulary.\n",
    "            batch_size: Batch size.\n",
    "            maxlen: Maximum length of an input.\n",
    "    \"\"\"\n",
    "    # Pre-processing dataset\n",
    "    with open(path, \"rb\") as dataset:\n",
    "        if IS_PYTHON2:\n",
    "            data = pickle.load(dataset)\n",
    "        else:\n",
    "            data = pickle.load(dataset, fix_imports=True)\n",
    "\n",
    "    # Decode data\n",
    "    try:\n",
    "        texts = [unicode(x) for x in data[\"texts\"]]\n",
    "    except UnicodeDecodeError:\n",
    "        texts = [x.decode(\"utf-8\") for x in data[\"texts\"]]\n",
    "\n",
    "    # Extract labels\n",
    "    labels = [x[\"label\"] for x in data[\"info\"]]\n",
    "\n",
    "    batch_size, maxlen = calculate_batchsize_maxlen(texts)\n",
    "\n",
    "    st = SentenceTokenizer(vocab, maxlen)\n",
    "\n",
    "    # Split up dataset. Extend the existing vocabulary with up to extend_with\n",
    "    # tokens from the training dataset.\n",
    "    texts, labels, added = st.split_train_val_test(\n",
    "        texts,\n",
    "        labels,\n",
    "        [data[\"train_ind\"], data[\"val_ind\"], data[\"test_ind\"]],\n",
    "        extend_with=extend_with,\n",
    "    )\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "        \"added\": added,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"maxlen\": maxlen,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_batchsize_maxlen(texts):\n",
    "    \"\"\"Calculates the maximum length in the provided texts and a suitable\n",
    "        batch size. Rounds up maxlen to the nearest multiple of ten.\n",
    "\n",
    "    # Arguments:\n",
    "        texts: List of inputs.\n",
    "\n",
    "    # Returns:\n",
    "        Batch size,\n",
    "        max length\n",
    "    \"\"\"\n",
    "\n",
    "    def roundup(x):\n",
    "        return int(math.ceil(x / 10.0)) * 10\n",
    "\n",
    "    # Calculate max length of sequences considered\n",
    "    # Adjust batch_size accordingly to prevent GPU overflow\n",
    "    lengths = [len(tokenize(t)) for t in texts]\n",
    "    maxlen = roundup(np.percentile(lengths, 80.0))\n",
    "    batch_size = 250 if maxlen <= 100 else 50\n",
    "    return batch_size, maxlen\n",
    "\n",
    "\n",
    "def freeze_layers(model, unfrozen_types=[], unfrozen_keyword=None):\n",
    "    \"\"\"Freezes all layers in the given model, except for ones that are\n",
    "        explicitly specified to not be frozen.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model whose layers should be modified.\n",
    "        unfrozen_types: List of layer types which shouldn't be frozen.\n",
    "        unfrozen_keyword: Name keywords of layers that shouldn't be frozen.\n",
    "\n",
    "    # Returns:\n",
    "        Model with the selected layers frozen.\n",
    "    \"\"\"\n",
    "    # Get trainable modules\n",
    "    trainable_modules = [\n",
    "        (n, m)\n",
    "        for n, m in model.named_children()\n",
    "        if len([id(p) for p in m.parameters()]) != 0\n",
    "    ]\n",
    "    for name, module in trainable_modules:\n",
    "        trainable = any(typ in str(module) for typ in unfrozen_types) or (\n",
    "            unfrozen_keyword is not None and unfrozen_keyword.lower() in name.lower()\n",
    "        )\n",
    "        change_trainable(module, trainable, verbose=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def change_trainable(module, trainable, verbose=False):\n",
    "    \"\"\"Helper method that freezes or unfreezes a given layer.\n",
    "\n",
    "    # Arguments:\n",
    "        module: Module to be modified.\n",
    "        trainable: Whether the layer should be frozen or unfrozen.\n",
    "        verbose: Verbosity flag.\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Changing MODULE\", module, \"to trainable =\", trainable)\n",
    "    for name, param in module.named_parameters():\n",
    "        if verbose:\n",
    "            print(\"Setting weight\", name, \"to trainable =\", trainable)\n",
    "        param.requires_grad = trainable\n",
    "\n",
    "    if verbose:\n",
    "        action = \"Unfroze\" if trainable else \"Froze\"\n",
    "        if verbose:\n",
    "            print(\"{} {}\".format(action, module))\n",
    "\n",
    "\n",
    "def find_f1_threshold(model, val_gen, test_gen, average=\"binary\"):\n",
    "    \"\"\"Choose a threshold for F1 based on the validation dataset\n",
    "        (see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4442797/\n",
    "        for details on why to find another threshold than simply 0.5)\n",
    "\n",
    "    # Arguments:\n",
    "        model: pyTorch model\n",
    "        val_gen: Validation set dataloader.\n",
    "        test_gen: Testing set dataloader.\n",
    "\n",
    "    # Returns:\n",
    "        F1 score for the given data and\n",
    "        the corresponding F1 threshold\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.01, 0.5, step=0.01)\n",
    "    f1_scores = []\n",
    "\n",
    "    model.eval()\n",
    "    val_out = [(y, model(X)) for X, y in val_gen]\n",
    "    y_val, y_pred_val = (list(t) for t in zip(*val_out))\n",
    "\n",
    "    test_out = [(y, model(X)) for X, y in test_gen]\n",
    "    y_test, y_pred_test = (list(t) for t in zip(*val_out))\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_pred_val_ind = y_pred_val > t\n",
    "        f1_val = f1_score(y_val, y_pred_val_ind, average=average)\n",
    "        f1_scores.append(f1_val)\n",
    "\n",
    "    best_t = thresholds[np.argmax(f1_scores)]\n",
    "    y_pred_ind = y_pred_test > best_t\n",
    "    f1_test = f1_score(y_test, y_pred_ind, average=average)\n",
    "    return f1_test, best_t\n",
    "\n",
    "\n",
    "def finetune(\n",
    "    model,\n",
    "    texts,\n",
    "    labels,\n",
    "    nb_classes,\n",
    "    batch_size,\n",
    "    method,\n",
    "    metric=\"acc\",\n",
    "    epoch_size=5000,\n",
    "    nb_epochs=1000,\n",
    "    embed_l2=1e-6,\n",
    "    verbose=1,\n",
    "    weights_dir=\"./weights\",\n",
    "):\n",
    "    \"\"\"Compiles and finetunes the given pytorch model.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned\n",
    "        texts: List of three lists, containing tokenized inputs for training,\n",
    "            validation and testing (in that order).\n",
    "        labels: List of three lists, containing labels for training,\n",
    "            validation and testing (in that order).\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        batch_size: Batch size.\n",
    "        method: Finetuning method to be used. For available methods, see\n",
    "            FINETUNING_METHODS in global_variables.py.\n",
    "        metric: Evaluation metric to be used. For available metrics, see\n",
    "            FINETUNING_METRICS in global_variables.py.\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs. Doesn't matter much as early stopping is used.\n",
    "        embed_l2: L2 regularization for the embedding layer.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Model after finetuning,\n",
    "        score after finetuning using the provided metric.\n",
    "    \"\"\"\n",
    "\n",
    "    if method not in FINETUNING_METHODS:\n",
    "        raise ValueError(\n",
    "            \"ERROR (finetune): Invalid method parameter. \"\n",
    "            \"Available options: {}\".format(FINETUNING_METHODS)\n",
    "        )\n",
    "    if metric not in FINETUNING_METRICS:\n",
    "        raise ValueError(\n",
    "            \"ERROR (finetune): Invalid metric parameter. \"\n",
    "            \"Available options: {}\".format(FINETUNING_METRICS)\n",
    "        )\n",
    "\n",
    "    train_gen = get_data_loader(\n",
    "        texts[0],\n",
    "        labels[0],\n",
    "        batch_size,\n",
    "        extended_batch_sampler=True,\n",
    "        epoch_size=epoch_size,\n",
    "    )\n",
    "    val_gen = get_data_loader(\n",
    "        texts[1], labels[1], batch_size, extended_batch_sampler=False\n",
    "    )\n",
    "    test_gen = get_data_loader(\n",
    "        texts[2], labels[2], batch_size, extended_batch_sampler=False\n",
    "    )\n",
    "\n",
    "    checkpoint_path = \"{}/torchmoji-checkpoint-{}.bin\".format(\n",
    "        weights_dir, str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    if method in [\"last\", \"new\"]:\n",
    "        lr = 0.001\n",
    "    elif method in [\"full\", \"chain-thaw\"]:\n",
    "        lr = 0.0001\n",
    "\n",
    "    loss_op = nn.BCEWithLogitsLoss() if nb_classes <= 2 else nn.CrossEntropyLoss()\n",
    "\n",
    "    # Freeze layers if using last\n",
    "    if method == \"last\":\n",
    "        model = freeze_layers(model, unfrozen_keyword=\"output_layer\")\n",
    "\n",
    "    # Define optimizer, for chain-thaw we define it later (after freezing)\n",
    "    if method == \"last\":\n",
    "        adam = optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
    "    elif method in [\"full\", \"new\"]:\n",
    "        # Add L2 regulation on embeddings only\n",
    "        embed_params_id = [id(p) for p in model.embed.parameters()]\n",
    "        output_layer_params_id = [id(p) for p in model.output_layer.parameters()]\n",
    "        base_params = [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if id(p) not in embed_params_id\n",
    "            and id(p) not in output_layer_params_id\n",
    "            and p.requires_grad\n",
    "        ]\n",
    "        embed_params = [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if id(p) in embed_params_id and p.requires_grad\n",
    "        ]\n",
    "        output_layer_params = [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if id(p) in output_layer_params_id and p.requires_grad\n",
    "        ]\n",
    "        adam = optim.Adam(\n",
    "            [\n",
    "                {\"params\": base_params},\n",
    "                {\"params\": embed_params, \"weight_decay\": embed_l2},\n",
    "                {\"params\": output_layer_params, \"lr\": 0.001},\n",
    "            ],\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "    # Training\n",
    "    if verbose:\n",
    "        print(\"Method:  {}\".format(method))\n",
    "        print(\"Metric:  {}\".format(metric))\n",
    "        print(\"Classes: {}\".format(nb_classes))\n",
    "\n",
    "    if method == \"chain-thaw\":\n",
    "        result = chain_thaw(\n",
    "            model,\n",
    "            train_gen,\n",
    "            val_gen,\n",
    "            test_gen,\n",
    "            nb_epochs,\n",
    "            checkpoint_path,\n",
    "            loss_op,\n",
    "            embed_l2=embed_l2,\n",
    "            evaluate=metric,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    else:\n",
    "        result = tune_trainable(\n",
    "            model,\n",
    "            loss_op,\n",
    "            adam,\n",
    "            train_gen,\n",
    "            val_gen,\n",
    "            test_gen,\n",
    "            nb_epochs,\n",
    "            checkpoint_path,\n",
    "            evaluate=metric,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    return model, result\n",
    "\n",
    "\n",
    "def tune_trainable(\n",
    "    model,\n",
    "    loss_op,\n",
    "    optim_op,\n",
    "    train_gen,\n",
    "    val_gen,\n",
    "    test_gen,\n",
    "    nb_epochs,\n",
    "    checkpoint_path,\n",
    "    patience=5,\n",
    "    evaluate=\"acc\",\n",
    "    verbose=2,\n",
    "):\n",
    "    \"\"\"Finetunes the given model using the accuracy measure.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned.\n",
    "        nb_classes: Number of classes in the given dataset.\n",
    "        train: Training data, given as a tuple of (inputs, outputs)\n",
    "        val: Validation data, given as a tuple of (inputs, outputs)\n",
    "        test: Testing data, given as a tuple of (inputs, outputs)\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs.\n",
    "        batch_size: Batch size.\n",
    "        checkpoint_weight_path: Filepath where weights will be checkpointed to\n",
    "            during training. This file will be rewritten by the function.\n",
    "        patience: Patience for callback methods.\n",
    "        evaluate: Evaluation method to use. Can be 'acc' or 'weighted_f1'.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Accuracy of the trained model, ONLY if 'evaluate' is set.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Trainable weights: {}\".format(\n",
    "                [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "            )\n",
    "        )\n",
    "        print(\"Training...\")\n",
    "        if evaluate == \"acc\":\n",
    "            print(\n",
    "                \"Evaluation on test set prior training:\",\n",
    "                evaluate_using_acc(model, test_gen),\n",
    "            )\n",
    "        elif evaluate == \"weighted_f1\":\n",
    "            print(\n",
    "                \"Evaluation on test set prior training:\",\n",
    "                evaluate_using_weighted_f1(model, test_gen, val_gen),\n",
    "            )\n",
    "\n",
    "    fit_model(\n",
    "        model,\n",
    "        loss_op,\n",
    "        optim_op,\n",
    "        train_gen,\n",
    "        val_gen,\n",
    "        nb_epochs,\n",
    "        checkpoint_path,\n",
    "        patience,\n",
    "    )\n",
    "\n",
    "    # Reload the best weights found to avoid overfitting\n",
    "    # Wait a bit to allow proper closing of weights file\n",
    "    sleep(1)\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    if verbose >= 2:\n",
    "        print(\"Loaded weights from {}\".format(checkpoint_path))\n",
    "\n",
    "    if evaluate == \"acc\":\n",
    "        return evaluate_using_acc(model, test_gen)\n",
    "    elif evaluate == \"weighted_f1\":\n",
    "        return evaluate_using_weighted_f1(model, test_gen, val_gen)\n",
    "\n",
    "\n",
    "def evaluate_using_weighted_f1(model, test_gen, val_gen):\n",
    "    \"\"\"Evaluation function using macro weighted F1 score.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be evaluated.\n",
    "        X_test: Inputs of the testing set.\n",
    "        y_test: Outputs of the testing set.\n",
    "        X_val: Inputs of the validation set.\n",
    "        y_val: Outputs of the validation set.\n",
    "        batch_size: Batch size.\n",
    "\n",
    "    # Returns:\n",
    "        Weighted F1 score of the given model.\n",
    "    \"\"\"\n",
    "    # Evaluate on test and val data\n",
    "    f1_test, _ = find_f1_threshold(model, test_gen, val_gen, average=\"weighted_f1\")\n",
    "    return f1_test\n",
    "\n",
    "\n",
    "def evaluate_using_acc(model, test_gen):\n",
    "    \"\"\"Evaluation function using accuracy.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be evaluated.\n",
    "        test_gen: Testing data iterator (DataLoader)\n",
    "\n",
    "    # Returns:\n",
    "        Accuracy of the given model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate on test_data\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for i, data in enumerate(test_gen):\n",
    "        x, y = data\n",
    "        outs = model(x)\n",
    "        if model.nb_classes > 2:\n",
    "            pred = torch.max(outs, 1)[1]\n",
    "            acc = accuracy_score(y.squeeze().numpy(), pred.squeeze().numpy())\n",
    "        else:\n",
    "            pred = (outs >= 0).long()\n",
    "            acc = (pred == y).double().sum() / len(pred)\n",
    "        accs.append(acc)\n",
    "    return np.mean(accs)\n",
    "\n",
    "\n",
    "def chain_thaw(\n",
    "    model,\n",
    "    train_gen,\n",
    "    val_gen,\n",
    "    test_gen,\n",
    "    nb_epochs,\n",
    "    checkpoint_path,\n",
    "    loss_op,\n",
    "    patience=5,\n",
    "    initial_lr=0.001,\n",
    "    next_lr=0.0001,\n",
    "    embed_l2=1e-6,\n",
    "    evaluate=\"acc\",\n",
    "    verbose=1,\n",
    "):\n",
    "    \"\"\"Finetunes given model using chain-thaw and evaluates using accuracy.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned.\n",
    "        train: Training data, given as a tuple of (inputs, outputs)\n",
    "        val: Validation data, given as a tuple of (inputs, outputs)\n",
    "        test: Testing data, given as a tuple of (inputs, outputs)\n",
    "        batch_size: Batch size.\n",
    "        loss: Loss function to be used during training.\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs.\n",
    "        checkpoint_weight_path: Filepath where weights will be checkpointed to\n",
    "            during training. This file will be rewritten by the function.\n",
    "        initial_lr: Initial learning rate. Will only be used for the first\n",
    "            training step (i.e. the output_layer layer)\n",
    "        next_lr: Learning rate for every subsequent step.\n",
    "        seed: Random number generator seed.\n",
    "        verbose: Verbosity flag.\n",
    "        evaluate: Evaluation method to use. Can be 'acc' or 'weighted_f1'.\n",
    "\n",
    "    # Returns:\n",
    "        Accuracy of the finetuned model.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Training..\")\n",
    "\n",
    "    # Train using chain-thaw\n",
    "    train_by_chain_thaw(\n",
    "        model,\n",
    "        train_gen,\n",
    "        val_gen,\n",
    "        loss_op,\n",
    "        patience,\n",
    "        nb_epochs,\n",
    "        checkpoint_path,\n",
    "        initial_lr,\n",
    "        next_lr,\n",
    "        embed_l2,\n",
    "        verbose,\n",
    "    )\n",
    "\n",
    "    if evaluate == \"acc\":\n",
    "        return evaluate_using_acc(model, test_gen)\n",
    "    elif evaluate == \"weighted_f1\":\n",
    "        return evaluate_using_weighted_f1(model, test_gen, val_gen)\n",
    "\n",
    "\n",
    "def train_by_chain_thaw(\n",
    "    model,\n",
    "    train_gen,\n",
    "    val_gen,\n",
    "    loss_op,\n",
    "    patience,\n",
    "    nb_epochs,\n",
    "    checkpoint_path,\n",
    "    initial_lr=0.001,\n",
    "    next_lr=0.0001,\n",
    "    embed_l2=1e-6,\n",
    "    verbose=1,\n",
    "):\n",
    "    \"\"\"Finetunes model using the chain-thaw method.\n",
    "\n",
    "    This is done as follows:\n",
    "    1) Freeze every layer except the last (output_layer) layer and train it.\n",
    "    2) Freeze every layer except the first layer and train it.\n",
    "    3) Freeze every layer except the second etc., until the second last layer.\n",
    "    4) Unfreeze all layers and train entire model.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be trained.\n",
    "        train_gen: Training sample generator.\n",
    "        val_data: Validation data.\n",
    "        loss: Loss function to be used.\n",
    "        finetuning_args: Training early stopping and checkpoint saving parameters\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs.\n",
    "        checkpoint_weight_path: Where weight checkpoints should be saved.\n",
    "        batch_size: Batch size.\n",
    "        initial_lr: Initial learning rate. Will only be used for the first\n",
    "            training step (i.e. the output_layer layer)\n",
    "        next_lr: Learning rate for every subsequent step.\n",
    "        verbose: Verbosity flag.\n",
    "    \"\"\"\n",
    "    # Get trainable layers\n",
    "    layers = [m for m in model.children() if len([id(p) for p in m.parameters()]) != 0]\n",
    "\n",
    "    # Bring last layer to front\n",
    "    layers.insert(0, layers.pop(len(layers) - 1))\n",
    "\n",
    "    # Add None to the end to signify finetuning all layers\n",
    "    layers.append(None)\n",
    "\n",
    "    lr = None\n",
    "    # Finetune each layer one by one and finetune all of them at once\n",
    "    # at the end\n",
    "    for layer in layers:\n",
    "        if lr is None:\n",
    "            lr = initial_lr\n",
    "        elif lr == initial_lr:\n",
    "            lr = next_lr\n",
    "\n",
    "        # Freeze all except current layer\n",
    "        for _layer in layers:\n",
    "            if _layer is not None:\n",
    "                trainable = _layer == layer or layer is None\n",
    "                change_trainable(_layer, trainable=trainable, verbose=False)\n",
    "\n",
    "        # Verify we froze the right layers\n",
    "        for _layer in model.children():\n",
    "            assert (\n",
    "                all(p.requires_grad == (_layer == layer) for p in _layer.parameters())\n",
    "                or layer is None\n",
    "            )\n",
    "\n",
    "        if verbose:\n",
    "            if layer is None:\n",
    "                print(\"Finetuning all layers\")\n",
    "            else:\n",
    "                print(\"Finetuning {}\".format(layer))\n",
    "\n",
    "        special_params = [id(p) for p in model.embed.parameters()]\n",
    "        base_params = [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if id(p) not in special_params and p.requires_grad\n",
    "        ]\n",
    "        embed_parameters = [\n",
    "            p for p in model.parameters() if id(p) in special_params and p.requires_grad\n",
    "        ]\n",
    "        adam = optim.Adam(\n",
    "            [\n",
    "                {\"params\": base_params},\n",
    "                {\"params\": embed_parameters, \"weight_decay\": embed_l2},\n",
    "            ],\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "        fit_model(\n",
    "            model,\n",
    "            loss_op,\n",
    "            adam,\n",
    "            train_gen,\n",
    "            val_gen,\n",
    "            nb_epochs,\n",
    "            checkpoint_path,\n",
    "            patience,\n",
    "        )\n",
    "\n",
    "        # Reload the best weights found to avoid overfitting\n",
    "        # Wait a bit to allow proper closing of weights file\n",
    "        sleep(1)\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        if verbose >= 2:\n",
    "            print(\"Loaded weights from {}\".format(checkpoint_path))\n",
    "\n",
    "\n",
    "def calc_loss(loss_op, pred, yv):\n",
    "    if type(loss_op) is nn.CrossEntropyLoss:\n",
    "        return loss_op(pred.squeeze(), yv.squeeze())\n",
    "    else:\n",
    "        return loss_op(pred.squeeze(), yv.squeeze().float())\n",
    "\n",
    "\n",
    "def fit_model(\n",
    "    model, loss_op, optim_op, train_gen, val_gen, epochs, checkpoint_path, patience\n",
    "):\n",
    "    \"\"\"Analog to Keras fit_generator function.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned.\n",
    "        loss_op: loss operation (BCEWithLogitsLoss or CrossEntropy for e.g.)\n",
    "        optim_op: optimization operation (Adam e.g.)\n",
    "        train_gen: Training data iterator (DataLoader)\n",
    "        val_gen: Validation data iterator (DataLoader)\n",
    "        epochs: Number of epochs.\n",
    "        checkpoint_path: Filepath where weights will be checkpointed to\n",
    "            during training. This file will be rewritten by the function.\n",
    "        patience: Patience for callback methods.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Accuracy of the trained model, ONLY if 'evaluate' is set.\n",
    "    \"\"\"\n",
    "    # Save original checkpoint\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    model.eval()\n",
    "    best_loss = np.mean(\n",
    "        [\n",
    "            calc_loss(loss_op, model(Variable(xv)), Variable(yv)).data.cpu().numpy()[0]\n",
    "            for xv, yv in val_gen\n",
    "        ]\n",
    "    )\n",
    "    print(\"original val loss\", best_loss)\n",
    "\n",
    "    epoch_without_impr = 0\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_gen):\n",
    "            X_train, y_train = data\n",
    "            X_train = Variable(X_train, requires_grad=False)\n",
    "            y_train = Variable(y_train, requires_grad=False)\n",
    "            model.train()\n",
    "            optim_op.zero_grad()\n",
    "            output = model(X_train)\n",
    "            loss = calc_loss(loss_op, output, y_train)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(model.parameters(), 1)\n",
    "            optim_op.step()\n",
    "\n",
    "            acc = evaluate_using_acc(model, [(X_train.data, y_train.data)])\n",
    "            print(\n",
    "                \"== Epoch\",\n",
    "                epoch,\n",
    "                \"step\",\n",
    "                i,\n",
    "                \"train loss\",\n",
    "                loss.data.cpu().numpy()[0],\n",
    "                \"train acc\",\n",
    "                acc,\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        acc = evaluate_using_acc(model, val_gen)\n",
    "        print(\"val acc\", acc)\n",
    "\n",
    "        val_loss = np.mean(\n",
    "            [\n",
    "                calc_loss(loss_op, model(Variable(xv)), Variable(yv))\n",
    "                .data.cpu()\n",
    "                .numpy()[0]\n",
    "                for xv, yv in val_gen\n",
    "            ]\n",
    "        )\n",
    "        print(\"val loss\", val_loss)\n",
    "        if best_loss is not None and val_loss >= best_loss:\n",
    "            epoch_without_impr += 1\n",
    "            print(\"No improvement over previous best loss: \", best_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        if best_loss is None or val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(\"Saving model at\", checkpoint_path)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch_without_impr >= patience:\n",
    "            break\n",
    "\n",
    "\n",
    "def get_data_loader(\n",
    "    X_in,\n",
    "    y_in,\n",
    "    batch_size,\n",
    "    extended_batch_sampler=True,\n",
    "    epoch_size=25000,\n",
    "    upsample=False,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"Returns a dataloader that enables larger epochs on small datasets and\n",
    "        has upsampling functionality.\n",
    "\n",
    "    # Arguments:\n",
    "        X_in: Inputs of the given dataset.\n",
    "        y_in: Outputs of the given dataset.\n",
    "        batch_size: Batch size.\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        upsample: Whether upsampling should be done. This flag should only be\n",
    "            set on binary class problems.\n",
    "\n",
    "    # Returns:\n",
    "        DataLoader.\n",
    "    \"\"\"\n",
    "    dataset = DeepMojiDataset(X_in, y_in)\n",
    "\n",
    "    if extended_batch_sampler:\n",
    "        batch_sampler = DeepMojiBatchSampler(\n",
    "            y_in, batch_size, epoch_size=epoch_size, upsample=upsample, seed=seed\n",
    "        )\n",
    "    else:\n",
    "        batch_sampler = BatchSampler(\n",
    "            SequentialSampler(y_in), batch_size, drop_last=False\n",
    "        )\n",
    "\n",
    "    return DataLoader(dataset, batch_sampler=batch_sampler, num_workers=0)\n",
    "\n",
    "\n",
    "class DeepMojiDataset(Dataset):\n",
    "    \"\"\"A simple Dataset class.\n",
    "\n",
    "    # Arguments:\n",
    "        X_in: Inputs of the given dataset.\n",
    "        y_in: Outputs of the given dataset.\n",
    "\n",
    "    # __getitem__ output:\n",
    "        (torch.LongTensor, torch.LongTensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_in, y_in):\n",
    "        # Check if we have Torch.LongTensor inputs (assume Numpy array otherwise)\n",
    "        if not isinstance(X_in, torch.LongTensor):\n",
    "            X_in = torch.from_numpy(X_in.astype(\"int64\")).long()\n",
    "        if not isinstance(y_in, torch.LongTensor):\n",
    "            y_in = torch.from_numpy(y_in.astype(\"int64\")).long()\n",
    "\n",
    "        self.X_in = torch.split(X_in, 1, dim=0)\n",
    "        self.y_in = torch.split(y_in, 1, dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_in)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_in[idx].squeeze(), self.y_in[idx].squeeze()\n",
    "\n",
    "\n",
    "class DeepMojiBatchSampler(object):\n",
    "    \"\"\"A Batch sampler that enables larger epochs on small datasets and\n",
    "        has upsampling functionality.\n",
    "\n",
    "    # Arguments:\n",
    "        y_in: Labels of the dataset.\n",
    "        batch_size: Batch size.\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        upsample: Whether upsampling should be done. This flag should only be\n",
    "            set on binary class problems.\n",
    "        seed: Random number generator seed.\n",
    "\n",
    "    # __iter__ output:\n",
    "        iterator of lists (batches) of indices in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_in, batch_size, epoch_size, upsample, seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch_size = epoch_size\n",
    "        self.upsample = upsample\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        if upsample:\n",
    "            # Should only be used on binary class problems\n",
    "            assert len(y_in.shape) == 1\n",
    "            neg = np.where(y_in.numpy() == 0)[0]\n",
    "            pos = np.where(y_in.numpy() == 1)[0]\n",
    "            assert epoch_size % 2 == 0\n",
    "            samples_pr_class = int(epoch_size / 2)\n",
    "        else:\n",
    "            ind = range(len(y_in))\n",
    "\n",
    "        if not upsample:\n",
    "            # Randomly sample observations in a balanced way\n",
    "            self.sample_ind = np.random.choice(ind, epoch_size, replace=True)\n",
    "        else:\n",
    "            # Randomly sample observations in a balanced way\n",
    "            sample_neg = np.random.choice(neg, samples_pr_class, replace=True)\n",
    "            sample_pos = np.random.choice(pos, samples_pr_class, replace=True)\n",
    "            concat_ind = np.concatenate((sample_neg, sample_pos), axis=0)\n",
    "\n",
    "            # Shuffle to avoid labels being in specific order\n",
    "            # (all negative then positive)\n",
    "            p = np.random.permutation(len(concat_ind))\n",
    "            self.sample_ind = concat_ind[p]\n",
    "\n",
    "            label_dist = np.mean(y_in.numpy()[self.sample_ind])\n",
    "            assert label_dist > 0.45\n",
    "            assert label_dist < 0.55\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Hand-off data using batch_size\n",
    "        for i in range(int(self.epoch_size / self.batch_size)):\n",
    "            start = i * self.batch_size\n",
    "            end = min(start + self.batch_size, self.epoch_size)\n",
    "            yield self.sample_ind[start:end]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Take care of the last (maybe incomplete) batch\n",
    "        return (self.epoch_size + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f608dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/class_avg_finetuning.py\n",
    "\"\"\" Class average finetuning functions. Before using any of these finetuning\n",
    "    functions, ensure that the model is set up with nb_classes=2.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def relabel(y, current_label_nr, nb_classes):\n",
    "    \"\"\"Makes a binary classification for a specific class in a\n",
    "        multi-class dataset.\n",
    "\n",
    "    # Arguments:\n",
    "        y: Outputs to be relabelled.\n",
    "        current_label_nr: Current label number.\n",
    "        nb_classes: Total number of classes.\n",
    "\n",
    "    # Returns:\n",
    "        Relabelled outputs of a given multi-class dataset into a binary\n",
    "        classification dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handling binary classification\n",
    "    if nb_classes == 2 and len(y.shape) == 1:\n",
    "        return y\n",
    "\n",
    "    y_new = np.zeros(len(y))\n",
    "    y_cut = y[:, current_label_nr]\n",
    "    label_pos = np.where(y_cut == 1)[0]\n",
    "    y_new[label_pos] = 1\n",
    "    return y_new\n",
    "\n",
    "\n",
    "def class_avg_finetune(\n",
    "    model,\n",
    "    texts,\n",
    "    labels,\n",
    "    nb_classes,\n",
    "    batch_size,\n",
    "    method,\n",
    "    epoch_size=5000,\n",
    "    nb_epochs=1000,\n",
    "    embed_l2=1e-6,\n",
    "    verbose=True,\n",
    "    weights_dir=\"./weights\",\n",
    "):\n",
    "    \"\"\"Compiles and finetunes the given model.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned\n",
    "        texts: List of three lists, containing tokenized inputs for training,\n",
    "            validation and testing (in that order).\n",
    "        labels: List of three lists, containing labels for training,\n",
    "            validation and testing (in that order).\n",
    "        nb_classes: Number of classes in the dataset.\n",
    "        batch_size: Batch size.\n",
    "        method: Finetuning method to be used. For available methods, see\n",
    "            FINETUNING_METHODS in global_variables.py. Note that the model\n",
    "            should be defined accordingly (see docstring for torchmoji_transfer())\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs. Doesn't matter much as early stopping is used.\n",
    "        embed_l2: L2 regularization for the embedding layer.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Model after finetuning,\n",
    "        score after finetuning using the class average F1 metric.\n",
    "    \"\"\"\n",
    "\n",
    "    if method not in FINETUNING_METHODS:\n",
    "        raise ValueError(\n",
    "            \"ERROR (class_avg_tune_trainable): \"\n",
    "            \"Invalid method parameter. \"\n",
    "            \"Available options: {}\".format(FINETUNING_METHODS)\n",
    "        )\n",
    "\n",
    "    (X_train, y_train) = (texts[0], labels[0])\n",
    "    (X_val, y_val) = (texts[1], labels[1])\n",
    "    (X_test, y_test) = (texts[2], labels[2])\n",
    "\n",
    "    checkpoint_path = \"{}/torchmoji-checkpoint-{}.bin\".format(\n",
    "        weights_dir, str(uuid.uuid4())\n",
    "    )\n",
    "\n",
    "    f1_init_path = \"{}/torchmoji-f1-init-{}.bin\".format(weights_dir, str(uuid.uuid4()))\n",
    "\n",
    "    if method in [\"last\", \"new\"]:\n",
    "        lr = 0.001\n",
    "    elif method in [\"full\", \"chain-thaw\"]:\n",
    "        lr = 0.0001\n",
    "\n",
    "    loss_op = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Freeze layers if using last\n",
    "    if method == \"last\":\n",
    "        model = freeze_layers(model, unfrozen_keyword=\"output_layer\")\n",
    "\n",
    "    # Define optimizer, for chain-thaw we define it later (after freezing)\n",
    "    if method == \"last\":\n",
    "        adam = optim.Adam((p for p in model.parameters() if p.requires_grad), lr=lr)\n",
    "    elif method in [\"full\", \"new\"]:\n",
    "        # Add L2 regulation on embeddings only\n",
    "        special_params = [id(p) for p in model.embed.parameters()]\n",
    "        base_params = [\n",
    "            p\n",
    "            for p in model.parameters()\n",
    "            if id(p) not in special_params and p.requires_grad\n",
    "        ]\n",
    "        embed_parameters = [\n",
    "            p for p in model.parameters() if id(p) in special_params and p.requires_grad\n",
    "        ]\n",
    "        adam = optim.Adam(\n",
    "            [\n",
    "                {\"params\": base_params},\n",
    "                {\"params\": embed_parameters, \"weight_decay\": embed_l2},\n",
    "            ],\n",
    "            lr=lr,\n",
    "        )\n",
    "\n",
    "    # Training\n",
    "    if verbose:\n",
    "        print(\"Method:  {}\".format(method))\n",
    "        print(\"Classes: {}\".format(nb_classes))\n",
    "\n",
    "    if method == \"chain-thaw\":\n",
    "        result = class_avg_chainthaw(\n",
    "            model,\n",
    "            nb_classes=nb_classes,\n",
    "            loss_op=loss_op,\n",
    "            train=(X_train, y_train),\n",
    "            val=(X_val, y_val),\n",
    "            test=(X_test, y_test),\n",
    "            batch_size=batch_size,\n",
    "            epoch_size=epoch_size,\n",
    "            nb_epochs=nb_epochs,\n",
    "            checkpoint_weight_path=checkpoint_path,\n",
    "            f1_init_weight_path=f1_init_path,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    else:\n",
    "        result = class_avg_tune_trainable(\n",
    "            model,\n",
    "            nb_classes=nb_classes,\n",
    "            loss_op=loss_op,\n",
    "            optim_op=adam,\n",
    "            train=(X_train, y_train),\n",
    "            val=(X_val, y_val),\n",
    "            test=(X_test, y_test),\n",
    "            epoch_size=epoch_size,\n",
    "            nb_epochs=nb_epochs,\n",
    "            batch_size=batch_size,\n",
    "            init_weight_path=f1_init_path,\n",
    "            checkpoint_weight_path=checkpoint_path,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    return model, result\n",
    "\n",
    "\n",
    "def prepare_labels(y_train, y_val, y_test, iter_i, nb_classes):\n",
    "    # Relabel into binary classification\n",
    "    y_train_new = relabel(y_train, iter_i, nb_classes)\n",
    "    y_val_new = relabel(y_val, iter_i, nb_classes)\n",
    "    y_test_new = relabel(y_test, iter_i, nb_classes)\n",
    "    return y_train_new, y_val_new, y_test_new\n",
    "\n",
    "\n",
    "def prepare_generators(X_train, y_train_new, X_val, y_val_new, batch_size, epoch_size):\n",
    "    # Create sample generators\n",
    "    # Make a fixed validation set to avoid fluctuations in validation\n",
    "    train_gen = get_data_loader(\n",
    "        X_train, y_train_new, batch_size, extended_batch_sampler=True\n",
    "    )\n",
    "    val_gen = get_data_loader(X_val, y_val_new, epoch_size, extended_batch_sampler=True)\n",
    "    X_val_resamp, y_val_resamp = next(iter(val_gen))\n",
    "    return train_gen, X_val_resamp, y_val_resamp\n",
    "\n",
    "\n",
    "def class_avg_tune_trainable(\n",
    "    model,\n",
    "    nb_classes,\n",
    "    loss_op,\n",
    "    optim_op,\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    epoch_size,\n",
    "    nb_epochs,\n",
    "    batch_size,\n",
    "    init_weight_path,\n",
    "    checkpoint_weight_path,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Finetunes the given model using the F1 measure.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned.\n",
    "        nb_classes: Number of classes in the given dataset.\n",
    "        train: Training data, given as a tuple of (inputs, outputs)\n",
    "        val: Validation data, given as a tuple of (inputs, outputs)\n",
    "        test: Testing data, given as a tuple of (inputs, outputs)\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs.\n",
    "        batch_size: Batch size.\n",
    "        init_weight_path: Filepath where weights will be initially saved before\n",
    "            training each class. This file will be rewritten by the function.\n",
    "        checkpoint_weight_path: Filepath where weights will be checkpointed to\n",
    "            during training. This file will be rewritten by the function.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        F1 score of the trained model\n",
    "    \"\"\"\n",
    "    total_f1 = 0\n",
    "    nb_iter = nb_classes if nb_classes > 2 else 1\n",
    "\n",
    "    # Unpack args\n",
    "    X_train, y_train = train\n",
    "    X_val, y_val = val\n",
    "    X_test, y_test = test\n",
    "\n",
    "    # Save and reload initial weights after running for\n",
    "    # each class to avoid learning across classes\n",
    "    torch.save(model.state_dict(), init_weight_path)\n",
    "    for i in range(nb_iter):\n",
    "        if verbose:\n",
    "            print(\"Iteration number {}/{}\".format(i + 1, nb_iter))\n",
    "\n",
    "        model.load_state_dict(torch.load(init_weight_path))\n",
    "        y_train_new, y_val_new, y_test_new = prepare_labels(\n",
    "            y_train, y_val, y_test, i, nb_classes\n",
    "        )\n",
    "        train_gen, X_val_resamp, y_val_resamp = prepare_generators(\n",
    "            X_train, y_train_new, X_val, y_val_new, batch_size, epoch_size\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training..\")\n",
    "        fit_model(\n",
    "            model,\n",
    "            loss_op,\n",
    "            optim_op,\n",
    "            train_gen,\n",
    "            [(X_val_resamp, y_val_resamp)],\n",
    "            nb_epochs,\n",
    "            checkpoint_weight_path,\n",
    "            patience,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Reload the best weights found to avoid overfitting\n",
    "        # Wait a bit to allow proper closing of weights file\n",
    "        sleep(1)\n",
    "        model.load_state_dict(torch.load(checkpoint_weight_path))\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_val = model(X_val).cpu().numpy()\n",
    "        y_pred_test = model(X_test).cpu().numpy()\n",
    "\n",
    "        f1_test, best_t = find_f1_threshold(\n",
    "            y_val_new, y_pred_val, y_test_new, y_pred_test\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"f1_test: {}\".format(f1_test))\n",
    "            print(\"best_t:  {}\".format(best_t))\n",
    "        total_f1 += f1_test\n",
    "\n",
    "    return total_f1 / nb_iter\n",
    "\n",
    "\n",
    "def class_avg_chainthaw(\n",
    "    model,\n",
    "    nb_classes,\n",
    "    loss_op,\n",
    "    train,\n",
    "    val,\n",
    "    test,\n",
    "    batch_size,\n",
    "    epoch_size,\n",
    "    nb_epochs,\n",
    "    checkpoint_weight_path,\n",
    "    f1_init_weight_path,\n",
    "    patience=5,\n",
    "    initial_lr=0.001,\n",
    "    next_lr=0.0001,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Finetunes given model using chain-thaw and evaluates using F1.\n",
    "        For a dataset with multiple classes, the model is trained once for\n",
    "        each class, relabeling those classes into a binary classification task.\n",
    "        The result is an average of all F1 scores for each class.\n",
    "\n",
    "    # Arguments:\n",
    "        model: Model to be finetuned.\n",
    "        nb_classes: Number of classes in the given dataset.\n",
    "        train: Training data, given as a tuple of (inputs, outputs)\n",
    "        val: Validation data, given as a tuple of (inputs, outputs)\n",
    "        test: Testing data, given as a tuple of (inputs, outputs)\n",
    "        batch_size: Batch size.\n",
    "        loss: Loss function to be used during training.\n",
    "        epoch_size: Number of samples in an epoch.\n",
    "        nb_epochs: Number of epochs.\n",
    "        checkpoint_weight_path: Filepath where weights will be checkpointed to\n",
    "            during training. This file will be rewritten by the function.\n",
    "        f1_init_weight_path: Filepath where weights will be saved to and\n",
    "            reloaded from before training each class. This ensures that\n",
    "            each class is trained independently. This file will be rewritten.\n",
    "        initial_lr: Initial learning rate. Will only be used for the first\n",
    "            training step (i.e. the softmax layer)\n",
    "        next_lr: Learning rate for every subsequent step.\n",
    "        seed: Random number generator seed.\n",
    "        verbose: Verbosity flag.\n",
    "\n",
    "    # Returns:\n",
    "        Averaged F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack args\n",
    "    X_train, y_train = train\n",
    "    X_val, y_val = val\n",
    "    X_test, y_test = test\n",
    "\n",
    "    total_f1 = 0\n",
    "    nb_iter = nb_classes if nb_classes > 2 else 1\n",
    "\n",
    "    torch.save(model.state_dict(), f1_init_weight_path)\n",
    "\n",
    "    for i in range(nb_iter):\n",
    "        if verbose:\n",
    "            print(\"Iteration number {}/{}\".format(i + 1, nb_iter))\n",
    "\n",
    "        model.load_state_dict(torch.load(f1_init_weight_path))\n",
    "        y_train_new, y_val_new, y_test_new = prepare_labels(\n",
    "            y_train, y_val, y_test, i, nb_classes\n",
    "        )\n",
    "        train_gen, X_val_resamp, y_val_resamp = prepare_generators(\n",
    "            X_train, y_train_new, X_val, y_val_new, batch_size, epoch_size\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training..\")\n",
    "\n",
    "        # Train using chain-thaw\n",
    "        train_by_chain_thaw(\n",
    "            model=model,\n",
    "            train_gen=train_gen,\n",
    "            val_gen=[(X_val_resamp, y_val_resamp)],\n",
    "            loss_op=loss_op,\n",
    "            patience=patience,\n",
    "            nb_epochs=nb_epochs,\n",
    "            checkpoint_path=checkpoint_weight_path,\n",
    "            initial_lr=initial_lr,\n",
    "            next_lr=next_lr,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred_val = model(X_val).cpu().numpy()\n",
    "        y_pred_test = model(X_test).cpu().numpy()\n",
    "\n",
    "        f1_test, best_t = find_f1_threshold(\n",
    "            y_val_new, y_pred_val, y_test_new, y_pred_test\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"f1_test: {}\".format(f1_test))\n",
    "            print(\"best_t:  {}\".format(best_t))\n",
    "        total_f1 += f1_test\n",
    "\n",
    "    return total_f1 / nb_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/filter_input.py\n",
    "\n",
    "\n",
    "def read_english(path=\"english_words.txt\", add_emojis=True):\n",
    "    # read english words for filtering (includes emojis as part of set)\n",
    "    english = set()\n",
    "    with codecs.open(path, \"r\", \"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip().lower().replace(\"\\n\", \"\")\n",
    "            if len(line):\n",
    "                english.add(line)\n",
    "    if add_emojis:\n",
    "        for e in UNICODE_EMOJI:\n",
    "            english.add(e)\n",
    "    return english\n",
    "\n",
    "\n",
    "def read_wanted_emojis(path=\"wanted_emojis.csv\"):\n",
    "    emojis = []\n",
    "    with open(path, \"rb\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for line in reader:\n",
    "            line = line[0].strip().replace(\"\\n\", \"\")\n",
    "            line = line.decode(\"unicode-escape\")\n",
    "            emojis.append(line)\n",
    "    return emojis\n",
    "\n",
    "\n",
    "def read_non_english_users(path=\"unwanted_users.npz\"):\n",
    "    try:\n",
    "        neu_set = set(np.load(path)[\"userids\"])\n",
    "    except IOError:\n",
    "        neu_set = set()\n",
    "    return neu_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/filter_utils.py\n",
    "\n",
    "try:\n",
    "    unichr  # Python 2\n",
    "except NameError:\n",
    "    unichr = chr  # Python 3\n",
    "\n",
    "\n",
    "AtMentionRegex = re.compile(RE_MENTION)\n",
    "urlRegex = re.compile(RE_URL)\n",
    "\n",
    "# from http://bit.ly/2rdjgjE (UTF-8 encodings and Unicode chars)\n",
    "VARIATION_SELECTORS = [\n",
    "    \"\\ufe00\",\n",
    "    \"\\ufe01\",\n",
    "    \"\\ufe02\",\n",
    "    \"\\ufe03\",\n",
    "    \"\\ufe04\",\n",
    "    \"\\ufe05\",\n",
    "    \"\\ufe06\",\n",
    "    \"\\ufe07\",\n",
    "    \"\\ufe08\",\n",
    "    \"\\ufe09\",\n",
    "    \"\\ufe0a\",\n",
    "    \"\\ufe0b\",\n",
    "    \"\\ufe0c\",\n",
    "    \"\\ufe0d\",\n",
    "    \"\\ufe0e\",\n",
    "    \"\\ufe0f\",\n",
    "]\n",
    "\n",
    "# from https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python\n",
    "ALL_CHARS = (unichr(i) for i in range(sys.maxunicode))\n",
    "CONTROL_CHARS = \"\".join(map(unichr, list(range(0, 32)) + list(range(127, 160))))\n",
    "CONTROL_CHAR_REGEX = re.compile(\"[%s]\" % re.escape(CONTROL_CHARS))\n",
    "\n",
    "\n",
    "def is_special_token(word):\n",
    "    equal = False\n",
    "    for spec in SPECIAL_TOKENS:\n",
    "        if word == spec:\n",
    "            equal = True\n",
    "            break\n",
    "    return equal\n",
    "\n",
    "\n",
    "def mostly_english(\n",
    "    words,\n",
    "    english,\n",
    "    pct_eng_short=0.5,\n",
    "    pct_eng_long=0.6,\n",
    "    ignore_special_tokens=True,\n",
    "    min_length=2,\n",
    "):\n",
    "    \"\"\"Ensure text meets threshold for containing English words\"\"\"\n",
    "\n",
    "    n_words = 0\n",
    "    n_english = 0\n",
    "\n",
    "    if english is None:\n",
    "        return True, 0, 0\n",
    "\n",
    "    for w in words:\n",
    "        if len(w) < min_length:\n",
    "            continue\n",
    "        if punct_word(w):\n",
    "            continue\n",
    "        if ignore_special_tokens and is_special_token(w):\n",
    "            continue\n",
    "        n_words += 1\n",
    "        if w in english:\n",
    "            n_english += 1\n",
    "\n",
    "    if n_words < 2:\n",
    "        return True, n_words, n_english\n",
    "    if n_words < 5:\n",
    "        valid_english = n_english >= n_words * pct_eng_short\n",
    "    else:\n",
    "        valid_english = n_english >= n_words * pct_eng_long\n",
    "    return valid_english, n_words, n_english\n",
    "\n",
    "\n",
    "def correct_length(words, min_words, max_words, ignore_special_tokens=True):\n",
    "    \"\"\"Ensure text meets threshold for containing English words\n",
    "    and that it's within the min and max words limits.\"\"\"\n",
    "\n",
    "    if min_words is None:\n",
    "        min_words = 0\n",
    "\n",
    "    if max_words is None:\n",
    "        max_words = 99999\n",
    "\n",
    "    n_words = 0\n",
    "    for w in words:\n",
    "        if punct_word(w):\n",
    "            continue\n",
    "        if ignore_special_tokens and is_special_token(w):\n",
    "            continue\n",
    "        n_words += 1\n",
    "    valid = min_words <= n_words and n_words <= max_words\n",
    "    return valid\n",
    "\n",
    "\n",
    "def punct_word(word, punctuation=string.punctuation):\n",
    "    return all([True if c in punctuation else False for c in word])\n",
    "\n",
    "\n",
    "def load_non_english_user_set():\n",
    "    non_english_user_set = set(np.load(\"uids.npz\")[\"data\"])\n",
    "    return non_english_user_set\n",
    "\n",
    "\n",
    "def non_english_user(userid, non_english_user_set):\n",
    "    neu_found = int(userid) in non_english_user_set\n",
    "    return neu_found\n",
    "\n",
    "\n",
    "def separate_emojis_and_text(text):\n",
    "    emoji_chars = []\n",
    "    non_emoji_chars = []\n",
    "    for c in text:\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            emoji_chars.append(c)\n",
    "        else:\n",
    "            non_emoji_chars.append(c)\n",
    "    return \"\".join(emoji_chars), \"\".join(non_emoji_chars)\n",
    "\n",
    "\n",
    "def extract_emojis(text, wanted_emojis):\n",
    "    text = remove_variation_selectors(text)\n",
    "    return [c for c in text if c in wanted_emojis]\n",
    "\n",
    "\n",
    "def remove_variation_selectors(text):\n",
    "    \"\"\"Remove styling glyph variants for Unicode characters.\n",
    "    For instance, remove skin color from emojis.\n",
    "    \"\"\"\n",
    "    for var in VARIATION_SELECTORS:\n",
    "        text = text.replace(var, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def shorten_word(word):\n",
    "    \"\"\"Shorten groupings of 3+ identical consecutive chars to 2, e.g. '!!!!' --> '!!'\"\"\"\n",
    "\n",
    "    # only shorten ASCII words\n",
    "    try:\n",
    "        word.decode(\"ascii\")\n",
    "    except (UnicodeDecodeError, UnicodeEncodeError, AttributeError) as e:\n",
    "        return word\n",
    "\n",
    "    # must have at least 3 char to be shortened\n",
    "    if len(word) < 3:\n",
    "        return word\n",
    "\n",
    "    # find groups of 3+ consecutive letters\n",
    "    letter_groups = [list(g) for k, g in groupby(word)]\n",
    "    triple_or_more = [\"\".join(g) for g in letter_groups if len(g) >= 3]\n",
    "    if len(triple_or_more) == 0:\n",
    "        return word\n",
    "\n",
    "    # replace letters to find the short word\n",
    "    short_word = word\n",
    "    for trip in triple_or_more:\n",
    "        short_word = short_word.replace(trip, trip[0] * 2)\n",
    "\n",
    "    return short_word\n",
    "\n",
    "\n",
    "def detect_special_tokens(word):\n",
    "    try:\n",
    "        int(word)\n",
    "        word = SPECIAL_TOKENS[4]\n",
    "    except ValueError:\n",
    "        if AtMentionRegex.findall(word):\n",
    "            word = SPECIAL_TOKENS[2]\n",
    "        elif urlRegex.findall(word):\n",
    "            word = SPECIAL_TOKENS[3]\n",
    "    return word\n",
    "\n",
    "\n",
    "def process_word(word):\n",
    "    \"\"\"Shortening and converting the word to a special token if relevant.\"\"\"\n",
    "    word = shorten_word(word)\n",
    "    word = detect_special_tokens(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def remove_control_chars(text):\n",
    "    return CONTROL_CHAR_REGEX.sub(\"\", text)\n",
    "\n",
    "\n",
    "def convert_nonbreaking_space(text):\n",
    "    # ugly hack handling non-breaking space no matter how badly it's been encoded in the input\n",
    "    for r in [\"\\\\\\\\xc2\", \"\\\\xc2\", \"\\xc2\", \"\\\\\\\\xa0\", \"\\\\xa0\", \"\\xa0\"]:\n",
    "        text = text.replace(r, \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_linebreaks(text):\n",
    "    # ugly hack handling non-breaking space no matter how badly it's been encoded in the input\n",
    "    # space around to ensure proper tokenization\n",
    "    for r in [\"\\\\\\\\n\", \"\\\\n\", \"\\n\", \"\\\\\\\\r\", \"\\\\r\", \"\\r\", \"<br>\"]:\n",
    "        text = text.replace(r, \" \" + SPECIAL_TOKENS[5] + \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# torchmoji/word_generator.py\n",
    "\n",
    "\"\"\" Extracts lists of words from a given input to be used for later vocabulary\n",
    "    generation or for creating tokenized datasets.\n",
    "    Supports functionality for handling different file types and\n",
    "    filtering/processing of this input.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    unicode  # Python 2\n",
    "except NameError:\n",
    "    unicode = str  # Python 3\n",
    "\n",
    "# Only catch retweets in the beginning of the tweet as those are the\n",
    "# automatically added ones.\n",
    "# We do not want to remove tweets like \"Omg.. please RT this!!\"\n",
    "RETWEETS_RE = re.compile(r\"^[rR][tT]\")\n",
    "\n",
    "# Use fast and less precise regex for removing tweets with URLs\n",
    "# It doesn't matter too much if a few tweets with URL's make it through\n",
    "URLS_RE = re.compile(r\"https?://|www\\.\")\n",
    "\n",
    "MENTION_RE = re.compile(RE_MENTION)\n",
    "ALLOWED_CONVERTED_UNICODE_PUNCTUATION = \"\"\"!\"#$'()+,-.:;<=>?@`~\"\"\"\n",
    "\n",
    "\n",
    "class WordGenerator:\n",
    "    \"\"\"Cleanses input and converts into words. Needs all sentences to be in\n",
    "        Unicode format. Has subclasses that read sentences differently based on\n",
    "        file type.\n",
    "\n",
    "    Takes a generator as input. This can be from e.g. a file.\n",
    "    unicode_handling in ['ignore_sentence', 'convert_punctuation', 'allow']\n",
    "    unicode_handling in ['ignore_emoji', 'ignore_sentence', 'allow']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stream,\n",
    "        allow_unicode_text=False,\n",
    "        ignore_emojis=True,\n",
    "        remove_variation_selectors=True,\n",
    "        break_replacement=True,\n",
    "    ):\n",
    "        self.stream = stream\n",
    "        self.allow_unicode_text = allow_unicode_text\n",
    "        self.remove_variation_selectors = remove_variation_selectors\n",
    "        self.ignore_emojis = ignore_emojis\n",
    "        self.break_replacement = break_replacement\n",
    "        self.reset_stats()\n",
    "\n",
    "    def get_words(self, sentence):\n",
    "        \"\"\"Tokenizes a sentence into individual words.\n",
    "        Converts Unicode punctuation into ASCII if that option is set.\n",
    "        Ignores sentences with Unicode if that option is set.\n",
    "        Returns an empty list of words if the sentence has Unicode and\n",
    "        that is not allowed.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(sentence, unicode):\n",
    "            raise ValueError(\"All sentences should be Unicode-encoded!\")\n",
    "        sentence = sentence.strip().lower()\n",
    "\n",
    "        if self.break_replacement:\n",
    "            sentence = convert_linebreaks(sentence)\n",
    "\n",
    "        if self.remove_variation_selectors:\n",
    "            sentence = remove_variation_selectors(sentence)\n",
    "\n",
    "        # Split into words using simple whitespace splitting and convert\n",
    "        # Unicode. This is done to prevent word splitting issues with\n",
    "        # twokenize and Unicode\n",
    "        words = sentence.split()\n",
    "        converted_words = []\n",
    "        for w in words:\n",
    "            accept_sentence, c_w = self.convert_unicode_word(w)\n",
    "            # Unicode word detected and not allowed\n",
    "            if not accept_sentence:\n",
    "                return []\n",
    "            else:\n",
    "                converted_words.append(c_w)\n",
    "        sentence = \" \".join(converted_words)\n",
    "\n",
    "        words = tokenize(sentence)\n",
    "        words = [process_word(w) for w in words]\n",
    "        return words\n",
    "\n",
    "    def check_ascii(self, word):\n",
    "        \"\"\"Returns whether a word is ASCII\"\"\"\n",
    "\n",
    "        try:\n",
    "            word.decode(\"ascii\")\n",
    "            return True\n",
    "        except (UnicodeDecodeError, UnicodeEncodeError, AttributeError):\n",
    "            return False\n",
    "\n",
    "    def convert_unicode_punctuation(self, word):\n",
    "        word_converted_punct = []\n",
    "        for c in word:\n",
    "            decoded_c = unidecode(c).lower()\n",
    "            if len(decoded_c) == 0:\n",
    "                # Cannot decode to anything reasonable\n",
    "                word_converted_punct.append(c)\n",
    "            else:\n",
    "                # Check if all punctuation and therefore fine\n",
    "                # to include unidecoded version\n",
    "                allowed_punct = punct_word(\n",
    "                    decoded_c, punctuation=ALLOWED_CONVERTED_UNICODE_PUNCTUATION\n",
    "                )\n",
    "\n",
    "                if allowed_punct:\n",
    "                    word_converted_punct.append(decoded_c)\n",
    "                else:\n",
    "                    word_converted_punct.append(c)\n",
    "        return \"\".join(word_converted_punct)\n",
    "\n",
    "    def convert_unicode_word(self, word):\n",
    "        \"\"\"Converts Unicode words to ASCII using unidecode. If Unicode is not\n",
    "        allowed (set as a variable during initialization), then only\n",
    "        punctuation that can be converted to ASCII will be allowed.\n",
    "        \"\"\"\n",
    "        if self.check_ascii(word):\n",
    "            return True, word\n",
    "\n",
    "        # First we ensure that the Unicode is normalized so it's\n",
    "        # always a single character.\n",
    "        word = unicodedata.normalize(\"NFKC\", word)\n",
    "\n",
    "        # Convert Unicode punctuation to ASCII equivalent. We want\n",
    "        # e.g. \"\\u203c\" (double exclamation mark) to be treated the same\n",
    "        # as \"!!\" no matter if we allow other Unicode characters or not.\n",
    "        word = self.convert_unicode_punctuation(word)\n",
    "\n",
    "        if self.ignore_emojis:\n",
    "            _, word = separate_emojis_and_text(word)\n",
    "\n",
    "        # If conversion of punctuation and removal of emojis took care\n",
    "        # of all the Unicode or if we allow Unicode then everything is fine\n",
    "        if self.check_ascii(word) or self.allow_unicode_text:\n",
    "            return True, word\n",
    "        else:\n",
    "            # Sometimes we might want to simply ignore Unicode sentences\n",
    "            # (e.g. for vocabulary creation). This is another way to prevent\n",
    "            # \"polution\" of strange Unicode tokens from low quality datasets\n",
    "            return False, \"\"\n",
    "\n",
    "    def data_preprocess_filtering(self, line, iter_i):\n",
    "        \"\"\"To be overridden with specific preprocessing/filtering behavior\n",
    "        if desired.\n",
    "\n",
    "        Returns a boolean of whether the line should be accepted and the\n",
    "        preprocessed text.\n",
    "\n",
    "        Runs prior to tokenization.\n",
    "        \"\"\"\n",
    "        return True, line, {}\n",
    "\n",
    "    def data_postprocess_filtering(self, words, iter_i):\n",
    "        \"\"\"To be overridden with specific postprocessing/filtering behavior\n",
    "        if desired.\n",
    "\n",
    "        Returns a boolean of whether the line should be accepted and the\n",
    "        postprocessed text.\n",
    "\n",
    "        Runs after tokenization.\n",
    "        \"\"\"\n",
    "        return True, words, {}\n",
    "\n",
    "    def extract_valid_sentence_words(self, line):\n",
    "        \"\"\"Line may either a string of a list of strings depending on how\n",
    "        the stream is being parsed.\n",
    "        Domain-specific processing and filtering can be done both prior to\n",
    "        and after tokenization.\n",
    "        Custom information about the line can be extracted during the\n",
    "        processing phases and returned as a dict.\n",
    "        \"\"\"\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        pre_valid, pre_line, pre_info = self.data_preprocess_filtering(\n",
    "            line, self.stats[\"total\"]\n",
    "        )\n",
    "        info.update(pre_info)\n",
    "        if not pre_valid:\n",
    "            self.stats[\"pretokenization_filtered\"] += 1\n",
    "            return False, [], info\n",
    "\n",
    "        words = self.get_words(pre_line)\n",
    "        if len(words) == 0:\n",
    "            self.stats[\"unicode_filtered\"] += 1\n",
    "            return False, [], info\n",
    "\n",
    "        post_valid, post_words, post_info = self.data_postprocess_filtering(\n",
    "            words, self.stats[\"total\"]\n",
    "        )\n",
    "        info.update(post_info)\n",
    "        if not post_valid:\n",
    "            self.stats[\"posttokenization_filtered\"] += 1\n",
    "        return post_valid, post_words, info\n",
    "\n",
    "    def generate_array_from_input(self):\n",
    "        sentences = []\n",
    "        for words in self:\n",
    "            sentences.append(words)\n",
    "        return sentences\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.stats = {\n",
    "            \"pretokenization_filtered\": 0,\n",
    "            \"unicode_filtered\": 0,\n",
    "            \"posttokenization_filtered\": 0,\n",
    "            \"total\": 0,\n",
    "            \"valid\": 0,\n",
    "        }\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.stream is None:\n",
    "            raise ValueError(\"Stream should be set before iterating over it!\")\n",
    "\n",
    "        for line in self.stream:\n",
    "            valid, words, info = self.extract_valid_sentence_words(line)\n",
    "\n",
    "            # Words may be filtered away due to unidecode etc.\n",
    "            # In that case the words should not be passed on.\n",
    "            if valid and len(words):\n",
    "                self.stats[\"valid\"] += 1\n",
    "                yield words, info\n",
    "\n",
    "            self.stats[\"total\"] += 1\n",
    "\n",
    "\n",
    "class TweetWordGenerator(WordGenerator):\n",
    "    \"\"\"Returns np array or generator of ASCII sentences for given tweet input.\n",
    "    Any file opening/closing should be handled outside of this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stream,\n",
    "        wanted_emojis=None,\n",
    "        english_words=None,\n",
    "        non_english_user_set=None,\n",
    "        allow_unicode_text=False,\n",
    "        ignore_retweets=True,\n",
    "        ignore_url_tweets=True,\n",
    "        ignore_mention_tweets=False,\n",
    "    ):\n",
    "\n",
    "        self.wanted_emojis = wanted_emojis\n",
    "        self.english_words = english_words\n",
    "        self.non_english_user_set = non_english_user_set\n",
    "        self.ignore_retweets = ignore_retweets\n",
    "        self.ignore_url_tweets = ignore_url_tweets\n",
    "        self.ignore_mention_tweets = ignore_mention_tweets\n",
    "        WordGenerator.__init__(self, stream, allow_unicode_text=allow_unicode_text)\n",
    "\n",
    "    def validated_tweet(self, data):\n",
    "        \"\"\"A bunch of checks to determine whether the tweet is valid.\n",
    "        Also returns emojis contained by the tweet.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ordering of validations is important for speed\n",
    "        # If it passes all checks, then the tweet is validated for usage\n",
    "\n",
    "        # Skips incomplete tweets\n",
    "        if len(data) <= 9:\n",
    "            return False, []\n",
    "\n",
    "        text = data[9]\n",
    "\n",
    "        if self.ignore_retweets and RETWEETS_RE.search(text):\n",
    "            return False, []\n",
    "\n",
    "        if self.ignore_url_tweets and URLS_RE.search(text):\n",
    "            return False, []\n",
    "\n",
    "        if self.ignore_mention_tweets and MENTION_RE.search(text):\n",
    "            return False, []\n",
    "\n",
    "        if self.wanted_emojis is not None:\n",
    "            uniq_emojis = np.unique(extract_emojis(text, self.wanted_emojis))\n",
    "            if len(uniq_emojis) == 0:\n",
    "                return False, []\n",
    "        else:\n",
    "            uniq_emojis = []\n",
    "\n",
    "        if self.non_english_user_set is not None and non_english_user(\n",
    "            data[1], self.non_english_user_set\n",
    "        ):\n",
    "            return False, []\n",
    "        return True, uniq_emojis\n",
    "\n",
    "    def data_preprocess_filtering(self, line, iter_i):\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        valid, emojis = self.validated_tweet(fields)\n",
    "        text = (\n",
    "            fields[9].replace(\"\\\\n\", \"\").replace(\"\\\\r\", \"\").replace(\"&amp\", \"&\")\n",
    "            if valid\n",
    "            else \"\"\n",
    "        )\n",
    "        return valid, text, {\"emojis\": emojis}\n",
    "\n",
    "    def data_postprocess_filtering(self, words, iter_i):\n",
    "        valid_length = correct_length(words, 1, None)\n",
    "        valid_english, n_words, n_english = mostly_english(words, self.english_words)\n",
    "        if valid_length and valid_english:\n",
    "            return (\n",
    "                True,\n",
    "                words,\n",
    "                {\n",
    "                    \"length\": len(words),\n",
    "                    \"n_normal_words\": n_words,\n",
    "                    \"n_english\": n_english,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                False,\n",
    "                [],\n",
    "                {\n",
    "                    \"length\": len(words),\n",
    "                    \"n_normal_words\": n_words,\n",
    "                    \"n_english\": n_english,\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TorchMojiInterface:\n",
    "    def __init__(self, vocabulary_file: str, model_path: str, maxlen: int = 30):\n",
    "        with open(vocabulary_file, \"r\") as f:\n",
    "            self.vocabulary = json.load(f)\n",
    "        self.st = SentenceTokenizer(self.vocabulary, maxlen)\n",
    "        self.emoji_model = torchmoji_emojis(model_path)\n",
    "        self.feature_model = torchmoji_feature_encoding(model_path)\n",
    "\n",
    "    def top_elements(self, array, k):\n",
    "        ind = np.argpartition(array, -k)[-k:]\n",
    "        return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "    def top_n_emojis(self, text: List[str], n_emojis: int = 5):\n",
    "        \"\"\"\n",
    "        Returns the top n emojis in the form [\":joy:\",\":unamused:\"]\n",
    "        \"\"\"\n",
    "        tokenized, _, _ = self.st.tokenize_sentences(text)\n",
    "        predicted_probs = self.emoji_model(tokenized)\n",
    "        print(predicted_probs.shape)\n",
    "        emojis = []\n",
    "        for probs in predicted_probs:\n",
    "            emoji_ids = self.top_elements(probs, n_emojis)\n",
    "            emojis.append(list(map(lambda x: EMOJIS[x], emoji_ids)))\n",
    "        return emojis\n",
    "\n",
    "    def encode_texts(self, text: List[str]):\n",
    "        \"\"\"\n",
    "        Returns the encoded embedding for the text input\n",
    "        Output shape: [len(text),2304]\n",
    "        \"\"\"\n",
    "        tokenized, _, _ = self.st.tokenize_sentences(text)\n",
    "        encoding = self.feature_model(tokenized)\n",
    "        return encoding\n",
    "\n",
    "    def blend_text_encodings(self, texts: List[str], weights: List[float]):\n",
    "        \"\"\"\n",
    "        Computes encodings and blends them\n",
    "        \"\"\"\n",
    "        encodings = self.encode_texts(texts)\n",
    "        return self.blend_encodings(encodings, weights)\n",
    "\n",
    "    def blend_encodings(self, encodings, weights):\n",
    "        \"\"\"\n",
    "        Compute weighted average of encodings\n",
    "        \"\"\"\n",
    "        return np.average(encodings, axis=0, weights=weights)\n",
    "\n",
    "    def enc2emojis(self, encodings, n_emojis=5):\n",
    "        predicted_probs = self.emoji_model.output_layer(encodings).detach().numpy()\n",
    "        print(predicted_probs.shape)\n",
    "        emojis = []\n",
    "        for probs in predicted_probs:\n",
    "            emoji_ids = self.top_elements(probs, n_emojis)\n",
    "            emojis.append(list(map(lambda x: EMOJIS[x], emoji_ids)))\n",
    "        return emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cee3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:204: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:206: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:208: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:212: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "import emoji\n",
    "\n",
    "VOCAB_PATH = \"../models/vocabulary.json\"\n",
    "PRETRAINED_PATH = \"../models/pytorch_model.bin\"\n",
    "torchmoji = TorchMojiInterface(VOCAB_PATH, PRETRAINED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "texts = [\"I am the happiest person!\", \"My dog just died.\"]\n",
    "blended1 = torchmoji.blend_text_encodings(texts, np.array([50, 50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f480be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 64)\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "texts = [\n",
    "    \"I am the happiest person!\",\n",
    "    \"My dog just died.\",\n",
    "    \"I love my mother!\",\n",
    "    \"I hate you idiot!\",\n",
    "    \"Staring at the contents of your fridge but never deciding what to eat is a cool way to diet\",\n",
    "    \"This is neutral text.\",\n",
    "]\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "predicted_emojis = torchmoji.top_n_emojis(texts, 5)\n",
    "# print(f\"Finished {len(texts)} in {time.time() - start:.3f}s\")\n",
    "# for text, emojis in zip(texts, predicted_emojis):\n",
    "#     print(emoji.emojize(\"{}:\\n{}\\n\".format(text, \" \".join(emojis)), use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c0e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mldev/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# skip\n",
    "blended_emojis = torchmoji.enc2emojis(\n",
    "    torch.tensor([blended1], requires_grad=False).float()\n",
    ")\n",
    "# print(f\"Blended emojis\\n{texts[0]} and {texts[1]}\")\n",
    "# print(emoji.emojize(f\"{' '.join(blended_emojis[0])}\", use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c59b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "start = time.time()\n",
    "encoding = torchmoji.encode_texts(texts)\n",
    "# for text, vector in zip(texts, encoding):\n",
    "#     magnitude = np.linalg.norm(vector)\n",
    "#     print(f\"{text}:\\n{magnitude}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83043ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "from scipy import spatial\n",
    "\n",
    "similar_result_0 = 1 - spatial.distance.cosine(encoding[0], encoding[1])\n",
    "similar_result_1 = 1 - spatial.distance.cosine(encoding[2], encoding[3])\n",
    "different_result_0 = 1 - spatial.distance.cosine(encoding[0], encoding[2])\n",
    "different_result_1 = 1 - spatial.distance.cosine(encoding[1], encoding[3])\n",
    "\n",
    "# print(\"Cosine Similarity Experiment\")\n",
    "# print(f\"Similar 0:   {similar_result_0:.3f}\")\n",
    "# print(f\"Similar 1:   {similar_result_1:.3f}\")\n",
    "# print(f\"Different 0: {different_result_0:.3f}\")\n",
    "# print(f\"Different 1: {different_result_1:.3f}\")\n",
    "# assert similar_result_0 > different_result_0\n",
    "# assert similar_result_1 > different_result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655f501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
