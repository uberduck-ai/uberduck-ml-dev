{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01281d7d",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c43cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exec.dataset_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Any, Dict\n",
    "import json\n",
    "\n",
    "from g2p_en import G2p\n",
    "import librosa\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mdutils.mdutils import MdUtils\n",
    "import numpy as np\n",
    "from pydub import AudioSegment, silence\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from uberduck_ml_dev.data.statistics import (\n",
    "    AbsoluteMetrics,\n",
    "    count_frequency,\n",
    "    create_wordcloud,\n",
    "    get_sample_format,\n",
    "    pace_character,\n",
    "    pace_phoneme,\n",
    "    word_frequencies,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import clean_text, text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import compute_yin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_summary_statistics(arr):\n",
    "    if len(arr) == 0:\n",
    "        return {}\n",
    "    arr_np = np.array(arr)\n",
    "    return {\n",
    "        \"p10\": float(np.percentile(arr_np, 10)),\n",
    "        \"p25\": float(np.percentile(arr_np, 25)),\n",
    "        \"p50\": float(np.percentile(arr_np, 50)),\n",
    "        \"p75\": float(np.percentile(arr_np, 75)),\n",
    "        \"p90\": float(np.percentile(arr_np, 90)),\n",
    "        \"max\": float(np.max(arr_np)),\n",
    "        \"min\": float(np.min(arr_np)),\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_statistics(\n",
    "    dataset_path, input_file, output_folder, delimiter, metrics=True, wordcloud=True\n",
    "):\n",
    "    n_clips = 0\n",
    "    sample_rates = {}\n",
    "    channels = {\"mono\": 0, \"stereo\": 0}\n",
    "    extensions = {}\n",
    "    sample_formats = {}\n",
    "    total_lengths = []\n",
    "    leading_silence_lengths = []\n",
    "    trailing_silence_lengths = []\n",
    "    paces_characters = []  # number of characters / seconds in audio clip\n",
    "    paces_phonemes = []  # number of phonemes / seconds in audio clip\n",
    "    lookup_results = {\n",
    "        \"RNN\": [],\n",
    "        \"CMU\": [],\n",
    "        \"non-alphanumeric\": [],\n",
    "        \"homograph\": [],\n",
    "    }  # keep track of how arpabet sequences were generated\n",
    "    mosnet_scores = []\n",
    "    srmr_scores = []\n",
    "    word_freqs = []\n",
    "    all_words = []\n",
    "    all_pitches = np.array([])\n",
    "    all_loudness = []\n",
    "\n",
    "    g2p = G2p()\n",
    "    files_with_error = []\n",
    "    if metrics:\n",
    "        abs_metrics = AbsoluteMetrics()\n",
    "\n",
    "    with open(os.path.join(dataset_path, input_file)) as transcripts:\n",
    "        for line in tqdm(transcripts.readlines()):\n",
    "            try:\n",
    "                line = line.strip()  # remove trailing newline character\n",
    "                file, transcription = line.lower().split(delimiter)\n",
    "                transcription_cleaned = clean_text(transcription, [\"english_cleaners\"])\n",
    "\n",
    "                _, file_extension = os.path.splitext(file)\n",
    "                path_to_file = os.path.join(dataset_path, file)\n",
    "                file_pydub = AudioSegment.from_wav(path_to_file)\n",
    "                data_np, _ = librosa.load(path_to_file)\n",
    "\n",
    "                # Format Metadata\n",
    "                sr = file_pydub.frame_rate\n",
    "                if sr in sample_rates.keys():\n",
    "                    sample_rates[sr] += 1\n",
    "                else:\n",
    "                    sample_rates[sr] = 1\n",
    "\n",
    "                if file_pydub.channels == 1:\n",
    "                    channels[\"mono\"] += 1\n",
    "                else:\n",
    "                    channels[\"stereo\"] += 1\n",
    "\n",
    "                if file_extension in extensions.keys():\n",
    "                    extensions[file_extension] += 1\n",
    "                else:\n",
    "                    extensions[file_extension] = 1\n",
    "\n",
    "                fmt = get_sample_format(path_to_file)\n",
    "                if fmt in sample_formats.keys():\n",
    "                    sample_formats[fmt] += 1\n",
    "                else:\n",
    "                    sample_formats[fmt] = 1\n",
    "\n",
    "                # lengths\n",
    "                total_lengths.append(file_pydub.duration_seconds)\n",
    "                leading_silence_lengths.append(\n",
    "                    silence.detect_leading_silence(file_pydub)\n",
    "                )\n",
    "                trailing_silence_lengths.append(\n",
    "                    silence.detect_leading_silence(file_pydub.reverse())\n",
    "                )\n",
    "\n",
    "                # Paces\n",
    "                paces_phonemes.append(\n",
    "                    pace_phoneme(text=transcription_cleaned, audio=path_to_file)\n",
    "                )\n",
    "                paces_characters.append(\n",
    "                    pace_character(text=transcription_cleaned, audio=path_to_file)\n",
    "                )\n",
    "\n",
    "                # Pitch\n",
    "                pitches, harmonic_rates, argmins, times = compute_yin(data_np, sr=sr)\n",
    "                pitches = np.array(pitches)\n",
    "                pitches = pitches[pitches > 10]\n",
    "                all_pitches = np.append(all_pitches, pitches)\n",
    "\n",
    "                # Loudness\n",
    "                all_loudness.append(file_pydub.dBFS)\n",
    "\n",
    "                # Quality\n",
    "                if metrics:\n",
    "                    scores = abs_metrics(path_to_file)\n",
    "                    mosnet_scores.append(scores[\"mosnet\"][0][0])\n",
    "                    srmr_scores.append(scores[\"srmr\"])\n",
    "\n",
    "                # Transcription\n",
    "                word_freqs.extend(word_frequencies(transcription_cleaned))\n",
    "                transcription_lookups = g2p.check_lookup(transcription_cleaned)\n",
    "                for k in transcription_lookups:\n",
    "                    lookup_results[k].extend(transcription_lookups[k])\n",
    "\n",
    "                all_words.append(transcription_cleaned)\n",
    "\n",
    "                n_clips += 1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                files_with_error.append(file)\n",
    "\n",
    "    if n_clips == 0:\n",
    "        return None\n",
    "\n",
    "    if wordcloud:\n",
    "        create_wordcloud(\n",
    "            \" \".join(all_words),\n",
    "            os.path.join(dataset_path, output_folder, \"wordcloud.png\"),\n",
    "        )\n",
    "\n",
    "    # Length graph\n",
    "    plt.clf()\n",
    "    sns.histplot(total_lengths)\n",
    "    plt.title(\"Audio length distribution\")\n",
    "    plt.xlabel(\"Audio length (s)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"lengths.png\"))\n",
    "\n",
    "    # Word Frequencies graph\n",
    "    plt.clf()\n",
    "    sns.histplot(word_freqs, bins=10)\n",
    "    plt.title(\"Word frequency distribution [0-1]\")\n",
    "    plt.xlabel(\"Word frequency\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"word_frequencies.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Pitches graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(all_pitches)\n",
    "    plt.title(\"Pitch distribution\")\n",
    "    plt.xlabel(\"Fundamental Frequency (Hz)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(all_loudness)\n",
    "    plt.title(\"Loudness distribution\")\n",
    "    plt.xlabel(\"Loudness (dBFS)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"pitch_loudness.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Silences graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(leading_silence_lengths)\n",
    "    plt.title(\"Leading silence distribution\")\n",
    "    plt.xlabel(\"Leading silence (ms)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(trailing_silence_lengths)\n",
    "    plt.title(\"Traling silence distribution\")\n",
    "    plt.xlabel(\"Trailing silence (ms)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"silences.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Metrics graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(mosnet_scores)\n",
    "    plt.title(\"Mosnet score distribution\")\n",
    "    plt.xlabel(\"Mosnet score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(srmr_scores)\n",
    "    plt.title(\"SRMR score distribution\")\n",
    "    plt.xlabel(\"SRMR score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"metrics.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Paces graph\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(paces_characters)\n",
    "    plt.title(\"Pace (chars/s)\")\n",
    "    plt.xlabel(\"Characters / second\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(paces_phonemes)\n",
    "    plt.title(\"Pace (phonemes/s)\")\n",
    "    plt.xlabel(\"Phonemes / second\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(os.path.join(dataset_path, output_folder, \"paces.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"n_clips\": n_clips,\n",
    "        \"total_lengths_summary\": get_summary_statistics(total_lengths),\n",
    "        \"paces_phonemes_summary\": get_summary_statistics(paces_phonemes),\n",
    "        \"paces_characters_summary\": get_summary_statistics(paces_characters),\n",
    "        \"mosnet_scores_summary\": get_summary_statistics(mosnet_scores),\n",
    "        \"srmr_scores_summary\": get_summary_statistics(srmr_scores),\n",
    "        \"pitch_summary\": get_summary_statistics(all_pitches),\n",
    "        \"loudness_summary\": get_summary_statistics(all_loudness),\n",
    "        \"total_lengths\": total_lengths,\n",
    "        \"paces_phonemes\": paces_phonemes,\n",
    "        \"paces_characters\": paces_characters,\n",
    "        \"mosnet_scores\": mosnet_scores,\n",
    "        \"srmr_scores\": srmr_scores,\n",
    "        \"sample_rates\": sample_rates,\n",
    "        \"channels\": channels,\n",
    "        \"extensions\": extensions,\n",
    "        \"sample_formats\": sample_formats,\n",
    "        \"lookup_results\": lookup_results,\n",
    "        \"files_with_error\": files_with_error,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_markdown(output_file, dataset_path, output_folder, data):\n",
    "    mdFile = MdUtils(\n",
    "        file_name=os.path.join(dataset_path, output_file), title=f\"Dataset statistics\"\n",
    "    )\n",
    "\n",
    "    total_length_mins = sum(data[\"total_lengths\"]) / 60.0\n",
    "    mdFile.new_header(level=1, title=\"Overview\")\n",
    "    mdFile.new_line(f\"**Number of clips:** {data['n_clips']}\")\n",
    "    mdFile.new_line(\n",
    "        f\"**Total data:** {math.floor(total_length_mins)} minutes {math.ceil(total_length_mins % 1 * 60.0)} seconds\"\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        f\"**Mean clip length:** {sum(data['total_lengths'])/data['n_clips']:.2f} seconds\"\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        f\"**Mean pace:** {sum(data['paces_phonemes'])/len(data['paces_phonemes']):.2f} \\\n",
    "            phonemes/sec {sum(data['paces_characters'])/len(data['paces_characters']):.2f} chars/sec\"\n",
    "    )\n",
    "    if len(data[\"mosnet_scores\"]) > 0:\n",
    "        mdFile.new_line(\n",
    "            f\"**Mean MOSNet:** {sum(data['mosnet_scores'])/len(data['mosnet_scores']):.2f}\"\n",
    "        )\n",
    "        mdFile.new_line(\n",
    "            f\"**Mean SRMR:** {sum(data['srmr_scores'])/len(data['srmr_scores']):.2f}\"\n",
    "        )\n",
    "\n",
    "    if len(data[\"files_with_error\"]) > 0:\n",
    "        mdFile.new_line(f\"**Errored Files:** {', '.join(data['files_with_error'])}\")\n",
    "\n",
    "    list_of_strings = [\"Sample Rate (Hz)\", \"Count\"]\n",
    "    for k in data[\"sample_rates\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"sample_rates\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"sample_rates\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Audio Type\", \"Count\"]\n",
    "    n_rows = 1\n",
    "    for k in data[\"channels\"].keys():\n",
    "        if data[\"channels\"][k] > 0:\n",
    "            n_rows += 1\n",
    "            list_of_strings.extend([str(k), str(data[\"channels\"][k])])\n",
    "    mdFile.new_table(columns=2, rows=n_rows, text=list_of_strings, text_align=\"center\")\n",
    "\n",
    "    list_of_strings = [\"Audio Format\", \"Count\"]\n",
    "    for k in data[\"extensions\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"extensions\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"extensions\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Sample Format\", \"Count\"]\n",
    "    for k in data[\"sample_formats\"].keys():\n",
    "        list_of_strings.extend([str(k), str(data[\"sample_formats\"][k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"sample_formats\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    list_of_strings = [\"Arpabet Lookup Type\", \"Count\"]\n",
    "    for k in data[\"lookup_results\"].keys():\n",
    "        list_of_strings.extend([str(k), str(len(data[\"lookup_results\"][k]))])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=len(data[\"lookup_results\"].keys()) + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Wordcloud\", path=os.path.join(output_folder, \"wordcloud.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Audio Lengths\", path=os.path.join(output_folder, \"lengths.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Paces\", path=os.path.join(output_folder, \"paces.png\")\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Silences\", path=os.path.join(output_folder, \"silences.png\")\n",
    "        )\n",
    "    )\n",
    "    if len(data[\"mosnet_scores\"]) > 0:\n",
    "        mdFile.new_line(\n",
    "            mdFile.new_inline_image(\n",
    "                text=\"Metrics\", path=os.path.join(output_folder, \"metrics.png\")\n",
    "            )\n",
    "        )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Word Frequencies\",\n",
    "            path=os.path.join(output_folder, \"word_frequencies.png\"),\n",
    "        )\n",
    "    )\n",
    "    mdFile.new_line(\n",
    "        mdFile.new_inline_image(\n",
    "            text=\"Pitch and Loudness\",\n",
    "            path=os.path.join(output_folder, \"pitch_loudness.png\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rnn_frequency_counts = count_frequency(data[\"lookup_results\"][\"RNN\"])\n",
    "\n",
    "    list_of_strings = [\"Frequently Missed Words\", \"Count\"]\n",
    "    n_rows = 0\n",
    "    for k in rnn_frequency_counts.keys():\n",
    "        if rnn_frequency_counts[k] > 1:\n",
    "            n_rows += 1\n",
    "            list_of_strings.extend([str(k), str(rnn_frequency_counts[k])])\n",
    "    mdFile.new_table(\n",
    "        columns=2,\n",
    "        rows=n_rows + 1,\n",
    "        text=list_of_strings,\n",
    "        text_align=\"center\",\n",
    "    )\n",
    "\n",
    "    mdFile.new_line(\n",
    "        f'**Words not found in CMU:** {\", \".join(data[\"lookup_results\"][\"RNN\"])}'\n",
    "    )\n",
    "    mdFile.create_md_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5eb1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-d\", \"--dataset_path\", help=\"Path to the dataset.\", type=str, required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-i\",\n",
    "        \"--input_file\",\n",
    "        help=\"Path to the transcription file.\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--output_file\",\n",
    "        help=\"Markdown file to write statistics to.\",\n",
    "        type=str,\n",
    "        default=\"README\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_folder\",\n",
    "        help=\"Folder to save plots and images.\",\n",
    "        type=str,\n",
    "        default=\"stats\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--delimiter\", help=\"Transcription file delimiter.\", type=str, default=\"|\"\n",
    "    )\n",
    "    parser.add_argument(\"--metrics\", dest=\"metrics\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-metrics\", dest=\"metrics\", action=\"store_false\")\n",
    "    parser.add_argument(\"--wordcloud\", dest=\"wordcloud\", action=\"store_true\")\n",
    "    parser.add_argument(\"--no-wordcloud\", dest=\"wordcloud\", action=\"store_false\")\n",
    "    parser.set_defaults(metrics=True, wordcloud=True)\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "\n",
    "def run(\n",
    "    dataset_path, input_file, output_file, output_folder, delimiter, metrics, wordcloud\n",
    "):\n",
    "    if not os.path.exists(os.path.join(dataset_path, input_file)):\n",
    "        raise Exception(\n",
    "            f\"Transcription file {os.path.join(dataset_path,input_file)} does not exist\"\n",
    "        )\n",
    "\n",
    "    os.makedirs(os.path.join(dataset_path, output_folder), exist_ok=True)\n",
    "    data = calculate_statistics(\n",
    "        dataset_path, input_file, output_folder, delimiter, metrics, wordcloud\n",
    "    )\n",
    "    if data:\n",
    "        generate_markdown(output_file, dataset_path, output_folder, data)\n",
    "        with open(os.path.join(dataset_path, \"stats.json\"), \"w\") as outfile:\n",
    "            keys = [\n",
    "                \"n_clips\",\n",
    "                \"total_lengths_summary\",\n",
    "                \"paces_phonemes_summary\",\n",
    "                \"paces_characters_summary\",\n",
    "                \"mosnet_scores_summary\",\n",
    "                \"srmr_scores_summary\",\n",
    "                \"pitch_summary\",\n",
    "                \"loudness_summary\",\n",
    "                \"sample_rates\",\n",
    "                \"channels\",\n",
    "                \"extensions\",\n",
    "                \"sample_formats\",\n",
    "            ]\n",
    "            json_data = {k: data[k] for k in keys}\n",
    "            json_data[\"arpabet_rnn\"] = data[\"lookup_results\"][\"RNN\"]\n",
    "            json.dump(json_data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# folders = glob.glob(\"/home/ubuntu/data/uberduck-multispeaker/*/*.txt\")\n",
    "\n",
    "# for dataset in folders:\n",
    "#     split = dataset.split(\"/\")\n",
    "#     file = split[-1]\n",
    "#     dataset_path = \"/\".join(split[:-1])\n",
    "\n",
    "#     run(\n",
    "#         dataset_path=dataset_path,\n",
    "#         input_file=file,\n",
    "#         output_file=\"README.md\",\n",
    "#         output_folder=\"imgs\",\n",
    "#         delimiter=\"|\",\n",
    "#         metrics=True,\n",
    "#         wordcloud=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2832141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(\n",
    "#     dataset_path=\"/home/ubuntu/data/uberduck-multispeaker/bullwinkle\",\n",
    "#     input_file=\"list.txt\",\n",
    "#     output_file=\"STATISTICS.md\",\n",
    "#     output_folder=\"imgs\",\n",
    "#     delimiter=\"|\",\n",
    "#     metrics=False,\n",
    "#     wordcloud=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e79961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "try:\n",
    "    from nbdev.imports import IN_NOTEBOOK\n",
    "except:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "if __name__ == \"__main__\" and not IN_NOTEBOOK:\n",
    "    args = parse_args(sys.argv[1:])\n",
    "\n",
    "    if os.path.exists(\n",
    "        os.path.join(args.dataset_path, args.output_file)\n",
    "    ) or os.path.exists(os.path.join(args.dataset_path, args.output_file + \".md\")):\n",
    "        inp = input(\n",
    "            f\"This script will overwite everything in the {args.output_file} file with dataset statistics. Would you like to continue? (y/n) \"\n",
    "        ).lower()\n",
    "        if inp != \"y\":\n",
    "            print(\"Not calculating statistics...\")\n",
    "            print(\"HINT: Use -o/--output-file to specify a new markdown file name\")\n",
    "            sys.exit()\n",
    "    print(\"Calculating statistics...\")\n",
    "    run(\n",
    "        args.dataset_path,\n",
    "        args.input_file,\n",
    "        args.output_file,\n",
    "        args.output_folder,\n",
    "        args.delimiter,\n",
    "        args.metrics,\n",
    "        args.wordcloud,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b78a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "assert g2p.check_lookup(\"this is a test\") == {\"CMU\": [\"this\", \"is\", \"a\", \"test\"]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
