{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a7467cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6dab9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import uuid\n",
    "from pathlib import PosixPath\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "CACHE_LOCATION = uberduck_ml_dev.data.cache.CACHE_LOCATION\n",
    "STANDARD_MULTISPEAKER = \"standard-multispeaker\"\n",
    "STANDARD_SINGLESPEAKER = \"standard-singlespeaker\"\n",
    "VCTK = \"vctk\"\n",
    "\n",
    "\n",
    "def _cache_filelists(folder, fmt, conn, dataset_name: str = None):\n",
    "    \"\"\"\n",
    "    records a filelist into the speaker cache\n",
    "    \"\"\"\n",
    "    if fmt == STANDARD_MULTISPEAKER:\n",
    "        _parse_ms(root=folder, dataset_name=dataset_name)\n",
    "    if fmt == STANDARD_SINGLESPEAKER:\n",
    "        _parse_ss(\n",
    "            conn=conn,\n",
    "            root=folder,\n",
    "            speaker_name=dataset_name,\n",
    "            dir_path=folder,\n",
    "            dataset_name=dataset_name,\n",
    "        )\n",
    "    if fmt == VCTK:\n",
    "        raise\n",
    "\n",
    "\n",
    "def _add_speaker_to_db(\n",
    "    filelist_path: str,\n",
    "    speaker_name: str,\n",
    "    speaker_id=None,\n",
    "    dir_path: str = None,\n",
    "    rel_path: str = None,\n",
    "    dataset_name: str = None,\n",
    "    conn=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    filelist: the path of the filelist being added\n",
    "    speaker_name: the name of the speaker\n",
    "    dir_path: the path of the data repository containing the filelist\n",
    "    rel_path: the path of the wavs within the repository\n",
    "    dataset_name: the name of the dataset\n",
    "    \"\"\"\n",
    "    uuid_ = uuid.uuid4()\n",
    "    if conn is None:\n",
    "        conn = sqlite3.connect(str(CACHE_LOCATION_EXP))\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"INSERT OR REPLACE INTO FILELISTS VALUES (?, ?, ?, ?, ?, ?, ?)\",\n",
    "        (\n",
    "            str(uuid_),\n",
    "            filelist_path,\n",
    "            speaker_name,\n",
    "            speaker_id,\n",
    "            dir_path,\n",
    "            rel_path,\n",
    "            dataset_name,\n",
    "        ),\n",
    "    )\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "def _parse_ms(root: str, dataset_name: str, conn):\n",
    "    speakers = os.listdir(root)\n",
    "    for speaker in tqdm(speakers):\n",
    "        speaker_path = Path(root) / Path(speaker)\n",
    "        if not path.is_dir() or path.parts[-1].startswith(\".\"):\n",
    "            continue\n",
    "        _parse_ss(\n",
    "            root=speaker_path,\n",
    "            speaker_name=speaker,\n",
    "            speaker_id=None,\n",
    "            dir_path=root,\n",
    "            dataset_name=dataset_name,\n",
    "            rel_path=speaker,\n",
    "            conn=conn,\n",
    "        )\n",
    "\n",
    "\n",
    "def _parse_ss(\n",
    "    conn,\n",
    "    root: str,\n",
    "    speaker_name: str,\n",
    "    speaker_id=None,\n",
    "    dir_path: str = None,\n",
    "    dataset_name: str = None,\n",
    "    rel_path=\"\",\n",
    "):\n",
    "    files = os.listdir(root)\n",
    "    filelist_paths = [f for f in files if f.endswith(\".txt\")]\n",
    "    for filelist_path in filelist_paths:\n",
    "        _add_speaker_to_db(\n",
    "            filelist_path=filelist_path,\n",
    "            speaker_name=speaker_name,\n",
    "            speaker_id=speaker_id,\n",
    "            dir_path=root,\n",
    "            dataset_name=dataset_name,\n",
    "            rel_path=rel_path,\n",
    "            conn=conn,\n",
    "        )\n",
    "\n",
    "\n",
    "def _generate_filelist(config_path, conn, out):\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        filelist_config = json.load(f)\n",
    "    speaker_id = 0\n",
    "    save_path = Path(out)\n",
    "    exp_path = Path(os.path.join(*save_path.parts[:-1]))\n",
    "    if not os.path.exists(exp_path):\n",
    "        exp_path.mkdir(parents=True)\n",
    "    with open(save_path, \"w\") as f_out:\n",
    "        for filelist in filelist_config[\"filelists\"]:\n",
    "            uuid = filelist[\"uuid\"]\n",
    "            dir_path = filelist[\"dir_path\"]\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\n",
    "                \"SELECT dir_path,rel_path,filelist_path FROM FILELISTS WHERE uuid = :uuid\", {\"uuid\": uuid}\n",
    "            )\n",
    "            results = cursor.fetchall()\n",
    "            assert len(results) == 1\n",
    "            in_path = Path(os.path.join(*results[0]))\n",
    "            with (in_path).open(\"r\") as txn_f:\n",
    "                transcriptions = txn_f.readlines()\n",
    "            for line in transcriptions:\n",
    "                line = line.strip(\"\\n\")\n",
    "                try:\n",
    "                    line_path, line_txn, *_ = line.split(\"|\")\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(line)\n",
    "                    raise\n",
    "                out_path = os.path.join(\n",
    "                    *([dir_path] + list(results[0][1:2]) + [line_path])\n",
    "                )\n",
    "                f_out.write(f\"{out_path}|{line_txn}|{speaker_id}\\n\")\n",
    "            speaker_id += 1\n",
    "\n",
    "\n",
    "def _write_db_to_csv(conn, output_path):\n",
    "    cursor = conn.cursor()\n",
    "    query = cursor.execute(\"SELECT * From FILELISTS\")\n",
    "\n",
    "    cols = [column[0] for column in query.description]\n",
    "    results = pd.DataFrame.from_records(data=query.fetchall(), columns=cols)\n",
    "    results.to_csv(output_path, header=True)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1868c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_data = _parse_vctk(\"/mnt/disks/uberduck-experiments-v0/data/vctk/\")\n",
    "# speaker_data['p314']\n",
    "# speaker_data['p314'][1]\n",
    "\n",
    "# def load_filepaths_and_text(filename: str, split: str = \"|\"):\n",
    "#     with open(filename, encoding=\"utf-8\") as f:\n",
    "#         filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "#     return filepaths_and_text\n",
    "\n",
    "# def _parse_vctk(root: str):\n",
    "#     \"\"\"Parse VCTK dataset and return a dict representation.\"\"\"\n",
    "#     wav_dir = os.path.join(root, \"wav48_silence_trimmed\")\n",
    "#     txt_dir = os.path.join(root, \"txt\")\n",
    "#     speaker_wavs = os.listdir(wav_dir)\n",
    "#     speaker_txts = os.listdir(txt_dir)\n",
    "#     speakers = list(set(speaker_wavs) & set(speaker_txts))\n",
    "#     output_dict = {}\n",
    "#     for speaker in speakers:\n",
    "#         speaker_wav_dir = os.path.join(wav_dir, speaker)\n",
    "#         speaker_txt_dir = os.path.join(txt_dir, speaker)\n",
    "#         wav_files_speaker = np.asarray(os.listdir(speaker_wav_dir))\n",
    "#         txt_files_speaker = np.asarray(os.listdir(speaker_txt_dir))\n",
    "\n",
    "#         transcription_basenames = np.asarray([t[:8] for t in txt_files_speaker])\n",
    "#         audio_basenames = np.asarray([w[:8] for w in wav_files_speaker])\n",
    "#         mic = np.asarray([w[12] for w in wav_files_speaker])\n",
    "#         mic1_ind = mic == \"1\"\n",
    "#         wav_files_speaker = wav_files_speaker[mic1_ind]\n",
    "#         audio_basenames = audio_basenames[mic1_ind]\n",
    "\n",
    "#         combined_files = np.intersect1d(transcription_basenames, audio_basenames)\n",
    "#         matching_inds1 = np.where(np.isin(transcription_basenames, combined_files))[0]\n",
    "#         matching_inds2 = np.where(np.isin(audio_basenames, combined_files))[0]\n",
    "#         inds1 = matching_inds1[transcription_basenames[matching_inds1].argsort()]\n",
    "#         inds2 = matching_inds2[audio_basenames[matching_inds2].argsort()]\n",
    "#         txt_files_speaker = txt_files_speaker[inds1]\n",
    "#         wav_files_speaker = wav_files_speaker[inds2]\n",
    "#         texts, wavs = [], []\n",
    "#         for text_basename, wav_basename in zip(txt_files_speaker, wav_files_speaker):\n",
    "#             text_file = os.path.join(speaker_txt_dir, text_basename)\n",
    "#             with open(text_file) as f:\n",
    "#                 contents = f.read().strip(\"\\n\")\n",
    "#             texts.append(contents)\n",
    "#             wav_file = os.path.join(speaker_wav_dir, wav_basename)\n",
    "#             wavs.append(wav_file)\n",
    "\n",
    "#         if len(wavs):\n",
    "#             output_dict[speaker] = list(zip(texts, wavs))\n",
    "#     return output_dict\n",
    "\n",
    "# def _convert_vctk(f, inp: str):\n",
    "#     vctk_data = parse_vctk(inp)\n",
    "#     speaker_id = 0\n",
    "#     conn = sqlite3.connect(str(CACHE_LOCATION))\n",
    "#     with conn:\n",
    "#         for speaker_name, speaker_data in tqdm(vctk_data.items()):\n",
    "#             insert_speaker(f.name, speaker_name, speaker_id, conn)\n",
    "#             speaker_out_path = Path(out_path) / speaker_name\n",
    "#             if not speaker_out_path.exists():\n",
    "#                 os.makedirs(speaker_out_path)\n",
    "#             for transcription, flac_path in speaker_data:\n",
    "#                 assert flac_path.endswith(\".flac\")\n",
    "#                 wav_path = flac_path.replace(\".flac\", \".wav\")\n",
    "#                 convert_to_wav(flac_path, wav_path)\n",
    "#                 full_path = Path(full_path).resolve()\n",
    "#                 f.write(f\"{full_path}|{transcription}|{speaker_id}\\n\")\n",
    "#             speaker_id += 1\n",
    "\n",
    "\n",
    "# def load_filepaths_and_text(filename: str, split: str = \"|\"):\n",
    "#     with open(filename, encoding=\"utf-8\") as f:\n",
    "#         filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "#     return filepaths_and_text\n",
    "\n",
    "# def _parse_vctk(root: str):\n",
    "#     \"\"\"Parse VCTK dataset and return a dict representation.\"\"\"\n",
    "#     wav_dir = os.path.join(root, \"wav48_silence_trimmed\")\n",
    "#     txt_dir = os.path.join(root, \"txt\")\n",
    "#     speaker_wavs = os.listdir(wav_dir)\n",
    "#     speaker_txts = os.listdir(txt_dir)\n",
    "#     speakers = list(set(speaker_wavs) & set(speaker_txts))\n",
    "#     output_dict = {}\n",
    "#     for speaker in speakers:\n",
    "#         speaker_wav_dir = os.path.join(wav_dir, speaker)\n",
    "#         speaker_txt_dir = os.path.join(txt_dir, speaker)\n",
    "#         wav_files_speaker = np.asarray(os.listdir(speaker_wav_dir))\n",
    "#         txt_files_speaker = np.asarray(os.listdir(speaker_txt_dir))\n",
    "\n",
    "#         nwavfiles = len(wav_files_speaker)\n",
    "\n",
    "#         transcription_basenames = np.asarray([t[:8] for t in txt_files_speaker])\n",
    "#         audio_basenames = np.asarray([w[:8] for w in wav_files_speaker])\n",
    "#         mic = np.asarray([w[12] for w in wav_files_speaker])\n",
    "#         mic1_ind = mic == \"1\"\n",
    "#         wav_files_speaker = wav_files_speaker[mic1_ind]\n",
    "#         audio_basenames = audio_basenames[mic1_ind]\n",
    "\n",
    "#         combined_files = np.intersect1d(transcription_basenames, audio_basenames)\n",
    "#         matching_inds1 = np.where(np.isin(transcription_basenames, combined_files))[0]\n",
    "#         matching_inds2 = np.where(np.isin(audio_basenames, combined_files))[0]\n",
    "#         inds1 = matching_inds1[transcription_basenames[matching_inds1].argsort()]\n",
    "#         inds2 = matching_inds2[audio_basenames[matching_inds2].argsort()]\n",
    "#         txt_files_speaker = txt_files_speaker[inds1]\n",
    "#         wav_files_speaker = wav_files_speaker[inds2]\n",
    "#         texts, wavs = [], []\n",
    "#         for text_basename, wav_basename in zip(txt_files_speaker, wav_files_speaker):\n",
    "#             text_file = os.path.join(speaker_txt_dir, text_basename)\n",
    "#             with open(text_file) as f:\n",
    "#                 contents = f.read().strip(\"\\n\")\n",
    "#             texts.append(contents)\n",
    "#             wav_file = os.path.join(speaker_wav_dir, wav_basename)\n",
    "#             wavs.append(wav_file)\n",
    "\n",
    "#         if len(wavs):\n",
    "#             output_dict[speaker] = list(zip(texts, wavs))\n",
    "#     return output_dict\n",
    "\n",
    "# def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "#     data = pd.read_csv(mellotron_filelist, sep=\"|\", header=None, error_bad_lines=False)\n",
    "\n",
    "#     data[0] = data[0].str[17:]\n",
    "\n",
    "#     data[0] = source_folder + data[0].astype(str)\n",
    "#     return data\n",
    "\n",
    "# def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "#     data = load_filepaths_and_text(mellotron_filelist)\n",
    "#     data = pd.DataFrame(data)\n",
    "#     data[0] = data[0].str[17:]\n",
    "#     data[0] = source_folder + data[0].astype(str)\n",
    "#     return data\n",
    "\n",
    "# def parse_ljspeech(source_folder):\n",
    "#     source_file = source_folder + \"/metadata.csv\"\n",
    "#     data = load_filepaths_and_text(source_file)\n",
    "#     data = pd.DataFrame(data)\n",
    "#     nsamp = data.shape[0]\n",
    "\n",
    "#     data[0] = source_folder + \"/wavs/\" + data[0].astype(str)\n",
    "#     output = add_speakerid(data, speaker_key=0)\n",
    "#     for i in range(output.shape[0]):\n",
    "#         output.iloc[i, 0] = output.iloc[i, 0] + \".wav\"\n",
    "\n",
    "#     return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
