{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded346d",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0be6cd",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp text.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Cleaners are transformations that run over the input text at both training and eval time.\n",
    "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
    "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
    "  1. \"english_cleaners\" for English text\n",
    "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
    "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
    "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
    "     the symbols in symbols.py to match your data).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from g2p_en import G2p\n",
    "from phonemizer import phonemize\n",
    "from unidecode import unidecode\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import curly_re, words_re, symbols_to_sequence\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [\n",
    "    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n",
    "    for x in [\n",
    "        (\"mrs\", \"misess\"),\n",
    "        (\"mr\", \"mister\"),\n",
    "        (\"dr\", \"doctor\"),\n",
    "        (\"st\", \"saint\"),\n",
    "        (\"co\", \"company\"),\n",
    "        (\"jr\", \"junior\"),\n",
    "        (\"maj\", \"major\"),\n",
    "        (\"gen\", \"general\"),\n",
    "        (\"drs\", \"doctors\"),\n",
    "        (\"rev\", \"reverend\"),\n",
    "        (\"lt\", \"lieutenant\"),\n",
    "        (\"hon\", \"honorable\"),\n",
    "        (\"sgt\", \"sergeant\"),\n",
    "        (\"capt\", \"captain\"),\n",
    "        (\"esq\", \"esquire\"),\n",
    "        (\"ltd\", \"limited\"),\n",
    "        (\"col\", \"colonel\"),\n",
    "        (\"ft\", \"fort\"),\n",
    "    ]\n",
    "]\n",
    "\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n",
    "_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n",
    "_pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n",
    "_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n",
    "_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n",
    "_number_re = re.compile(r\"[0-9]+\")\n",
    "\n",
    "\n",
    "def _remove_commas(m):\n",
    "    return m.group(1).replace(\",\", \"\")\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "    return m.group(1).replace(\".\", \" point \")\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "    match = m.group(1)\n",
    "    parts = match.split(\".\")\n",
    "    if len(parts) > 2:\n",
    "        return match + \" dollars\"  # Unexpected format\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "    if dollars and cents:\n",
    "        dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "        cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "        return \"%s %s, %s %s\" % (dollars, dollar_unit, cents, cent_unit)\n",
    "    elif dollars:\n",
    "        dollar_unit = \"dollar\" if dollars == 1 else \"dollars\"\n",
    "        return \"%s %s\" % (dollars, dollar_unit)\n",
    "    elif cents:\n",
    "        cent_unit = \"cent\" if cents == 1 else \"cents\"\n",
    "        return \"%s %s\" % (cents, cent_unit)\n",
    "    else:\n",
    "        return \"zero dollars\"\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "    return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return \"two thousand\"\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return \"two thousand \" + _inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return _inflect.number_to_words(num // 100) + \" hundred\"\n",
    "        else:\n",
    "            return _inflect.number_to_words(\n",
    "                num, andword=\"\", zero=\"oh\", group=2\n",
    "            ).replace(\", \", \" \")\n",
    "    else:\n",
    "        return _inflect.number_to_words(num, andword=\"\")\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "    text = re.sub(_pounds_re, r\"\\1 pounds\", text)\n",
    "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "    text = re.sub(_number_re, _expand_number, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    return normalize_numbers(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "    return re.sub(_whitespace_re, \" \", text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "\n",
    "def convert_to_arpabet(text, overrides=None):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            f\"{{ {s.strip()} }}\" if s.strip() not in \",.\" else s.strip()\n",
    "            for s in \" \".join(g2p(text, overrides=overrides)).split(\"  \")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def basic_cleaners(text):\n",
    "    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transliteration_cleaners(text):\n",
    "    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def english_cleaners(text):\n",
    "    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def english_cleaners_phonemizer(text):\n",
    "    \"\"\"Pipeline for English text to phonemization, including number and abbreviation expansion.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = phonemize(\n",
    "        text,\n",
    "        language=\"en-us\",\n",
    "        backend=\"espeak\",\n",
    "        strip=True,\n",
    "        preserve_punctuation=True,\n",
    "        with_stress=True,\n",
    "    )\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def batch_english_cleaners_phonemizer(text: List[str]):\n",
    "    batch = []\n",
    "    for t in text:\n",
    "        t = convert_to_ascii(t)\n",
    "        t = lowercase(t)\n",
    "        t = expand_numbers(t)\n",
    "        t = expand_abbreviations(t)\n",
    "        batch.append(t)\n",
    "    batch = phonemize(\n",
    "        batch,\n",
    "        language=\"en-us\",\n",
    "        backend=\"espeak\",\n",
    "        strip=True,\n",
    "        preserve_punctuation=True,\n",
    "        with_stress=True,\n",
    "    )\n",
    "    batch = [collapse_whitespace(t) for t in batch]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert convert_to_arpabet(\"Dictionary\") == \"{ D IH1 K SH AH0 N EH2 R IY0 }\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    english_cleaners_phonemizer(\"Pipeline for English text to phonemization\")\n",
    "    == \"pˈaɪplaɪn fɔːɹ ˈɪŋɡlɪʃ tˈɛkst tə fˌoʊnmaɪzˈeɪʃən\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47352b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    normalize_numbers(\"234,321.2\")\n",
    "    == \"two hundred thirty-four thousand, three hundred twenty-one point two\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import random\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import (\n",
    "    DEFAULT_SYMBOLS,\n",
    "    IPA_SYMBOLS,\n",
    "    NVIDIA_TACO2_SYMBOLS,\n",
    "    GRAD_TTS_SYMBOLS,\n",
    "    id_to_symbol,\n",
    "    symbols_to_sequence,\n",
    "    arpabet_to_sequence,\n",
    ")\n",
    "\n",
    "BATCH_CLEANERS = {\n",
    "    \"english_cleaners_phonemizer\": batch_english_cleaners_phonemizer,\n",
    "}\n",
    "\n",
    "CLEANERS = {\n",
    "    \"english_cleaners\": english_cleaners,\n",
    "    \"english_cleaners_phonemizer\": english_cleaners_phonemizer,\n",
    "    \"basic_cleaners\": basic_cleaners,\n",
    "    \"transliteration_cleaners\": transliteration_cleaners,\n",
    "}\n",
    "\n",
    "\n",
    "def batch_clean_text(text: List[str], cleaner_names):\n",
    "    for name in cleaner_names:\n",
    "        cleaner = BATCH_CLEANERS[name]\n",
    "        text = cleaner(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_text(text, cleaner_names):\n",
    "    for name in cleaner_names:\n",
    "        cleaner = CLEANERS[name]\n",
    "        text = cleaner(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def english_to_arpabet(english_text):\n",
    "    arpabet_symbols = g2p(english_text)\n",
    "\n",
    "\n",
    "def cleaned_text_to_sequence(cleaned_text, symbol_set):\n",
    "    return symbols_to_sequence(cleaned_text, symbol_set=symbol_set, ignore_symbols=[])\n",
    "\n",
    "\n",
    "def text_to_sequence(\n",
    "    text,\n",
    "    cleaner_names,\n",
    "    p_arpabet=0.0,\n",
    "    symbol_set=DEFAULT_SYMBOLS,\n",
    "    arpabet_overrides=None,\n",
    "):\n",
    "    \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
    "    in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "\n",
    "    # Check for curly braces and treat their contents as ARPAbet:\n",
    "    while len(text):\n",
    "        m = curly_re.match(text)\n",
    "        if not m:\n",
    "            cleaned = clean_text(text, cleaner_names)\n",
    "            words_and_nonwords = words_re.findall(cleaned)\n",
    "            cleaned_words = []\n",
    "            for w, nw in words_and_nonwords:\n",
    "                if w and random.random() < p_arpabet:\n",
    "                    cleaned_words.append(\n",
    "                        convert_to_arpabet(w, overrides=arpabet_overrides)\n",
    "                    )\n",
    "                elif w:\n",
    "                    cleaned_words.append(w)\n",
    "                else:\n",
    "                    cleaned_words.append(nw)\n",
    "            for word in cleaned_words:\n",
    "                if word.startswith(\"{\"):\n",
    "                    sequence += arpabet_to_sequence(word, symbol_set)\n",
    "                else:\n",
    "                    sequence += symbols_to_sequence(word, symbol_set)\n",
    "            break\n",
    "        cleaned = clean_text(m.group(1), cleaner_names)\n",
    "        sequence += text_to_sequence(cleaned, cleaner_names, p_arpabet, symbol_set)\n",
    "        sequence += arpabet_to_sequence(m.group(2), symbol_set)\n",
    "        text = m.group(3)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def sequence_to_text(sequence, symbol_set=DEFAULT_SYMBOLS):\n",
    "    \"\"\"Converts a sequence of IDs back to a string\"\"\"\n",
    "    result = \"\"\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in id_to_symbol[symbol_set]:\n",
    "            s = id_to_symbol[symbol_set][symbol_id]\n",
    "            # Enclose ARPAbet back in curly braces:\n",
    "            if len(s) > 1 and s[0] == \"@\":\n",
    "                s = \"{%s}\" % s[1:]\n",
    "            result += s\n",
    "    return result.replace(\"}{\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def text_to_sequence_for_editts(text, cleaner_names, symbol_set=GRAD_TTS_SYMBOLS):\n",
    "    sequence = []\n",
    "    emphases = []\n",
    "    final_emphases = []\n",
    "    space = symbols_to_sequence(\" \", symbol_set=symbol_set)\n",
    "    cleaned = clean_text(text, cleaner_names)\n",
    "\n",
    "    i = 0\n",
    "    result = []\n",
    "    emphasis_interval = []\n",
    "    for w in cleaned.split(\" \"):\n",
    "        if w == \"|\":\n",
    "            emphasis_interval.append(i)\n",
    "            if len(emphasis_interval) == 2:\n",
    "                emphases.append(emphasis_interval)\n",
    "                emphasis_interval = []\n",
    "        else:\n",
    "            i += 1\n",
    "            result.append(convert_to_arpabet(w))\n",
    "\n",
    "    cleaned = result\n",
    "    emphasis_interval = []\n",
    "    cnt = 0\n",
    "    for i in range(len(cleaned)):\n",
    "        t = cleaned[i]\n",
    "        if cnt < len(emphases) and i == emphases[cnt][0]:\n",
    "            emphasis_interval.append(len(sequence))\n",
    "\n",
    "        if t.startswith(\"{\"):\n",
    "            sequence += arpabet_to_sequence(t[1:-1], symbol_set=symbol_set)\n",
    "        else:\n",
    "            sequence += symbols_to_sequence(t, symbol_set=symbol_set)\n",
    "\n",
    "        if cnt < len(emphases) and i == emphases[cnt][1] - 1:\n",
    "            emphasis_interval.append(len(sequence))\n",
    "            final_emphases.append(emphasis_interval)\n",
    "            emphasis_interval = []\n",
    "            cnt += 1\n",
    "\n",
    "        sequence += space\n",
    "\n",
    "    # remove trailing space\n",
    "    if sequence[-1] == space[0]:\n",
    "        sequence = sequence[:-1]\n",
    "\n",
    "    return sequence, final_emphases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9d2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 82, 79, 9, 90, 79, 88, 9, 83, 93, 9, 9, 76, 86, 95, 79, 5, 9]\n",
      "{N AA1 T} {B AE1 D} {B AA1 R T}, {N AA1 T} {B AE1 D} {AE1 T} {AO1 L}.\n",
      "{N AA1 T} {B AE1 D} {B AA1 R T} , {N AA1 T} {B AE1 D} {AE1 T} {AO1 L} .\n",
      "not bad {B AA1 R T}, {N AA1 T} bad at {AO1 L}.\n"
     ]
    }
   ],
   "source": [
    "print(text_to_sequence(\"The pen is | blue.| \", [\"english_cleaners\"]))\n",
    "assert len(text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"])) == 16\n",
    "assert len(text_to_sequence(\"The pen is {B L OW0}.\", [\"english_cleaners\"])) == 15\n",
    "assert (\n",
    "    sequence_to_text(text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"]))\n",
    "    == \"the pen is blue.\"\n",
    "), sequence_to_text(text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"]))\n",
    "assert (\n",
    "    sequence_to_text(text_to_sequence(\"The pen is {B L OW0}.\", [\"english_cleaners\"]))\n",
    "    == \"the pen is {B L OW0}.\"\n",
    ")\n",
    "assert (\n",
    "    len(\n",
    "        text_to_sequence(\n",
    "            \"{N AA1 T} {B AE1 D} {B AA1 R T}, {N AA1 T} {B AE1 D} {AE1 T} {AO1 L}.\",\n",
    "            [\"english_cleaners\"],\n",
    "        )\n",
    "    )\n",
    "    == 28\n",
    ")\n",
    "\n",
    "assert (\n",
    "    len(\n",
    "        text_to_sequence(\n",
    "            \"Not bad bart, not bad at all.\", [\"english_cleaners\"], p_arpabet=1.0\n",
    "        )\n",
    "    )\n",
    "    == 28\n",
    ")\n",
    "seq = text_to_sequence(\n",
    "    \"Not bad bart, not bad at all.\", [\"english_cleaners\"], p_arpabet=1.0\n",
    ")\n",
    "print(sequence_to_text(seq))\n",
    "seq2 = text_to_sequence(\n",
    "    \"{N AA1 T} {B AE1 D} {B AA1 R T} , {N AA1 T} {B AE1 D} {AE1 T} {AO1 L} .\",\n",
    "    [\"english_cleaners\"],\n",
    ")\n",
    "print(sequence_to_text(seq2))\n",
    "seq3 = text_to_sequence(\n",
    "    \"Not bad bart, not bad at all.\", [\"english_cleaners\"], p_arpabet=0.5\n",
    ")\n",
    "print(sequence_to_text(seq3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da332498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.text.symbols import symbols, symbol_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6493d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cleaned_text_to_sequence(\"Not bad bart, not bad at all\", DEFAULT_SYMBOLS) == [\n",
    "    62,\n",
    "    89,\n",
    "    94,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    78,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    92,\n",
    "    94,\n",
    "    4,\n",
    "    9,\n",
    "    88,\n",
    "    89,\n",
    "    94,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    78,\n",
    "    9,\n",
    "    75,\n",
    "    94,\n",
    "    9,\n",
    "    75,\n",
    "    86,\n",
    "    86,\n",
    "]\n",
    "assert text_to_sequence(\n",
    "    \"Not bad bart, not bad at all\", [\"english_cleaners\"], 0.0, DEFAULT_SYMBOLS\n",
    ") == [\n",
    "    88,\n",
    "    89,\n",
    "    94,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    78,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    92,\n",
    "    94,\n",
    "    4,\n",
    "    9,\n",
    "    88,\n",
    "    89,\n",
    "    94,\n",
    "    9,\n",
    "    76,\n",
    "    75,\n",
    "    78,\n",
    "    9,\n",
    "    75,\n",
    "    94,\n",
    "    9,\n",
    "    75,\n",
    "    86,\n",
    "    86,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import random\n",
    "\n",
    "utterances = [\n",
    "    \"Stop posting about Among Us, I'm tired of seeing it!\",\n",
    "    \"My friends on TikTok send me memes, on Discord it's fucking memes.\",\n",
    "    \"I'd just like to interject for a moment.\",\n",
    "    \"What you're referring to as Linux, is in fact, gnu slash Linux.\",\n",
    "    \"Wow! That was intense! Woo I just flew in from the new ruins level and boy are my arms tired.\",\n",
    "    \"Oh my god! They killed Kenny!\",\n",
    "    \"It needs to be about, twenty percent cooler.\",\n",
    "    \"Hey relax guy! I'm just your average joe! Take a rest!\",\n",
    "    \"I'm not bad, I'm just drawn that way.\",\n",
    "    \"Alright! we're here just sitting in the car. I want you to show me if you can get far.\",\n",
    "    \"Isn't it nice to have a computer that will talk to you?\",\n",
    "    \"This is where we hold them. This is where we fight!\",\n",
    "    \"I'll have two number nines, a number nine large, a number six with extra dip.\",\n",
    "    \"A number seven, two number forty fives, one with cheese, and a large soda.\",\n",
    "    \"Can you tell me how to get to Sesame Street?\",\n",
    "    \"You know what they say, all toasters toast toast.\",\n",
    "    \"Don't turn me into a marketable plushie!\",\n",
    "    \"I am speaking straight opinions, and that's all that matters.\",\n",
    "    \"Excuse me sir, but it appears that a package has arrived in the mailbox as of recent.\",\n",
    "    \"I'm going to order pizza, look at me, I'm on the phone, right now.\",\n",
    "    \"I started calling and I am hungry to the bone.\",\n",
    "    \"so while I wait, I start to sing the song of my people I know it since I was a baby.\",\n",
    "    \"When I was a lad, I ate four dozen eggs every morning to help me get large.\",\n",
    "    \"Now that I'm grown I eat five dozen eggs, so I'm roughly the size of a barge!\",\n",
    "    \"There's no crying. There's no crying in baseball.\",\n",
    "    \"Sphinx of black quartz, judge my vow.\",\n",
    "    \"Go to the Winchester, have a pint, and wait for all of this to blow over.\",\n",
    "    \"You should really stop pressing this button.\",\n",
    "    \"Minecraft is honestly a block game.\",\n",
    "    \"I like that song. Let it play.\",\n",
    "    \"When a zebras in the zone, leave him alone!\",\n",
    "    \"The FitnessGram Pacer Test is a multistage aerobic capacity test that progressively gets more difficult as it continues.\",\n",
    "    \"The 20 meter pacer test will begin in 30 seconds.\",\n",
    "    \"The running speed starts slowly, but gets faster each minute after you hear this signal. beep.\",\n",
    "    \"A single lap should be completed each time you hear this sound. ding.\",\n",
    "    \"Remember to run in a straight line, and run as long as possible.\",\n",
    "    \"The second time you fail to complete a lap before the sound, your test is over.\",\n",
    "    \"The test will begin on the word start. On your mark, get ready, start.\",\n",
    "    \"Oh my gosh. Nemo's swimming out to sea!\",\n",
    "    \"Go back. I want to be monkey!\",\n",
    "    \"Whoops! You have to put the C D in your computer.\",\n",
    "    \"Now the animators are gonna have to draw all this fire!\",\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"Now that's something you don't see every day!\",\n",
    "    \"You know, what can I say? I die hard.\",\n",
    "    \"Gosh darn it Kris, where the heck are we?\",\n",
    "    \"This is a test voice message.\",\n",
    "    \"I swear the toilet was full of guacamole when I bought it!\",\n",
    "    \"Did you ever hear the Tragedy of Darth Plagueis the wise?\",\n",
    "    \"I thought not. It's not a story the Jedi would tell you, it's a sith legend.\",\n",
    "    \"Darth Plagueis was a dark lord of the Sith, so powerful and so wise\",\n",
    "    \"He could use the force to influence the midichlorians to create life.\",\n",
    "    \"Never gonna give you up. Never gonna let you down.\",\n",
    "    \"I am the Milkman. My milk is delicious.\",\n",
    "    \"I'm just like my country. I'm young, scrappy, and hungry, and I am not throwing away my shot.\",\n",
    "    \"I'm still a piece of garbage.\",\n",
    "    \"Looks like you're the first one here! Use the people tab on your watch to invite your friends to join you!\",\n",
    "]\n",
    "\n",
    "\n",
    "def random_utterance():\n",
    "    return utterances[random.randint(0, len(utterances) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880f502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My friends on TikTok send me memes, on Discord it's fucking memes.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_utterance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
