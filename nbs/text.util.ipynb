{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c7a3c0",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\" from https://github.com/keithito/tacotron \"\"\"\n",
    "\n",
    "'''\n",
    "Cleaners are transformations that run over the input text at both training and eval time.\n",
    "Cleaners can be selected by passing a comma-delimited list of cleaner names as the \"cleaners\"\n",
    "hyperparameter. Some cleaners are English-specific. You'll typically want to use:\n",
    "  1. \"english_cleaners\" for English text\n",
    "  2. \"transliteration_cleaners\" for non-English text that can be transliterated to ASCII using\n",
    "     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n",
    "  3. \"basic_cleaners\" if you do not want to transliterate (in this case, you should also update\n",
    "     the symbols in symbols.py to match your data).\n",
    "'''\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import curly_re\n",
    "\n",
    "\n",
    "# Regular expression matching whitespace:\n",
    "_whitespace_re = re.compile(r'\\s+')\n",
    "\n",
    "# List of (regular expression, replacement) pairs for abbreviations:\n",
    "_abbreviations = [(re.compile('\\\\b%s\\\\.' % x[0], re.IGNORECASE), x[1]) for x in [\n",
    "  ('mrs', 'misess'),\n",
    "  ('mr', 'mister'),\n",
    "  ('dr', 'doctor'),\n",
    "  ('st', 'saint'),\n",
    "  ('co', 'company'),\n",
    "  ('jr', 'junior'),\n",
    "  ('maj', 'major'),\n",
    "  ('gen', 'general'),\n",
    "  ('drs', 'doctors'),\n",
    "  ('rev', 'reverend'),\n",
    "  ('lt', 'lieutenant'),\n",
    "  ('hon', 'honorable'),\n",
    "  ('sgt', 'sergeant'),\n",
    "  ('capt', 'captain'),\n",
    "  ('esq', 'esquire'),\n",
    "  ('ltd', 'limited'),\n",
    "  ('col', 'colonel'),\n",
    "  ('ft', 'fort'),\n",
    "]]\n",
    "\n",
    "import inflect\n",
    "import re\n",
    "\n",
    "\n",
    "_inflect = inflect.engine()\n",
    "_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n",
    "_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n",
    "_pounds_re = re.compile(r'£([0-9\\,]*[0-9]+)')\n",
    "_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n",
    "_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n",
    "_number_re = re.compile(r'[0-9]+')\n",
    "\n",
    "\n",
    "def _remove_commas(m):\n",
    "    return m.group(1).replace(',', '')\n",
    "\n",
    "\n",
    "def _expand_decimal_point(m):\n",
    "    return m.group(1).replace('.', ' point ')\n",
    "\n",
    "\n",
    "def _expand_dollars(m):\n",
    "    match = m.group(1)\n",
    "    parts = match.split('.')\n",
    "    if len(parts) > 2:\n",
    "        return match + ' dollars'  # Unexpected format\n",
    "    dollars = int(parts[0]) if parts[0] else 0\n",
    "    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "    if dollars and cents:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n",
    "    elif dollars:\n",
    "        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n",
    "        return '%s %s' % (dollars, dollar_unit)\n",
    "    elif cents:\n",
    "        cent_unit = 'cent' if cents == 1 else 'cents'\n",
    "        return '%s %s' % (cents, cent_unit)\n",
    "    else:\n",
    "        return 'zero dollars'\n",
    "\n",
    "\n",
    "def _expand_ordinal(m):\n",
    "    return _inflect.number_to_words(m.group(0))\n",
    "\n",
    "\n",
    "def _expand_number(m):\n",
    "    num = int(m.group(0))\n",
    "    if num > 1000 and num < 3000:\n",
    "        if num == 2000:\n",
    "            return 'two thousand'\n",
    "        elif num > 2000 and num < 2010:\n",
    "            return 'two thousand ' + _inflect.number_to_words(num % 100)\n",
    "        elif num % 100 == 0:\n",
    "            return _inflect.number_to_words(num // 100) + ' hundred'\n",
    "        else:\n",
    "            return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n",
    "    else:\n",
    "        return _inflect.number_to_words(num, andword='')\n",
    "\n",
    "\n",
    "def normalize_numbers(text):\n",
    "    text = re.sub(_comma_number_re, _remove_commas, text)\n",
    "    text = re.sub(_pounds_re, r'\\1 pounds', text)\n",
    "    text = re.sub(_dollars_re, _expand_dollars, text)\n",
    "    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n",
    "    text = re.sub(_ordinal_re, _expand_ordinal, text)\n",
    "    text = re.sub(_number_re, _expand_number, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    for regex, replacement in _abbreviations:\n",
    "        text = re.sub(regex, replacement, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_numbers(text):\n",
    "    return normalize_numbers(text)\n",
    "\n",
    "\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def collapse_whitespace(text):\n",
    "    return re.sub(_whitespace_re, ' ', text)\n",
    "\n",
    "\n",
    "def convert_to_ascii(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "\n",
    "def basic_cleaners(text):\n",
    "    '''Basic pipeline that lowercases and collapses whitespace without transliteration.'''\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transliteration_cleaners(text):\n",
    "    '''Pipeline for non-English text that transliterates to ASCII.'''\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text\n",
    "\n",
    "def english_cleaners(text):\n",
    "    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n",
    "    text = convert_to_ascii(text)\n",
    "    text = lowercase(text)\n",
    "    text = expand_numbers(text)\n",
    "    text = expand_abbreviations(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "from uberduck_ml_dev.text.symbols import (\n",
    "    id_to_symbol,\n",
    "    symbols_to_sequence,\n",
    "    arpabet_to_sequence,\n",
    ")\n",
    "\n",
    "CLEANERS = {\n",
    "    \"english_cleaners\": english_cleaners,\n",
    "    \"basic_cleaners\": basic_cleaners,\n",
    "    \"transliteration_cleaners\": transliteration_cleaners,\n",
    "    \n",
    "}\n",
    "\n",
    "def clean_text(text, cleaner_names):\n",
    "    for name in cleaner_names:\n",
    "        cleaner = CLEANERS[name]\n",
    "        text = cleaner(text)\n",
    "    return text\n",
    "        \n",
    "\n",
    "\n",
    "def english_to_arpabet(english_text):\n",
    "    arpabet_symbols = g2p(english_text)\n",
    "\n",
    "\n",
    "def text_to_sequence(text, cleaner_names):\n",
    "    \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n",
    "    in it. For example, \"Turn left on {HH AW1 S S T AH0 N} Street.\"\n",
    "    Args:\n",
    "      text: string to convert to a sequence\n",
    "      cleaner_names: names of the cleaner functions to run the text through\n",
    "    Returns:\n",
    "      List of integers corresponding to the symbols in the text\n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "\n",
    "    # Check for curly braces and treat their contents as ARPAbet:\n",
    "    while len(text):\n",
    "        m = curly_re.match(text)\n",
    "        if not m:\n",
    "            sequence += symbols_to_sequence(clean_text(text, cleaner_names))\n",
    "            break\n",
    "        sequence += symbols_to_sequence(clean_text(m.group(1), cleaner_names))\n",
    "        sequence += arpabet_to_sequence(m.group(2))\n",
    "        text = m.group(3)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def sequence_to_text(sequence):\n",
    "    \"\"\"Converts a sequence of IDs back to a string\"\"\"\n",
    "    result = \"\"\n",
    "    for symbol_id in sequence:\n",
    "        if symbol_id in _id_to_symbol:\n",
    "            s = _id_to_symbol[symbol_id]\n",
    "            # Enclose ARPAbet back in curly braces:\n",
    "            if len(s) > 1 and s[0] == \"@\":\n",
    "                s = \"{%s}\" % s[1:]\n",
    "            result += s\n",
    "    return result.replace(\"}{\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889f6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 82, 79, 8, 90, 79, 88, 8, 83, 93, 8, 76, 86, 95, 79, 4]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ff38f043887f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The pen is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"english_cleaners\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The pen is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"english_cleaners\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m53\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m46\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m58\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The pen is {B L OW0}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"english_cleaners\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m53\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m46\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m117\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m122\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0msequence_to_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The pen is blue.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"english_cleaners\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"the pen is blue.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#skip\n",
    "print(text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"]))\n",
    "assert text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"]) == [57, 45, 42, 11, 53, 42, 51, 11, 46, 56, 11, 39, 49, 58, 42, 7]\n",
    "assert text_to_sequence(\"The pen is {B L OW0}\", [\"english_cleaners\"]) == [57, 45, 42, 11, 53, 42, 51, 11, 46, 56, 11, 88, 117, 122]\n",
    "assert sequence_to_text(text_to_sequence(\"The pen is blue.\", [\"english_cleaners\"])) == \"the pen is blue.\"\n",
    "assert sequence_to_text(text_to_sequence(\"The pen is {B L OW0}.\", [\"english_cleaners\"])) == \"the pen is {B L OW0}.\"\n",
    "assert len(text_to_sequence(\"{N AA1 T} {B AE1 D} {B AA1 R T} , {N AA1 T} {B AE1 D} {AE1 T} {AO1 L} .\", [\"english_cleaners\"])) == 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da332498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uberduck_ml_dev.text.symbols import symbols, symbol_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6493d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " \"'\": 1,\n",
       " '\"': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " ':': 5,\n",
       " ';': 6,\n",
       " '?': 7,\n",
       " ' ': 8,\n",
       " '#': 9,\n",
       " '%': 10,\n",
       " '&': 11,\n",
       " '*': 12,\n",
       " '+': 13,\n",
       " '-': 14,\n",
       " '/': 15,\n",
       " '[': 16,\n",
       " ']': 17,\n",
       " '(': 18,\n",
       " ')': 19,\n",
       " '_': 20,\n",
       " '@': 21,\n",
       " '©': 22,\n",
       " '°': 23,\n",
       " '½': 24,\n",
       " '—': 25,\n",
       " '₩': 26,\n",
       " '€': 27,\n",
       " '$': 28,\n",
       " 'á': 29,\n",
       " 'ç': 30,\n",
       " 'é': 31,\n",
       " 'ê': 32,\n",
       " 'ë': 33,\n",
       " 'ñ': 34,\n",
       " 'ö': 35,\n",
       " 'ø': 36,\n",
       " 'ć': 37,\n",
       " 'ž': 38,\n",
       " '0': 39,\n",
       " '1': 40,\n",
       " '2': 41,\n",
       " '3': 42,\n",
       " '4': 43,\n",
       " '5': 44,\n",
       " '6': 45,\n",
       " '7': 46,\n",
       " '8': 47,\n",
       " '9': 48,\n",
       " 'A': 49,\n",
       " 'B': 50,\n",
       " 'C': 51,\n",
       " 'D': 52,\n",
       " 'E': 53,\n",
       " 'F': 54,\n",
       " 'G': 55,\n",
       " 'H': 56,\n",
       " 'I': 57,\n",
       " 'J': 58,\n",
       " 'K': 59,\n",
       " 'L': 60,\n",
       " 'M': 61,\n",
       " 'N': 62,\n",
       " 'O': 63,\n",
       " 'P': 64,\n",
       " 'Q': 65,\n",
       " 'R': 66,\n",
       " 'S': 67,\n",
       " 'T': 68,\n",
       " 'U': 69,\n",
       " 'V': 70,\n",
       " 'W': 71,\n",
       " 'X': 72,\n",
       " 'Y': 73,\n",
       " 'Z': 74,\n",
       " 'a': 75,\n",
       " 'b': 76,\n",
       " 'c': 77,\n",
       " 'd': 78,\n",
       " 'e': 79,\n",
       " 'f': 80,\n",
       " 'g': 81,\n",
       " 'h': 82,\n",
       " 'i': 83,\n",
       " 'j': 84,\n",
       " 'k': 85,\n",
       " 'l': 86,\n",
       " 'm': 87,\n",
       " 'n': 88,\n",
       " 'o': 89,\n",
       " 'p': 90,\n",
       " 'q': 91,\n",
       " 'r': 92,\n",
       " 's': 93,\n",
       " 't': 94,\n",
       " 'u': 95,\n",
       " 'v': 96,\n",
       " 'w': 97,\n",
       " 'x': 98,\n",
       " 'y': 99,\n",
       " 'z': 100,\n",
       " '@AA': 101,\n",
       " '@AA0': 102,\n",
       " '@AA1': 103,\n",
       " '@AA2': 104,\n",
       " '@AE': 105,\n",
       " '@AE0': 106,\n",
       " '@AE1': 107,\n",
       " '@AE2': 108,\n",
       " '@AH': 109,\n",
       " '@AH0': 110,\n",
       " '@AH1': 111,\n",
       " '@AH2': 112,\n",
       " '@AO': 113,\n",
       " '@AO0': 114,\n",
       " '@AO1': 115,\n",
       " '@AO2': 116,\n",
       " '@AW': 117,\n",
       " '@AW0': 118,\n",
       " '@AW1': 119,\n",
       " '@AW2': 120,\n",
       " '@AY': 121,\n",
       " '@AY0': 122,\n",
       " '@AY1': 123,\n",
       " '@AY2': 124,\n",
       " '@B': 125,\n",
       " '@CH': 126,\n",
       " '@D': 127,\n",
       " '@DH': 128,\n",
       " '@EH': 129,\n",
       " '@EH0': 130,\n",
       " '@EH1': 131,\n",
       " '@EH2': 132,\n",
       " '@ER': 133,\n",
       " '@ER0': 134,\n",
       " '@ER1': 135,\n",
       " '@ER2': 136,\n",
       " '@EY': 137,\n",
       " '@EY0': 138,\n",
       " '@EY1': 139,\n",
       " '@EY2': 140,\n",
       " '@F': 141,\n",
       " '@G': 142,\n",
       " '@HH': 143,\n",
       " '@IH': 144,\n",
       " '@IH0': 145,\n",
       " '@IH1': 146,\n",
       " '@IH2': 147,\n",
       " '@IY': 148,\n",
       " '@IY0': 149,\n",
       " '@IY1': 150,\n",
       " '@IY2': 151,\n",
       " '@JH': 152,\n",
       " '@K': 153,\n",
       " '@L': 154,\n",
       " '@M': 155,\n",
       " '@N': 156,\n",
       " '@NG': 157,\n",
       " '@OW': 158,\n",
       " '@OW0': 159,\n",
       " '@OW1': 160,\n",
       " '@OW2': 161,\n",
       " '@OY': 162,\n",
       " '@OY0': 163,\n",
       " '@OY1': 164,\n",
       " '@OY2': 165,\n",
       " '@P': 166,\n",
       " '@R': 167,\n",
       " '@S': 168,\n",
       " '@SH': 169,\n",
       " '@T': 170,\n",
       " '@TH': 171,\n",
       " '@UH': 172,\n",
       " '@UH0': 173,\n",
       " '@UH1': 174,\n",
       " '@UH2': 175,\n",
       " '@UW': 176,\n",
       " '@UW0': 177,\n",
       " '@UW1': 178,\n",
       " '@UW2': 179,\n",
       " '@V': 180,\n",
       " '@W': 181,\n",
       " '@Y': 182,\n",
       " '@Z': 183,\n",
       " '@ZH': 184}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b9a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
