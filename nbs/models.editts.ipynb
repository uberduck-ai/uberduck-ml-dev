{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52619583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.editts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import random\n",
    "import math\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import monotonic_align\n",
    "\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, text_to_sequence_for_editts\n",
    "from uberduck_ml_dev.utils.utils import intersperse, intersperse_emphases\n",
    "from uberduck_ml_dev.models.gradtts import (\n",
    "    GradTTS,\n",
    "    sequence_mask,\n",
    "    generate_path,\n",
    "    fix_len_compatibility,\n",
    "    get_noise,\n",
    "    DEFAULTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def fix_len_compatibility_text_edit(length, num_downsamplings_in_unet=2):\n",
    "    while True:\n",
    "        if length % (2 ** num_downsamplings_in_unet) == 0:\n",
    "            return length\n",
    "        length -= 1\n",
    "\n",
    "\n",
    "class EdiTTS(torch.nn.Module):\n",
    "    def __init__(self, gradtts_model):\n",
    "        super(EdiTTS, self).__init__()\n",
    "        self.gradtts_model = gradtts_model\n",
    "\n",
    "    def infer_edit_content(\n",
    "        self,\n",
    "        text1,\n",
    "        text2,\n",
    "        n_timesteps=10,\n",
    "        symbol_set=\"gradtts\",\n",
    "        temperature=1.0,\n",
    "        stoc=False,\n",
    "        length_scale=1.0,\n",
    "        soften_mask=True,\n",
    "        n_soften_text=9,\n",
    "        n_soften=16,\n",
    "        amax=0.9,\n",
    "        amin=0.1,\n",
    "        intersperse_token=148,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        EdiTTS\n",
    "        Edit speech/audio via content substitution.\n",
    "        This function will substitute the desired portion of text2 into the specified location of text1.\n",
    "\n",
    "        Arguments:\n",
    "        text1 (str): text to substitute content in to. e.g. \"This is a | blue | pencil\"\n",
    "        text2 (str): text to substitute audio from. e.g. \"This is a | red | pen.\"\n",
    "        n_timesteps (int): number of steps to use for reverse diffusion in decoder.\n",
    "        symbol_set (str): symbol set key to lookup the symbol set\n",
    "        intersperse_token (int): value used for interspersing\n",
    "\n",
    "        Output:\n",
    "        y_dec1: Mel spectrogram of text1\n",
    "        y_dec2: Mel spectrogram of text2\n",
    "        y_dec_edit: Mel spectrogram of source of text2 substituted in to text1 via EdiTTS\n",
    "        y_dec_cat: Mel spectrogram of source of text2 substituted in to text1 via mel concatenation\n",
    "\n",
    "        Usage:\n",
    "        y_dec1, y_dec2, y_dec_edit, y_dec_cat = model.infer_edit_content(\"This is a | blue | pencil.\",\n",
    "                                                                    \"This is a | red | pen.\",\n",
    "                                                                    n_timesteps=10,\n",
    "                                                                    symbol_set=\"gradtts\")\n",
    "        y_dec1: \"this is a blue pencil\"\n",
    "        y_dec2: \"this is a red pen\"\n",
    "        y_dec_edit: \"this is a red pencil\" (EdiTTS)\n",
    "        y_dec_cat: \"this is a red pencil\" (Mel concatenation)\n",
    "\n",
    "        \"\"\"\n",
    "        sequence1, emphases1 = text_to_sequence_for_editts(\n",
    "            text1, cleaner_names=[\"english_cleaners\"], symbol_set=symbol_set\n",
    "        )\n",
    "        sequence2, emphases2 = text_to_sequence_for_editts(\n",
    "            text2, cleaner_names=[\"english_cleaners\"], symbol_set=symbol_set\n",
    "        )\n",
    "        x1 = torch.LongTensor(intersperse(sequence1, intersperse_token)).cuda()[None]\n",
    "        x2 = torch.LongTensor(intersperse(sequence2, intersperse_token)).cuda()[None]\n",
    "        emphases1 = intersperse_emphases(emphases1)\n",
    "        emphases2 = intersperse_emphases(emphases2)\n",
    "        x_lengths1 = torch.LongTensor([x1.shape[-1]]).cuda()\n",
    "        x_lengths2 = torch.LongTensor([x2.shape[-1]]).cuda()\n",
    "\n",
    "        y_dec1, y_dec2, y_dec_edit, y_dec_cat = self.edit_content(\n",
    "            x1,\n",
    "            x2,\n",
    "            x_lengths1,\n",
    "            x_lengths2,\n",
    "            emphases1,\n",
    "            emphases2,\n",
    "            n_timesteps,\n",
    "            temperature=temperature,\n",
    "            stoc=stoc,\n",
    "            length_scale=length_scale,\n",
    "            soften_mask=soften_mask,\n",
    "            n_soften_text=n_soften_text,\n",
    "            n_soften=n_soften,\n",
    "            amax=amax,\n",
    "            amin=amin,\n",
    "        )\n",
    "        return y_dec1, y_dec2, y_dec_edit, y_dec_cat\n",
    "\n",
    "    def infer_edit_content_with_source(\n",
    "        self,\n",
    "        mel,\n",
    "        sub_time_start,\n",
    "        sub_time_stop,\n",
    "        text,\n",
    "        n_timesteps=50,\n",
    "        desired_time=None,\n",
    "        symbol_set=\"gradtts\",\n",
    "        temperature=1.0,\n",
    "        stoc=False,\n",
    "        length_scale=1.0,\n",
    "        soften_mask=True,\n",
    "        n_soften_text=9,\n",
    "        n_soften=16,\n",
    "        amax=0.9,\n",
    "        amin=0.1,\n",
    "        intersperse_token=148,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        EdiTTS\n",
    "        Edit speech/audio via content substitution.\n",
    "        This function will substitute the specified time region of mel with the portion of text surrounded by |.\n",
    "\n",
    "        Arguments:\n",
    "        mel (torch.Tensor): text to generate and use for editing content . e.g. \"This is a | blue | pencil\"\n",
    "        sub_time_start (float): Starting time for substitution in mel (seconds)\n",
    "        sub_time_stop (float): Ending time for substitution in mel (seconds)\n",
    "        n_timesteps (int): number of steps to use for reverse diffusion in decoder\n",
    "        symbol_set (str): symbol set key to lookup the symbol set\n",
    "        desired_time (float): Length of time for audio to be substituted into mel (seconds)\n",
    "        intersperse_token (int): value used for interspersing\n",
    "\n",
    "        Output:\n",
    "        y_dec1: Mel spectrogram of text1\n",
    "        y_dec2: Mel spectrogram of text2\n",
    "        y_dec_edit: Mel spectrogram of source of text2 substituted in to text1 via EdiTTS\n",
    "        y_dec_cat: Mel spectrogram of source of text2 substituted in to text1 via mel concatenation\n",
    "\n",
    "        \"\"\"\n",
    "        sequence, emphases = text_to_sequence_for_editts(\n",
    "            text, cleaner_names=[\"english_cleaners\"], symbol_set=symbol_set\n",
    "        )\n",
    "        x = torch.LongTensor(intersperse(sequence, intersperse_token)).cuda()[None]\n",
    "        emphases = intersperse_emphases(emphases)\n",
    "        x_lengths = torch.LongTensor([x.shape[-1]]).cuda()\n",
    "\n",
    "        y_dec1, y_dec2, y_dec_edit, y_dec_cat = self.edit_content(\n",
    "            x2=x,\n",
    "            x2_lengths=x_lengths,\n",
    "            emphases2=emphases,\n",
    "            n_timesteps=n_timesteps,\n",
    "            mel1=mel,\n",
    "            i1=sub_time_start,\n",
    "            j1=sub_time_stop,\n",
    "            desired_time=desired_time,\n",
    "            temperature=temperature,\n",
    "            stoc=stoc,\n",
    "            length_scale=length_scale,\n",
    "            soften_mask=soften_mask,\n",
    "            n_soften_text=n_soften_text,\n",
    "            n_soften=n_soften,\n",
    "            amax=amax,\n",
    "            amin=amin,\n",
    "        )\n",
    "        return y_dec1, y_dec2, y_dec_edit, y_dec_cat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def edit_content(\n",
    "        self,\n",
    "        x1=None,\n",
    "        x2=None,\n",
    "        x1_lengths=None,\n",
    "        x2_lengths=None,\n",
    "        emphases1=None,\n",
    "        emphases2=None,\n",
    "        n_timesteps=50,\n",
    "        mel1=None,\n",
    "        i1=None,\n",
    "        j1=None,\n",
    "        desired_time=None,\n",
    "        temperature=1.0,\n",
    "        stoc=False,\n",
    "        length_scale=1.0,\n",
    "        soften_mask=True,\n",
    "        n_soften_text=9,\n",
    "        n_soften=16,\n",
    "        amax=0.9,\n",
    "        amin=0.1,\n",
    "    ):\n",
    "        def _process_input(x, x_lengths):\n",
    "            x, x_lengths = self.gradtts_model.relocate_input([x, x_lengths])\n",
    "\n",
    "            # encoded_text, durations, text_mask\n",
    "            mu_x, logw, x_mask = self.gradtts_model.encoder(x, x_lengths)\n",
    "            w = torch.exp(logw) * x_mask\n",
    "            w_ceil = torch.ceil(w) * length_scale\n",
    "\n",
    "            y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "            y_max_length = int(y_lengths.max())\n",
    "            y_max_length_ = fix_len_compatibility(y_max_length)\n",
    "\n",
    "            y_mask = (\n",
    "                sequence_mask(y_lengths, y_max_length_).unsqueeze(1).to(x_mask.dtype)\n",
    "            )\n",
    "            attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)\n",
    "            attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "            mu_y = torch.matmul(attn.squeeze(1).transpose(1, 2), mu_x.transpose(1, 2))\n",
    "            mu_y = mu_y.transpose(1, 2)  # [1, n_mels, T]\n",
    "            return mu_y, attn, y_mask, y_max_length, y_lengths\n",
    "\n",
    "        def _process_input_time_constraint(x, x_lengths, emphases, desired_time):\n",
    "            x, x_lengths = self.gradtts_model.relocate_input([x, x_lengths])\n",
    "\n",
    "            # encoded_text, durations, text_mask\n",
    "            mu_x, logw, x_mask = self.gradtts_model.encoder(x, x_lengths)\n",
    "            w = torch.exp(logw) * x_mask\n",
    "            w_ceil = torch.ceil(w)\n",
    "\n",
    "            # Add time constraint\n",
    "            w_slice = w_ceil.squeeze()[emphases[0][0] : emphases[0][1]]\n",
    "            time_scale = (\n",
    "                (desired_time * self.gradtts_model.sampling_rate)\n",
    "                / self.gradtts_model.hop_length\n",
    "            ) / torch.sum(w_slice)\n",
    "            w_ceil = w_ceil * time_scale\n",
    "\n",
    "            y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "            y_max_length = int(y_lengths.max())\n",
    "            y_max_length_ = fix_len_compatibility(y_max_length)\n",
    "\n",
    "            y_mask = (\n",
    "                sequence_mask(y_lengths, y_max_length_).unsqueeze(1).to(x_mask.dtype)\n",
    "            )\n",
    "            attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)\n",
    "            attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "            mu_y = torch.matmul(attn.squeeze(1).transpose(1, 2), mu_x.transpose(1, 2))\n",
    "            mu_y = mu_y.transpose(1, 2)  # [1, n_mels, T]\n",
    "            return mu_y, attn, y_mask, y_max_length, y_lengths\n",
    "\n",
    "        def _soften_juntions(\n",
    "            y_edit, y1, y2, y_edit_lengths, y1_lengths, y2_lengths, i1, j1, i2, j2\n",
    "        ):\n",
    "            for n in range(1, n_soften_text + 1):\n",
    "                alpha = (amax - amin) * (n_soften_text - n) / (n_soften_text - 1) + amin\n",
    "                if i1 - n >= 0 and i2 - n >= 0:\n",
    "                    y_edit[:, :, i1 - n] = (1 - alpha) * y1[:, :, i1 - n] + alpha * y2[\n",
    "                        :, :, i2 - n\n",
    "                    ]\n",
    "                if (\n",
    "                    i1 + (j2 - i2) + n < y_edit_lengths\n",
    "                    and j1 + (n - 1) < y1_lengths\n",
    "                    and j2 + (n - 1) < y2_lengths\n",
    "                ):\n",
    "                    y_edit[:, :, i1 + (j2 - i2) + (n - 1)] = (1 - alpha) * y1[\n",
    "                        :, :, j1 + (n - 1)\n",
    "                    ] + alpha * y2[:, :, j2 + (n - 1)]\n",
    "            return y_edit\n",
    "\n",
    "        if mel1 is not None:\n",
    "            assert len(x2) == 1 and x1 is None\n",
    "            assert emphases2 is not None\n",
    "            assert len(emphases2) == 1\n",
    "        else:\n",
    "            assert len(x1) == 1 and len(x2) == 1\n",
    "            assert emphases1 is not None and emphases2 is not None\n",
    "            assert len(emphases1) == 1 and len(emphases2) == 1\n",
    "\n",
    "        if mel1 is not None:\n",
    "            mu_y1 = mel1\n",
    "            y1_max_length = mel1.shape[-1]\n",
    "            y1_lengths = torch.LongTensor([mel1.shape[-1]]).cuda()\n",
    "        else:\n",
    "            mu_y1, attn1, y1_mask, y1_max_length, y1_lengths = _process_input(\n",
    "                x1, x1_lengths\n",
    "            )  # mu_y1: [1, n_mels, T]\n",
    "\n",
    "        if desired_time:\n",
    "            (\n",
    "                mu_y2,\n",
    "                attn2,\n",
    "                y2_mask,\n",
    "                y2_max_length,\n",
    "                y2_lengths,\n",
    "            ) = _process_input_time_constraint(\n",
    "                x2, x2_lengths, emphases2, desired_time,\n",
    "            )  # mu_y2: [1, n_mels, T]\n",
    "        else:\n",
    "            mu_y2, attn2, y2_mask, y2_max_length, y2_lengths = _process_input(\n",
    "                x2, x2_lengths\n",
    "            )  # mu_y2: [1, n_mels, T]\n",
    "\n",
    "        if not i1 and not j1:\n",
    "            attn1 = attn1.squeeze()  # [N, T]\n",
    "            i1 = (\n",
    "                attn1[: emphases1[0][0]].sum().long().item()\n",
    "                if emphases1[0][0] > 0\n",
    "                else 0\n",
    "            )\n",
    "            j1 = attn1[: emphases1[0][1]].sum().long().item()\n",
    "\n",
    "        attn2 = attn2.squeeze()  # [N, T]\n",
    "        i2 = attn2[: emphases2[0][0]].sum().long().item() if emphases2[0][0] > 0 else 0\n",
    "        j2 = attn2[: emphases2[0][1]].sum().long().item()\n",
    "\n",
    "        # Step 1. Direct concatenation\n",
    "        mu_y1_a, mu_y1_c = mu_y1[:, :, :i1], mu_y1[:, :, j1:y1_lengths]\n",
    "        mu_y2_b = mu_y2[:, :, i2:j2]\n",
    "        mu_y_edit = torch.cat((mu_y1_a, mu_y2_b, mu_y1_c), dim=2)\n",
    "        y_edit_lengths = int(mu_y_edit.shape[2])\n",
    "\n",
    "        # Step 2. Soften junctions\n",
    "        mu_y_edit = _soften_juntions(\n",
    "            mu_y_edit,\n",
    "            mu_y1,\n",
    "            mu_y2,\n",
    "            y_edit_lengths,\n",
    "            y1_lengths,\n",
    "            y2_lengths,\n",
    "            i1,\n",
    "            j1,\n",
    "            i2,\n",
    "            j2,\n",
    "        )\n",
    "\n",
    "        y_edit_length_ = fix_len_compatibility_text_edit(y_edit_lengths)\n",
    "        y_edit_lengths_tensor = torch.tensor([y_edit_lengths]).long().to(x2.device)\n",
    "        y_edit_mask_for_scorenet = (\n",
    "            sequence_mask(y_edit_lengths_tensor, y_edit_length_)\n",
    "            .unsqueeze(1)\n",
    "            .to(mu_y1.dtype)\n",
    "        )\n",
    "\n",
    "        eps1 = torch.randn_like(mu_y1, device=mu_y1.device) / temperature\n",
    "        eps2 = torch.randn_like(mu_y2, device=mu_y1.device) / temperature\n",
    "        eps_edit = torch.cat(\n",
    "            (eps1[:, :, :i1], eps2[:, :, i2:j2], eps1[:, :, j1:y1_lengths]), dim=2\n",
    "        )\n",
    "        z1 = mu_y1 + eps1\n",
    "        z2 = mu_y2 + eps2\n",
    "        z_edit = mu_y_edit + eps_edit\n",
    "\n",
    "        if z_edit.shape[2] < y_edit_length_:\n",
    "            pad = y_edit_length_ - z_edit.shape[2]\n",
    "            zeros = torch.zeros_like(z_edit[:, :, :pad])\n",
    "            z_edit = torch.cat((z_edit, zeros), dim=2)\n",
    "            mu_y_edit = torch.cat((mu_y_edit, zeros), dim=2)\n",
    "        elif z_edit.shape[2] > y_edit_length_:\n",
    "            res = z_edit.shape[2] - y_edit_length_\n",
    "            z_edit = z_edit[:, :, :-res]\n",
    "            mu_y_edit = mu_y_edit[:, :, :-res]\n",
    "\n",
    "        y_edit_mask_for_gradient = torch.zeros_like(mu_y_edit[:, :1, :])\n",
    "        y_edit_mask_for_gradient[:, :, i1 : i1 + (j2 - i2)] = 1\n",
    "\n",
    "        if mel1 is not None:\n",
    "            dec1 = mu_y1\n",
    "        else:\n",
    "            dec1 = self.gradtts_model.decoder(z1, y1_mask, mu_y1, n_timesteps, stoc)\n",
    "\n",
    "        dec2, dec_edit = self.double_forward_text(\n",
    "            z2,\n",
    "            z_edit,\n",
    "            mu_y2,\n",
    "            mu_y_edit,\n",
    "            y2_mask,\n",
    "            y_edit_mask_for_scorenet,\n",
    "            y_edit_mask_for_gradient,\n",
    "            i1,\n",
    "            j1,\n",
    "            i2,\n",
    "            j2,\n",
    "            n_timesteps,\n",
    "            stoc,\n",
    "            soften_mask,\n",
    "            n_soften,\n",
    "        )\n",
    "\n",
    "        dec1 = dec1[:, :, :y1_max_length]\n",
    "        dec2 = dec2[:, :, :y2_max_length]\n",
    "        dec_edit = dec_edit[:, :, :y_edit_lengths]\n",
    "        dec_cat = torch.cat(\n",
    "            (dec1[:, :, :i1], dec2[:, :, i2:j2], dec1[:, :, j1:y1_lengths]), dim=2\n",
    "        )\n",
    "\n",
    "        return dec1, dec2, dec_edit, dec_cat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def double_forward_text(\n",
    "        self,\n",
    "        z,\n",
    "        z_edit,\n",
    "        mu,\n",
    "        mu_edit,\n",
    "        mask,\n",
    "        mask_edit_net,\n",
    "        mask_edit_grad,\n",
    "        i1,\n",
    "        j1,\n",
    "        i2,\n",
    "        j2,\n",
    "        n_timesteps,\n",
    "        stoc=False,\n",
    "        soften_mask=True,\n",
    "        n_soften=20,\n",
    "    ):\n",
    "        if soften_mask:\n",
    "            kernel = [\n",
    "                2 ** ((n_soften - 1) - abs(n_soften - 1 - i))\n",
    "                for i in range(2 * n_soften - 1)\n",
    "            ]  # [1, 2, 4, ..., 2^n_soften , 2^(n_soften-1), ..., 2, 1]\n",
    "            kernel = [i / sum(kernel[: len(kernel) // 2 + 1]) for i in kernel]\n",
    "            w = (\n",
    "                torch.tensor(kernel)\n",
    "                .view(1, 1, 1, len(kernel))\n",
    "                .to(mask_edit_grad.device)\n",
    "                .float()\n",
    "            )\n",
    "            mask_edit_soft = mask_edit_grad.unsqueeze(1).contiguous()\n",
    "            mask_edit_soft = F.pad(\n",
    "                mask_edit_soft,\n",
    "                (len(kernel) // 2, len(kernel) // 2, 0, 0),\n",
    "                mode=\"replicate\",\n",
    "            )\n",
    "            mask_edit_soft = F.conv2d(mask_edit_soft, w, bias=None, stride=1,)\n",
    "            mask_edit_soft = mask_edit_soft.squeeze(1)\n",
    "            mask_edit_grad = mask_edit_grad + (1 - mask_edit_grad) * mask_edit_soft\n",
    "\n",
    "        h = 1.0 / n_timesteps\n",
    "        xt = z * mask\n",
    "        xt_edit = z_edit * mask_edit_net\n",
    "\n",
    "        for i in range(n_timesteps):\n",
    "            t = (1.0 - (i + 0.5) * h) * torch.ones(\n",
    "                z.shape[0], dtype=z.dtype, device=z.device\n",
    "            )\n",
    "            time = t.unsqueeze(-1).unsqueeze(-1)\n",
    "            noise_t = get_noise(\n",
    "                time,\n",
    "                self.gradtts_model.decoder.beta_min,\n",
    "                self.gradtts_model.decoder.beta_max,\n",
    "                cumulative=False,\n",
    "            )\n",
    "            if stoc:  # adds stochastic term\n",
    "                # NOTE: should not come here\n",
    "                assert False\n",
    "                dxt_det = 0.5 * (mu - xt) - self.gradtts_model.decoder.estimator(\n",
    "                    xt, mask, mu, t\n",
    "                )\n",
    "                dxt_det = dxt_det * noise_t * h\n",
    "                dxt_stoc = torch.randn(\n",
    "                    z.shape, dtype=z.dtype, device=z.device, requires_grad=False\n",
    "                )\n",
    "                dxt_stoc = dxt_stoc * torch.sqrt(noise_t * h)\n",
    "                dxt = dxt_det + dxt_stoc\n",
    "            else:\n",
    "                dxt = 0.5 * (\n",
    "                    mu - xt - self.gradtts_model.decoder.estimator(xt, mask, mu, t)\n",
    "                )\n",
    "                dxt = dxt * noise_t * h\n",
    "                dxt_edit = 0.5 * (\n",
    "                    mu_edit\n",
    "                    - xt_edit\n",
    "                    - self.gradtts_model.decoder.estimator(\n",
    "                        xt_edit, mask_edit_net, mu_edit, t\n",
    "                    )\n",
    "                )\n",
    "                dxt_edit = dxt_edit * noise_t * h\n",
    "\n",
    "            xt = (xt - dxt) * mask\n",
    "\n",
    "            dxt_trg = torch.zeros_like(dxt_edit)\n",
    "            dxt_trg[:, :, i1 : i1 + (j2 - i2)] = dxt[:, :, i2:j2]\n",
    "\n",
    "            xt_edit = (\n",
    "                xt_edit - (mask_edit_grad * dxt_trg + (1 - mask_edit_grad) * dxt_edit)\n",
    "            ) * mask_edit_net\n",
    "\n",
    "        return xt, xt_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dabd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "checkpoint = \"../models/grad_1750.pt\"\n",
    "model = GradTTS(DEFAULTS)\n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eea1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "editts = EdiTTS(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "y_dec1, y_dec2, y_dec_edit, y_dec_cat = editts.infer_edit_content(\n",
    "    \"This is a | blue | pencil\", \"This is a | red | pencil\"\n",
    ")\n",
    "\n",
    "for mel in [y_dec1, y_dec2, y_dec_edit, y_dec_cat]:\n",
    "    assert mel.shape[0] == 1\n",
    "    assert mel.shape[1] == 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e11050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "i1 = int(0.56 * DEFAULTS.sampling_rate / DEFAULTS.hop_length)\n",
    "j1 = int(0.804 * DEFAULTS.sampling_rate / DEFAULTS.hop_length)\n",
    "y_dec1, y_dec2, y_dec_edit, y_dec_cat = editts.infer_edit_content_with_source(\n",
    "    y_dec1, i1, j1, \"I ate | pancakes | for breakfast\"\n",
    ")\n",
    "\n",
    "for mel in [y_dec1, y_dec2, y_dec_edit, y_dec_cat]:\n",
    "    assert mel.shape[0] == 1\n",
    "    assert mel.shape[1] == 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "from uberduck_ml_dev.vocoders.hifigan import HiFiGanGenerator\n",
    "\n",
    "hifigan = HiFiGanGenerator(\n",
    "    config=\"../models/hifigan-config.json\",\n",
    "    checkpoint=\"../models/gen_02640000_studio\",\n",
    "    cudnn_enabled=True,\n",
    ")\n",
    "audio_1 = hifigan.infer(y_dec1)\n",
    "audio_2 = hifigan.infer(y_dec2)\n",
    "audio_edit = hifigan.infer(y_dec_edit)\n",
    "audio_cat = hifigan.infer(y_dec_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# ipd.Audio(audio_1, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e0e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "\n",
    "# ipd.Audio(audio_2, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2af22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
