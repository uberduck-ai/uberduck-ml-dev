{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64bab1d",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385fe3f",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16881815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def load_filepaths_and_text(filename: str, split: str = \"|\"):\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "def synthesize_speakerids2(filelists, fix_indices_index=None):\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict_out = {}\n",
    "    for f in range(len(filelists)):\n",
    "        data = load_filepaths_and_text(filelists[f])\n",
    "        data_dict[filelists[f]] = pd.DataFrame(data)\n",
    "\n",
    "    source_files = list(data_dict.keys())\n",
    "\n",
    "    speaker_offset = {}\n",
    "    nfilelist = len(filelists)\n",
    "    reserved_speakers = np.unique(data_dict[filelists[fix_indices_index]].iloc[:, 2])\n",
    "\n",
    "    for s in range(nfilelist):\n",
    "        source_file = filelists[s]\n",
    "        data = data_dict[source_file]\n",
    "        if s != fix_indices_index:\n",
    "            speakers = np.unique(data.iloc[:, 2])\n",
    "            overlap = np.where(np.isin(speakers, reserved_speakers))[0]\n",
    "            reserved_speakers_temp = np.union1d(speakers, reserved_speakers)\n",
    "            newindices = np.setdiff1d(\n",
    "                list(range(len(reserved_speakers) + len(speakers))),\n",
    "                reserved_speakers_temp,\n",
    "            )[: len(overlap)]\n",
    "            for o in range(len(overlap)):\n",
    "                data.iloc[np.where(data.iloc[:, 2] == overlap[o])[0], 2] = newindices[o]\n",
    "\n",
    "            data_dict_out[source_file] = data\n",
    "            speakers = np.unique(data.iloc[:, 2])\n",
    "            reserved_speakers = np.union1d(speakers, reserved_speakers)\n",
    "        else:\n",
    "            data_dict_out[source_file] = data\n",
    "    return data_dict_out\n",
    "\n",
    "\n",
    "def parse_vctk(root: str):\n",
    "    \"\"\"Parse VCTK dataset and return a dict representation.\"\"\"\n",
    "    wav_dir = os.path.join(root, \"wav48_silence_trimmed\")\n",
    "    txt_dir = os.path.join(root, \"txt\")\n",
    "    speaker_wavs = os.listdir(wav_dir)\n",
    "    speaker_txts = os.listdir(txt_dir)\n",
    "    speakers = list(set(speaker_wavs) & set(speaker_txts))\n",
    "    output_dict = {}\n",
    "    for speaker in speakers:\n",
    "        speaker_wav_dir = os.path.join(wav_dir, speaker)\n",
    "        speaker_txt_dir = os.path.join(txt_dir, speaker)\n",
    "        wav_files_speaker = np.asarray(os.listdir(speaker_wav_dir))\n",
    "        txt_files_speaker = np.asarray(os.listdir(speaker_txt_dir))\n",
    "\n",
    "        nwavfiles = len(wav_files_speaker)\n",
    "\n",
    "        transcription_basenames = np.asarray([t[:8] for t in txt_files_speaker])\n",
    "        audio_basenames = np.asarray([w[:8] for w in wav_files_speaker])\n",
    "        mic = np.asarray([w[12] for w in wav_files_speaker])\n",
    "        mic1_ind = mic == \"1\"\n",
    "        wav_files_speaker = wav_files_speaker[mic1_ind]\n",
    "        audio_basenames = audio_basenames[mic1_ind]\n",
    "\n",
    "        combined_files = np.intersect1d(transcription_basenames, audio_basenames)\n",
    "        matching_inds1 = np.where(np.isin(transcription_basenames, combined_files))[0]\n",
    "        matching_inds2 = np.where(np.isin(audio_basenames, combined_files))[0]\n",
    "        inds1 = matching_inds1[transcription_basenames[matching_inds1].argsort()]\n",
    "        inds2 = matching_inds2[audio_basenames[matching_inds2].argsort()]\n",
    "        txt_files_speaker = txt_files_speaker[inds1]\n",
    "        wav_files_speaker = wav_files_speaker[inds2]\n",
    "        texts, wavs = [], []\n",
    "        for text_basename, wav_basename in zip(txt_files_speaker, wav_files_speaker):\n",
    "            text_file = os.path.join(speaker_txt_dir, text_basename)\n",
    "            with open(text_file) as f:\n",
    "                contents = f.read().strip(\"\\n\")\n",
    "            texts.append(contents)\n",
    "            wav_file = os.path.join(speaker_wav_dir, wav_basename)\n",
    "            wavs.append(wav_file)\n",
    "\n",
    "        if len(wavs):\n",
    "            output_dict[speaker] = list(zip(texts, wavs))\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "    data = pd.read_csv(mellotron_filelist, sep=\"|\", header=None, error_bad_lines=False)\n",
    "\n",
    "    data[0] = data[0].str[17:]\n",
    "\n",
    "    data[0] = source_folder + data[0].astype(str)\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_speakerid(data, speaker_key=0):\n",
    "    if data.shape[1] == 3:\n",
    "        if type(data[2]) == int:\n",
    "            pass\n",
    "        else:\n",
    "            speaker_ids = np.asarray(\n",
    "                np.ones(data.shape[0], dtype=int) * speaker_key, dtype=int\n",
    "            )\n",
    "            data[2] = speaker_ids\n",
    "    if data.shape[1] == 2:\n",
    "        speaker_ids = np.asarray(\n",
    "            np.ones(data.shape[0], dtype=int) * speaker_key, dtype=int\n",
    "        )\n",
    "        data[2] = speaker_ids\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_libritts_mellotron(source_folder, mellotron_filelist):\n",
    "    data = load_filepaths_and_text(mellotron_filelist)\n",
    "    data = pd.DataFrame(data)\n",
    "    data[0] = data[0].str[17:]\n",
    "\n",
    "    data[0] = source_folder + data[0].astype(str)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_uberduck(source_folder):\n",
    "    source_file = source_folder + \"/all.txt\"\n",
    "    data = load_filepaths_and_text(source_file)\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "    nsamp = data.shape[0]\n",
    "    data[0] = source_folder + \"/\" + data[0].astype(str)\n",
    "    output = add_speakerid(data, speaker_key=0)\n",
    "\n",
    "    for i in range(output.shape[0]):\n",
    "        loaded = librosa.load(output.iloc[i, 0])\n",
    "        sf.write(output.iloc[i, 0], loaded[0], loaded[1])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def parse_ljspeech(source_folder):\n",
    "    source_file = source_folder + \"/metadata.csv\"\n",
    "    data = load_filepaths_and_text(source_file)\n",
    "    data = pd.DataFrame(data)\n",
    "    nsamp = data.shape[0]\n",
    "\n",
    "    data[0] = source_folder + \"/wavs/\" + data[0].astype(str)\n",
    "    output = add_speakerid(data, speaker_key=0)\n",
    "    for i in range(output.shape[0]):\n",
    "        output.iloc[i, 0] = output.iloc[i, 0] + \".wav\"\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_alignment_metrics(alignments, average_across_batch=True):\n",
    "    \"\"\"See https://github.com/NVIDIA/tacotron2/pull/284,\n",
    "    https://github.com/CookiePPP/cookietts/blob/c871f5f7b5790656d5b57bcd9e63946a2da52f0f/CookieTTS/utils/model/utils.py#L59\"\"\"\n",
    "    alignments = alignments.transpose(1, 2)  # [B, dec, enc] -> [B, enc, dec]\n",
    "    input_lengths = torch.ones(alignments.size(0), device=alignments.device) * (\n",
    "        alignments.shape[1] - 1\n",
    "    )  # [B]\n",
    "    output_lengths = torch.ones(alignments.size(0), device=alignments.device) * (\n",
    "        alignments.shape[2] - 1\n",
    "    )  # [B]\n",
    "    batch_size = alignments.size(0)\n",
    "    optimums = torch.sqrt(\n",
    "        input_lengths.double().pow(2) + output_lengths.double().pow(2)\n",
    "    ).view(batch_size)\n",
    "\n",
    "    # [B, enc, dec] -> [B, dec], [B, dec]\n",
    "    values, cur_idxs = torch.max(alignments, 1)\n",
    "\n",
    "    cur_idxs = cur_idxs.float()\n",
    "    prev_indx = torch.cat((cur_idxs[:, 0][:, None], cur_idxs[:, :-1]), dim=1)\n",
    "    dist = ((prev_indx - cur_idxs).pow(2) + 1).pow(0.5)  # [B, dec]\n",
    "    dist.masked_fill_(\n",
    "        ~get_mask_from_lengths(output_lengths, max_len=dist.size(1)), 0.0\n",
    "    )  # set dist of padded to zero\n",
    "    dist = dist.sum(dim=(1))  # get total dist for each B\n",
    "    diagonalness = (dist + 1.4142135) / optimums  # dist / optimal dist\n",
    "\n",
    "    maxes = alignments.max(axis=1)[0].mean(axis=1)\n",
    "    if average_across_batch:\n",
    "        diagonalness = diagonalness.mean()\n",
    "        max_ = maxes.mean()\n",
    "\n",
    "    output = {}\n",
    "    output[\"diagonalness\"] = diagonalness\n",
    "    output[\"max\"] = max_\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f470161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import get_window\n",
    "import librosa.util as librosa_util\n",
    "\n",
    "\n",
    "def window_sumsquare(\n",
    "    window,\n",
    "    n_frames,\n",
    "    hop_length=200,\n",
    "    win_length=800,\n",
    "    n_fft=800,\n",
    "    dtype=np.float32,\n",
    "    norm=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    # from librosa 0.6\n",
    "    Compute the sum-square envelope of a window function at a given hop length.\n",
    "\n",
    "    This is used to estimate modulation effects induced by windowing\n",
    "    observations in short-time fourier transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window : string, tuple, number, callable, or list-like\n",
    "        Window specification, as in `get_window`\n",
    "\n",
    "    n_frames : int > 0\n",
    "        The number of analysis frames\n",
    "\n",
    "    hop_length : int > 0\n",
    "        The number of samples to advance between frames\n",
    "\n",
    "    win_length : [optional]\n",
    "        The length of the window function.  By default, this matches `n_fft`.\n",
    "\n",
    "    n_fft : int > 0\n",
    "        The length of each analysis frame.\n",
    "\n",
    "    dtype : np.dtype\n",
    "        The data type of the output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
    "        The sum-squared envelope of the window function\n",
    "    \"\"\"\n",
    "    if win_length is None:\n",
    "        win_length = n_fft\n",
    "\n",
    "    n = n_fft + hop_length * (n_frames - 1)\n",
    "    x = np.zeros(n, dtype=dtype)\n",
    "\n",
    "    # Compute the squared window at the desired length\n",
    "    win_sq = get_window(window, win_length, fftbins=True)\n",
    "    win_sq = librosa_util.normalize(win_sq, norm=norm) ** 2\n",
    "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
    "\n",
    "    # Fill the envelope\n",
    "    for i in range(n_frames):\n",
    "        sample = i * hop_length\n",
    "        x[sample : min(n, sample + n_fft)] += win_sq[: max(0, min(n_fft, n - sample))]\n",
    "    return x\n",
    "\n",
    "\n",
    "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    magnitudes: spectrogram magnitudes\n",
    "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
    "    \"\"\"\n",
    "\n",
    "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
    "    angles = angles.astype(np.float32)\n",
    "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
    "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        _, angles = stft_fn.transform(signal)\n",
    "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def dynamic_range_decompression(x, C=1):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor used to compress\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1744f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0415e+00, -3.7111e-01, -3.2838e-01, -6.2205e-01, -2.6169e-01,\n",
       "         -9.8103e-01, -1.8675e+00, -7.7150e-01, -1.7760e-01, -6.8241e-01],\n",
       "        [-5.1025e-01, -1.1788e+00,  1.2456e+00,  1.1683e+00, -1.5050e+00,\n",
       "          3.8878e-01,  3.4331e-01, -4.6678e-02, -4.9398e-01, -1.3587e+00],\n",
       "        [-2.0366e+00, -2.1385e-01,  1.2572e-01,  1.0080e+00, -4.1069e-01,\n",
       "         -1.1727e+00, -5.8472e-01, -1.5301e-01, -7.7718e-01, -1.4033e-01],\n",
       "        [-8.1851e-01,  1.1898e+00,  3.7606e-01, -2.1779e+00,  2.1037e-01,\n",
       "         -1.0227e+00,  7.8290e-01,  9.2825e-01,  1.7582e+00, -6.0930e-02],\n",
       "        [-2.2755e+00, -1.9536e+00,  2.1239e-01, -6.8103e-01, -1.1368e-02,\n",
       "          5.7643e-01, -6.6171e-01, -3.3325e-01, -8.7990e-01,  1.0076e+00],\n",
       "        [-3.9319e-01,  8.9259e-01, -6.4939e-01, -4.6909e-01, -8.9252e-01,\n",
       "          9.5496e-01,  5.3429e-02, -1.4436e-01, -1.6097e-01, -3.0059e-01],\n",
       "        [-3.0589e-01, -2.9385e-02, -1.1399e+00,  1.7461e+00,  1.1013e-01,\n",
       "          4.2333e-01, -1.7093e+00,  5.6877e-02,  9.2514e-01,  9.9316e-01],\n",
       "        [ 4.9825e-01, -1.1996e+00,  1.6417e+00, -9.8894e-01, -4.0618e-01,\n",
       "         -7.6356e-01, -9.5752e-01,  1.1703e+00,  1.3896e-01, -3.3922e-01],\n",
       "        [ 8.8861e-01,  5.1271e-01, -1.6149e+00,  5.4874e-01,  5.3368e-02,\n",
       "          1.0879e+00,  1.5214e-01, -1.4647e-01,  6.3347e-01, -1.5482e-03],\n",
       "        [-5.4721e-01,  1.2666e+00, -1.2289e+00,  2.3877e+00,  5.3439e-01,\n",
       "         -9.8616e-01, -1.8520e-01, -1.1032e+00,  7.2345e-01, -7.1658e-01]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_gpu(torch.randn(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28796e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(lengths: torch.Tensor, max_len: int = 0):\n",
    "    \"\"\"Return a mask matrix. Unmasked entires are true.\"\"\"\n",
    "    if max_len == 0:\n",
    "        max_len = int(torch.max(lengths).item())\n",
    "    ids = torch.arange(0, max_len, device=lengths.device, dtype=torch.long)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af55f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert (\n",
    "    get_mask_from_lengths(torch.LongTensor([1, 3, 2, 1]))\n",
    "    == torch.Tensor(\n",
    "        [\n",
    "            [True, False, False],\n",
    "            [True, True, True],\n",
    "            [True, True, False],\n",
    "            [True, False, False],\n",
    "        ]\n",
    "    )\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def subsequent_mask(length):\n",
    "    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37dff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (subsequent_mask(2) == torch.tensor([[[1, 0], [1, 1]]])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_pad_shape(pad_shape):\n",
    "    \"\"\"Reverse, then flatten a list of lists.\"\"\"\n",
    "    l = pad_shape[::-1]\n",
    "    pad_shape = [item for sublist in l for item in sublist]\n",
    "    return pad_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7678bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pad_shape([[1, 2], [3, 4], [5, 6, 7]]) == [5, 6, 7, 3, 4, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56693e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sequence_mask(length, max_length=None):\n",
    "    \"\"\"The same as get_mask_from_lengths\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    sequence_mask(torch.tensor([1, 3, 2, 1]))\n",
    "    == torch.Tensor(\n",
    "        [\n",
    "            [True, False, False],\n",
    "            [True, True, True],\n",
    "            [True, True, False],\n",
    "            [True, False, False],\n",
    "        ]\n",
    "    )\n",
    ").all()\n",
    "assert (\n",
    "    sequence_mask(torch.tensor([1, 3, 2, 1]), 4)\n",
    "    == torch.Tensor(\n",
    "        [\n",
    "            [True, False, False, False],\n",
    "            [True, True, True, False],\n",
    "            [True, True, False, False],\n",
    "            [True, False, False, False],\n",
    "        ]\n",
    "    )\n",
    ").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def generate_path(duration, mask):\n",
    "    \"\"\"\n",
    "    duration: [b, 1, t_x]\n",
    "    mask: [b, 1, t_y, t_x]\n",
    "    \"\"\"\n",
    "    device = duration.device\n",
    "\n",
    "    b, _, t_y, t_x = mask.shape\n",
    "    cum_duration = torch.cumsum(duration, -1)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n",
    "    path = path.view(b, t_x, t_y)\n",
    "    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n",
    "    path = path.unsqueeze(1).transpose(2, 3) * mask\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d134ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def slice_segments(x, ids_str, segment_size=4):\n",
    "    ret = torch.zeros_like(x[:, :, :segment_size])\n",
    "    for i in range(x.size(0)):\n",
    "        idx_str = ids_str[i]\n",
    "        idx_end = idx_str + segment_size\n",
    "        ret[i] = x[i, :, idx_str:idx_end]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def rand_slice_segments(x, x_lengths=None, segment_size=4):\n",
    "    b, d, t = x.size()\n",
    "    if x_lengths is None:\n",
    "        x_lengths = t\n",
    "    ids_str_max = x_lengths - segment_size + 1\n",
    "    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n",
    "    ret = slice_segments(x, ids_str, segment_size)\n",
    "    return ret, ids_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c767aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9547a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size * dilation - dilation) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e281e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@torch.jit.script\n",
    "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n",
    "    n_channels_int = n_channels[0]\n",
    "    in_act = input_a + input_b\n",
    "    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n",
    "    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n",
    "    acts = t_act * s_act\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032fa9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clip_grad_value_(parameters, clip_value, norm_type=2):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    norm_type = float(norm_type)\n",
    "    if clip_value is not None:\n",
    "        clip_value = float(clip_value)\n",
    "\n",
    "    total_norm = 0\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.data.norm(norm_type)\n",
    "        total_norm += param_norm.item() ** norm_type\n",
    "        if clip_value is not None:\n",
    "            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n",
    "    total_norm = total_norm ** (1.0 / norm_type)\n",
    "    return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f1ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def intersperse(lst, item):\n",
    "    result = [item] * (len(lst) * 2 + 1)\n",
    "    result[1::2] = lst\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ff5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersperse([1, 2, 3, 4], 0) == [0, 1, 0, 2, 0, 3, 0, 4, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
