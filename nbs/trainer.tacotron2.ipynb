{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d43fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trainer.tacotron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from random import choice, randint\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "from uberduck_ml_dev.models.tacotron2 import Tacotron2\n",
    "from uberduck_ml_dev.utils.plot import save_figure_to_numpy\n",
    "from uberduck_ml_dev.utils.utils import reduce_tensor\n",
    "from uberduck_ml_dev.monitoring.statistics import get_alignment_metrics\n",
    "\n",
    "\n",
    "class Tacotron2Loss(nn.Module):\n",
    "    def __init__(self, pos_weight):\n",
    "        if pos_weight is not None:\n",
    "            self.pos_weight = torch.tensor(pos_weight)\n",
    "        else:\n",
    "            self.pos_weight = pos_weight\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, model_output: List, target: List):\n",
    "        mel_target, gate_target = target[0], target[1]\n",
    "        mel_target.requires_grad = False\n",
    "        gate_target.requires_grad = False\n",
    "        mel_out, mel_out_postnet, gate_out, _ = model_output\n",
    "        mel_loss_batch = nn.MSELoss(reduction=\"none\")(mel_out, mel_target).mean(\n",
    "            axis=[1, 2]\n",
    "        ) + nn.MSELoss(reduction=\"none\")(mel_out_postnet, mel_target).mean(axis=[1, 2])\n",
    "\n",
    "        mel_loss = mel_loss_batch.mean()\n",
    "\n",
    "        gate_loss_batch = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=self.pos_weight, reduce=False\n",
    "        )(gate_out, gate_target).mean(axis=[1])\n",
    "        gate_loss = torch.mean(gate_loss_batch)\n",
    "\n",
    "        return mel_loss, gate_loss, mel_loss_batch, gate_loss_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16004eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from uberduck_ml_dev.models.common import MelSTFT\n",
    "from uberduck_ml_dev.models.torchmoji import TorchMojiInterface\n",
    "from uberduck_ml_dev.utils.plot import (\n",
    "    plot_attention,\n",
    "    plot_gate_outputs,\n",
    "    plot_spectrogram,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import text_to_sequence, random_utterance\n",
    "from uberduck_ml_dev.trainer.base import TTSTrainer\n",
    "from uberduck_ml_dev.data_loader import TextMelDataset, TextMelCollate\n",
    "import pdb\n",
    "\n",
    "\n",
    "class Tacotron2Trainer(TTSTrainer):\n",
    "\n",
    "    REQUIRED_HPARAMS = [\n",
    "        \"audiopaths_and_text\",\n",
    "        \"checkpoint_name\",\n",
    "        \"checkpoint_path\",\n",
    "        \"epochs\",\n",
    "        \"mel_fmax\",\n",
    "        \"mel_fmin\",\n",
    "        \"n_mel_channels\",\n",
    "        \"text_cleaners\",\n",
    "        \"pos_weight\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if self.hparams.get(\"gst_type\") == \"torchmoji\":\n",
    "            assert self.hparams.get(\n",
    "                \"torchmoji_vocabulary_file\"\n",
    "            ), \"torchmoji_vocabulary_file must be set\"\n",
    "            assert self.hparams.get(\n",
    "                \"torchmoji_model_file\"\n",
    "            ), \"torchmoji_model_file must be set\"\n",
    "            assert self.hparams.get(\"gst_dim\"), \"gst_dim must be set\"\n",
    "\n",
    "            self.torchmoji = TorchMojiInterface(\n",
    "                self.hparams.get(\"torchmoji_vocabulary_file\"),\n",
    "                self.hparams.get(\"torchmoji_model_file\"),\n",
    "            )\n",
    "            self.compute_gst = lambda texts: self.torchmoji.encode_texts(texts)\n",
    "        else:\n",
    "            self.compute_gst = None\n",
    "\n",
    "        if not self.sample_inference_speaker_ids:\n",
    "            self.sample_inference_speaker_ids = list(range(self.n_speakers))\n",
    "\n",
    "        # pass\n",
    "\n",
    "    def log_training(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        y_pred,\n",
    "        y,\n",
    "        loss,\n",
    "        mel_loss,\n",
    "        gate_loss,\n",
    "        mel_loss_batch,\n",
    "        gate_loss_batch,\n",
    "        grad_norm,\n",
    "        step_duration_seconds,\n",
    "    ):\n",
    "        self.log(\"Loss/train\", self.global_step, scalar=loss)\n",
    "        self.log(\"MelLoss/train\", self.global_step, scalar=mel_loss)\n",
    "        self.log(\"GateLoss/train\", self.global_step, scalar=gate_loss)\n",
    "        self.log(\"GradNorm\", self.global_step, scalar=grad_norm.item())\n",
    "        self.log(\"LearningRate\", self.global_step, scalar=self.learning_rate)\n",
    "        self.log(\n",
    "            \"StepDurationSeconds\", self.global_step, scalar=step_duration_seconds,\n",
    "        )\n",
    "\n",
    "        batch_levels = X[5]\n",
    "        batch_levels_unique = torch.unique(batch_levels)\n",
    "        for l in batch_levels_unique:\n",
    "            mlb = mel_loss_batch[torch.where(batch_levels == l)[0]].mean()\n",
    "            self.log(\n",
    "                f\"MelLoss/train/speaker{l.item()}\", self.global_step, scalar=mlb,\n",
    "            )\n",
    "            glb = gate_loss_batch[torch.where(batch_levels == l)[0]].mean()\n",
    "            self.log(\n",
    "                f\"GateLoss/train/speaker{l.item()}\", self.global_step, scalar=glb,\n",
    "            )\n",
    "            self.log(\n",
    "                f\"Loss/train/speaker{l.item()}\", self.global_step, scalar=mlb + glb,\n",
    "            )\n",
    "\n",
    "        if self.global_step % self.steps_per_sample == 0:\n",
    "            _, mel_out_postnet, gate_outputs, alignments, *_ = y_pred\n",
    "            mel_target, gate_target = y\n",
    "            alignment_metrics = get_alignment_metrics(alignments)\n",
    "            alignment_diagonalness = alignment_metrics[\"diagonalness\"]\n",
    "            alignment_max = alignment_metrics[\"max\"]\n",
    "            sample_idx = randint(0, mel_out_postnet.size(0) - 1)\n",
    "            audio = self.sample(mel=mel_out_postnet[sample_idx])\n",
    "            self.log(\n",
    "                \"AlignmentDiagonalness/train\",\n",
    "                self.global_step,\n",
    "                scalar=alignment_diagonalness,\n",
    "            )\n",
    "            self.log(\"AlignmentMax/train\", self.global_step, scalar=alignment_max)\n",
    "            self.log(\"AudioSample/train\", self.global_step, audio=audio)\n",
    "            self.log(\n",
    "                \"MelPredicted/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_spectrogram(mel_out_postnet[sample_idx].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "            self.log(\n",
    "                \"MelTarget/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_spectrogram(mel_target[sample_idx].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "            self.log(\n",
    "                \"Gate/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_gate_outputs(\n",
    "                        gate_targets=gate_target[sample_idx].data.cpu(),\n",
    "                        gate_outputs=gate_outputs[sample_idx].data.cpu(),\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            input_length = X[1][sample_idx].item()\n",
    "            output_length = X[4][sample_idx].item()\n",
    "            self.log(\n",
    "                \"Attention/train\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_attention(\n",
    "                        alignments[sample_idx].data.cpu().transpose(0, 1),\n",
    "                        encoder_length=input_length,\n",
    "                        decoder_length=output_length,\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            for speaker_id in self.sample_inference_speaker_ids:\n",
    "                if self.distributed_run:\n",
    "                    self.sample_inference(\n",
    "                        model.module, self.sample_inference_text, speaker_id,\n",
    "                    )\n",
    "                else:\n",
    "                    self.sample_inference(\n",
    "                        model, self.sample_inference_text, speaker_id,\n",
    "                    )\n",
    "\n",
    "    def sample_inference(self, model, transcription=None, speaker_id=None):\n",
    "        if self.rank is not None and self.rank != 0:\n",
    "            return\n",
    "        # Generate an audio sample\n",
    "        with torch.no_grad():\n",
    "            if transcription is None:\n",
    "                transcription = random_utterance()\n",
    "\n",
    "            if self.compute_gst:\n",
    "                gst_embedding = self.compute_gst([transcription])\n",
    "                gst_embedding = torch.FloatTensor(gst_embedding)\n",
    "            else:\n",
    "                gst_embedding = None\n",
    "\n",
    "            utterance = torch.LongTensor(\n",
    "                text_to_sequence(\n",
    "                    transcription,\n",
    "                    self.text_cleaners,\n",
    "                    p_arpabet=self.p_arpabet,\n",
    "                    symbol_set=self.symbol_set,\n",
    "                )\n",
    "            )[None]\n",
    "\n",
    "            input_lengths = torch.LongTensor([utterance.shape[1]])\n",
    "            speaker_id_tensor = torch.LongTensor([speaker_id])\n",
    "\n",
    "            if self.cudnn_enabled and torch.cuda.is_available():\n",
    "                utterance = utterance.cuda()\n",
    "                input_lengths = input_lengths.cuda()\n",
    "                gst_embedding = (\n",
    "                    gst_embedding.cuda() if gst_embedding is not None else None\n",
    "                )\n",
    "                speaker_id_tensor = speaker_id_tensor.cuda()\n",
    "\n",
    "            input_ = [\n",
    "                utterance,\n",
    "                input_lengths,\n",
    "                speaker_id_tensor,\n",
    "                gst_embedding,\n",
    "            ]\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            _, mel, gate, attn, lengths = model.inference(input_)\n",
    "\n",
    "            model.train()\n",
    "            try:\n",
    "                audio = self.sample(mel[0])\n",
    "                self.log(f\"SampleInference/{speaker_id}\", self.global_step, audio=audio)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception raised while doing sample inference: {e}\")\n",
    "                print(\"Mel shape: \", mel[0].shape)\n",
    "            self.log(\n",
    "                f\"Attention/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_attention(attn[0].data.cpu().transpose(0, 1))\n",
    "                ),\n",
    "            )\n",
    "            self.log(\n",
    "                f\"MelPredicted/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(plot_spectrogram(mel[0].data.cpu())),\n",
    "            )\n",
    "            self.log(\n",
    "                f\"Gate/{speaker_id}/sample_inference\",\n",
    "                self.global_step,\n",
    "                image=save_figure_to_numpy(\n",
    "                    plot_gate_outputs(gate_outputs=gate[0].data.cpu())\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def log_validation(\n",
    "        self,\n",
    "        X,\n",
    "        y_pred,\n",
    "        y,\n",
    "        mean_loss,\n",
    "        mean_mel_loss,\n",
    "        mean_gate_loss,\n",
    "        mel_loss_val,\n",
    "        gate_loss_val,\n",
    "        speakers_val,\n",
    "    ):\n",
    "        self.log(\"Loss/val\", self.global_step, scalar=mean_loss)\n",
    "        self.log(\"MelLoss/val\", self.global_step, scalar=mean_mel_loss)\n",
    "        self.log(\"GateLoss/val\", self.global_step, scalar=mean_gate_loss)\n",
    "\n",
    "        val_levels = speakers_val\n",
    "        val_levels_unique = torch.unique(val_levels)\n",
    "        for l in val_levels_unique:\n",
    "            mlv = mel_loss_val[torch.where(val_levels == l)[0]].mean()\n",
    "            self.log(\n",
    "                f\"MelLoss/val/speaker{l.item()}\", self.global_step, scalar=mlv,\n",
    "            )\n",
    "            glv = gate_loss_val[torch.where(val_levels == l)[0]].mean()\n",
    "            self.log(\n",
    "                f\"GateLoss/val/speaker{l.item()}\", self.global_step, scalar=glv,\n",
    "            )\n",
    "            self.log(\n",
    "                f\"Loss/val/speaker{l.item()}\", self.global_step, scalar=mlv + glv,\n",
    "            )\n",
    "        # Generate the sample from a random item from the last y_pred batch.\n",
    "        mel_target, gate_target = y\n",
    "        _, mel_out_postnet, gate_outputs, alignments, *_ = y_pred\n",
    "        alignment_metrics = get_alignment_metrics(alignments)\n",
    "        alignment_diagonalness = alignment_metrics[\"diagonalness\"]\n",
    "        alignment_max = alignment_metrics[\"max\"]\n",
    "\n",
    "        sample_idx = randint(0, mel_out_postnet.size(0) - 1)\n",
    "        audio = self.sample(mel=mel_out_postnet[sample_idx])\n",
    "\n",
    "        self.log(\n",
    "            \"AlignmentDiagonalness/val\", self.global_step, scalar=alignment_diagonalness\n",
    "        )\n",
    "        self.log(\"AlignmentMax/val\", self.global_step, scalar=alignment_max)\n",
    "        self.log(\"AudioSample/val\", self.global_step, audio=audio)\n",
    "        self.log(\n",
    "            \"MelPredicted/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_spectrogram(mel_out_postnet[sample_idx].data.cpu())\n",
    "            ),\n",
    "        )\n",
    "        self.log(\n",
    "            \"MelTarget/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_spectrogram(mel_target[sample_idx].data.cpu())\n",
    "            ),\n",
    "        )\n",
    "        self.log(\n",
    "            \"Gate/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_gate_outputs(\n",
    "                    gate_targets=gate_target[sample_idx].data.cpu(),\n",
    "                    gate_outputs=gate_outputs[sample_idx].data.cpu(),\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        input_length = X[1][sample_idx].item()\n",
    "        output_length = X[4][sample_idx].item()\n",
    "        self.log(\n",
    "            \"Attention/val\",\n",
    "            self.global_step,\n",
    "            image=save_figure_to_numpy(\n",
    "                plot_attention(\n",
    "                    alignments[sample_idx].data.cpu().transpose(0, 1),\n",
    "                    encoder_length=input_length,\n",
    "                    decoder_length=output_length,\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def initialize_loader(self, include_f0: bool = False, n_frames_per_step: int = 1):\n",
    "        train_set = TextMelDataset(\n",
    "            **self.training_dataset_args,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.batch_size,\n",
    "        )\n",
    "        val_set = TextMelDataset(\n",
    "            **self.val_dataset_args,\n",
    "            debug=self.debug,\n",
    "            debug_dataset_size=self.batch_size,\n",
    "        )\n",
    "        collate_fn = TextMelCollate(\n",
    "            n_frames_per_step=n_frames_per_step, include_f0=include_f0\n",
    "        )\n",
    "        sampler = None\n",
    "        if self.distributed_run:\n",
    "            self.init_distributed()\n",
    "            sampler = DistributedSampler(train_set, rank=self.rank)\n",
    "        train_loader = DataLoader(\n",
    "            train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=(sampler is None),\n",
    "            sampler=sampler,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "        return train_set, val_set, train_loader, sampler, collate_fn\n",
    "\n",
    "    def train(self):\n",
    "        train_start_time = time.perf_counter()\n",
    "        print(\"start train\", train_start_time)\n",
    "        train_set, val_set, train_loader, sampler, collate_fn = self.initialize_loader()\n",
    "        criterion = Tacotron2Loss(\n",
    "            pos_weight=self.pos_weight\n",
    "        )  # keep higher than 5 to make clips not stretch on\n",
    "\n",
    "        model = Tacotron2(self.hparams)\n",
    "        if self.device == \"cuda\" and self.cudnn_enabled:\n",
    "            model = model.cuda()\n",
    "        if self.distributed_run:\n",
    "            model = DDP(model, device_ids=[self.rank])\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay,\n",
    "        )\n",
    "        start_epoch = 0\n",
    "\n",
    "        if self.warm_start_name:\n",
    "            if self.distributed_run:\n",
    "                module, optimizer, start_epoch = self.warm_start(\n",
    "                    model.module, optimizer\n",
    "                )\n",
    "                model.module = module\n",
    "            else:\n",
    "                model, optimizer, start_epoch = self.warm_start(model, optimizer)\n",
    "\n",
    "        if self.fp16_run:\n",
    "            scaler = GradScaler()\n",
    "\n",
    "        start_time, previous_start_time = time.perf_counter(), time.perf_counter()\n",
    "        for epoch in range(start_epoch, self.epochs):\n",
    "            #             train_loader, sampler, collate_fn = self.adjust_frames_per_step(\n",
    "            #                 model, train_loader, sampler, collate_fn\n",
    "            #             )\n",
    "            if self.distributed_run:\n",
    "                sampler.set_epoch(epoch)\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                previous_start_time = start_time\n",
    "                start_time = time.perf_counter()\n",
    "                self.global_step += 1\n",
    "                model.zero_grad()\n",
    "                if self.distributed_run:\n",
    "                    X, y = model.module.parse_batch(batch)\n",
    "                else:\n",
    "                    X, y = model.parse_batch(batch)\n",
    "                if self.fp16_run:\n",
    "                    with autocast():\n",
    "                        y_pred = model(X)\n",
    "\n",
    "                        (\n",
    "                            mel_loss,\n",
    "                            gate_loss,\n",
    "                            mel_loss_batch,\n",
    "                            gate_loss_batch,\n",
    "                        ) = criterion(y_pred, y)\n",
    "                        loss = mel_loss + gate_loss\n",
    "                        loss_batch = mel_loss_batch + gate_loss_batch\n",
    "                else:\n",
    "                    y_pred = model(X)\n",
    "                    mel_loss, gate_loss, mel_loss_batch, gate_loss_batch = criterion(\n",
    "                        y_pred, y\n",
    "                    )\n",
    "                    loss = mel_loss + gate_loss\n",
    "                    loss_batch = mel_loss_batch + gate_loss_batch\n",
    "\n",
    "                if self.distributed_run:\n",
    "                    reduced_mel_loss = reduce_tensor(mel_loss, self.world_size).item()\n",
    "                    reduced_gate_loss = reduce_tensor(gate_loss, self.world_size).item()\n",
    "                    reduced_gate_loss_batch = reduce_tensor(\n",
    "                        gate_loss_batch, self.world_size\n",
    "                    )\n",
    "                    reduced_mel_loss_batch = reduce_tensor(\n",
    "                        mel_loss_batch, self.world_size\n",
    "                    )\n",
    "                else:\n",
    "                    reduced_mel_loss = mel_loss.item()\n",
    "                    reduced_gate_loss = gate_loss.item()\n",
    "                    reduced_gate_loss_batch = gate_loss_batch.detach()\n",
    "                    reduced_mel_loss_batch = mel_loss_batch.detach()\n",
    "\n",
    "                reduced_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                if self.fp16_run:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                        model.parameters(), self.grad_clip_thresh\n",
    "                    )\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "                        model.parameters(), self.grad_clip_thresh\n",
    "                    )\n",
    "                    optimizer.step()\n",
    "                step_duration_seconds = time.perf_counter() - start_time\n",
    "                log_start = time.time()\n",
    "                self.log_training(\n",
    "                    model,\n",
    "                    X,\n",
    "                    y_pred,\n",
    "                    y,\n",
    "                    reduced_loss,\n",
    "                    reduced_mel_loss,\n",
    "                    reduced_gate_loss,\n",
    "                    reduced_mel_loss_batch,\n",
    "                    reduced_gate_loss_batch,\n",
    "                    grad_norm,\n",
    "                    step_duration_seconds,\n",
    "                )\n",
    "                log_stop = time.time()\n",
    "                log_str = f\"epoch: {epoch}/{self.epochs} | batch: {batch_idx}/{len(train_loader)} | loss: {reduced_mel_loss:.2f} | mel: {reduced_loss:.2f} | gate: {reduced_gate_loss:.3f} | t: {start_time - previous_start_time:.2f}s | w: {(time.perf_counter() - train_start_time)/(60*60):.2f}h\"\n",
    "                if self.distributed_run:\n",
    "                    log_str += f\" | rank: {self.rank}\"\n",
    "                print(log_str)\n",
    "            if epoch % self.epochs_per_checkpoint == 0:\n",
    "                self.save_checkpoint(\n",
    "                    f\"{self.checkpoint_name}_{epoch}\",\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    iteration=epoch,\n",
    "                    learning_rate=self.learning_rate,\n",
    "                    global_step=self.global_step,\n",
    "                )\n",
    "\n",
    "            # There's no need to validate in debug mode since we're not really training.\n",
    "            if self.debug:\n",
    "                continue\n",
    "            self.validate(\n",
    "                model=model,\n",
    "                val_set=val_set,\n",
    "                collate_fn=collate_fn,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "    def validate(self, **kwargs):\n",
    "        val_start_time = time.perf_counter()\n",
    "\n",
    "        model = kwargs[\"model\"]\n",
    "        val_set = kwargs[\"val_set\"]\n",
    "        collate_fn = kwargs[\"collate_fn\"]\n",
    "        criterion = kwargs[\"criterion\"]\n",
    "        sampler = DistributedSampler(val_set) if self.distributed_run else None\n",
    "        (\n",
    "            total_loss,\n",
    "            total_mel_loss,\n",
    "            total_gate_loss,\n",
    "            total_mel_loss_val,\n",
    "            total_gate_loss_val,\n",
    "        ) = (0, 0, 0, 0, 0)\n",
    "        total_steps = 0\n",
    "        model.eval()\n",
    "        speakers_val = []\n",
    "        total_mel_loss_val = []\n",
    "        total_gate_loss_val = []\n",
    "        with torch.no_grad():\n",
    "            val_loader = DataLoader(\n",
    "                val_set,\n",
    "                sampler=sampler,\n",
    "                shuffle=False,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=collate_fn,\n",
    "            )\n",
    "            for batch in val_loader:\n",
    "                total_steps += 1\n",
    "                if self.distributed_run:\n",
    "                    X, y = model.module.parse_batch(batch)\n",
    "                    speakers_val.append(X[5])\n",
    "                else:\n",
    "                    X, y = model.parse_batch(batch)\n",
    "                    speakers_val.append(X[5])\n",
    "                y_pred = model(X)\n",
    "                mel_loss, gate_loss, mel_loss_batch, gate_loss_batch = criterion(\n",
    "                    y_pred, y\n",
    "                )\n",
    "                if self.distributed_run:\n",
    "                    reduced_mel_loss = reduce_tensor(mel_loss, self.world_size).item()\n",
    "                    reduced_gate_loss = reduce_tensor(gate_loss, self.world_size).item()\n",
    "                    reduced_val_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                    reduced_gate_loss_val = reduce_tensor(\n",
    "                        gate_loss_batch, self.world_size\n",
    "                    )\n",
    "                    reduced_mel_loss_val = reduce_tensor(\n",
    "                        mel_loss_batch, self.world_size\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    reduced_mel_loss = mel_loss.item()\n",
    "                    reduced_gate_loss = gate_loss.item()\n",
    "                    reduced_mel_loss_val = mel_loss_batch.detach()\n",
    "                    reduced_gate_loss_val = gate_loss_batch.detach()\n",
    "\n",
    "                total_mel_loss_val.append(reduced_mel_loss_val)\n",
    "                total_gate_loss_val.append(reduced_gate_loss_val)\n",
    "                reduced_val_loss = reduced_mel_loss + reduced_gate_loss\n",
    "                total_mel_loss += reduced_mel_loss\n",
    "                total_gate_loss += reduced_gate_loss\n",
    "                total_loss += reduced_val_loss\n",
    "\n",
    "            mean_mel_loss = total_mel_loss / total_steps\n",
    "            mean_gate_loss = total_gate_loss / total_steps\n",
    "            mean_loss = total_loss / total_steps\n",
    "            total_mel_loss_val = torch.hstack(total_mel_loss_val)\n",
    "            total_gate_loss_val = torch.hstack(total_gate_loss_val)\n",
    "            speakers_val = torch.hstack(speakers_val)\n",
    "            self.log_validation(\n",
    "                X,\n",
    "                y_pred,\n",
    "                y,\n",
    "                mean_loss,\n",
    "                mean_mel_loss,\n",
    "                mean_gate_loss,\n",
    "                total_mel_loss_val,\n",
    "                total_gate_loss_val,\n",
    "                speakers_val,\n",
    "            )\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        val_log_str = f\"Validation loss: {mean_loss:.2f} | mel: {mel_loss:.2f} | gate: {mean_gate_loss:.3f} | t: {time.perf_counter() - val_start_time:.2f}s\"\n",
    "        print(val_log_str)\n",
    "\n",
    "    @property\n",
    "    def val_dataset_args(self):\n",
    "\n",
    "        args = dict(**self.training_dataset_args)\n",
    "        args[\"audiopaths_and_text\"] = self.val_audiopaths_and_text\n",
    "        return args\n",
    "\n",
    "    @property\n",
    "    def training_dataset_args(self):\n",
    "        return {\n",
    "            \"audiopaths_and_text\": self.training_audiopaths_and_text,\n",
    "            \"text_cleaners\": self.text_cleaners,\n",
    "            \"p_arpabet\": self.p_arpabet,\n",
    "            \"n_mel_channels\": self.n_mel_channels,\n",
    "            \"sampling_rate\": self.sampling_rate,\n",
    "            \"mel_fmin\": self.mel_fmin,\n",
    "            \"mel_fmax\": self.mel_fmax,\n",
    "            \"filter_length\": self.filter_length,\n",
    "            \"hop_length\": self.hop_length,\n",
    "            \"win_length\": self.win_length,\n",
    "            \"symbol_set\": self.symbol_set,\n",
    "            \"max_wav_value\": self.max_wav_value,\n",
    "            \"pos_weight\": self.pos_weight,\n",
    "            \"compute_gst\": self.compute_gst,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3479476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.trainer.base import DEFAULTS as TRAINER_DEFAULTS\n",
    "from uberduck_ml_dev.models.tacotron2 import DEFAULTS as TACOTRON2_DEFAULTS\n",
    "\n",
    "config = TRAINER_DEFAULTS.values()\n",
    "config.update(TACOTRON2_DEFAULTS.values())\n",
    "DEFAULTS = HParams(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
