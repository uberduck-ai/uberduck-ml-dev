{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340afe16",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TextMelDataset\" data-toc-modified-id=\"TextMelDataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>TextMelDataset</a></span></li><li><span><a href=\"#TextMelCollate\" data-toc-modified-id=\"TextMelCollate-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TextMelCollate</a></span></li><li><span><a href=\"#TextAudioLoader\" data-toc-modified-id=\"TextAudioLoader-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TextAudioLoader</a></span></li><li><span><a href=\"#TextAudioCollate\" data-toc-modified-id=\"TextAudioCollate-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TextAudioCollate</a></span></li><li><span><a href=\"#DistributedBucketSampler\" data-toc-modified-id=\"DistributedBucketSampler-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DistributedBucketSampler</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#TextMelBatch-GradTTS\" data-toc-modified-id=\"TextMelBatch-GradTTS-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>TextMelBatch GradTTS</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5161412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db625c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "import nemo\n",
    "import pdb\n",
    "\n",
    "from uberduck_ml_dev.data.batch import Batch\n",
    "from uberduck_ml_dev.models.common import STFT, MelSTFT\n",
    "from uberduck_ml_dev.utils.duration import (\n",
    "    preprocess_tokens,\n",
    "    forward_extractor,\n",
    "    backward_extractor,\n",
    ")\n",
    "from uberduck_ml_dev.text.symbols import (\n",
    "    DEFAULT_SYMBOLS,\n",
    "    IPA_SYMBOLS,\n",
    "    NVIDIA_TACO2_SYMBOLS,\n",
    "    GRAD_TTS_SYMBOLS,\n",
    ")\n",
    "from uberduck_ml_dev.text.util import cleaned_text_to_sequence, text_to_sequence\n",
    "from uberduck_ml_dev.utils.audio import compute_yin, load_wav_to_torch\n",
    "from uberduck_ml_dev.utils.utils import (\n",
    "    load_filepaths_and_text,\n",
    "    intersperse,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pad_sequences(batch):\n",
    "    input_lengths = torch.LongTensor([len(x) for x in batch])\n",
    "    max_input_len = input_lengths.max()\n",
    "\n",
    "    text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "    text_padded.zero_()\n",
    "    for i in range(len(batch)):\n",
    "        text = batch[i]\n",
    "        text_padded[i, : text.size(0)] = text\n",
    "\n",
    "    return text_padded, input_lengths\n",
    "\n",
    "\n",
    "def prepare_input_sequence(\n",
    "    texts, cpu_run=False, arpabet=False, symbol_set=NVIDIA_TACO2_SYMBOLS\n",
    "):\n",
    "    p_arpabet = float(arpabet)\n",
    "    seqs = []\n",
    "    for text in texts:\n",
    "        seqs.append(\n",
    "            torch.IntTensor(\n",
    "                text_to_sequence(\n",
    "                    text,\n",
    "                    [\"english_cleaners\"],\n",
    "                    p_arpabet=p_arpabet,\n",
    "                    symbol_set=symbol_set,\n",
    "                )[:]\n",
    "            )\n",
    "        )\n",
    "    text_padded, input_lengths = pad_sequences(seqs)\n",
    "    if not cpu_run:\n",
    "        text_padded = text_padded.cuda().long()\n",
    "        input_lengths = input_lengths.cuda().long()\n",
    "    else:\n",
    "        text_padded = text_padded.long()\n",
    "        input_lengths = input_lengths.long()\n",
    "\n",
    "    return text_padded, input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def oversample(filepaths_text_sid, sid_to_weight):\n",
    "    assert all([isinstance(sid, str) for sid in sid_to_weight.keys()])\n",
    "    output = []\n",
    "    for fts in filepaths_text_sid:\n",
    "        sid = fts[2]\n",
    "        for _ in range(sid_to_weight.get(sid, 1)):\n",
    "            output.append(fts)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_fts = [\n",
    "    (\"speaker0/1.wav\", \"Test one two\", \"0\"),\n",
    "    (\"speaker0/2.wav\", \"Test one two\", \"0\"),\n",
    "    (\"speaker1/1.wav\", \"Test one two\", \"1\"),\n",
    "]\n",
    "assert oversample(mock_fts, {\"1\": 3}) == [\n",
    "    (\"speaker0/1.wav\", \"Test one two\", \"0\"),\n",
    "    (\"speaker0/2.wav\", \"Test one two\", \"0\"),\n",
    "    (\"speaker1/1.wav\", \"Test one two\", \"1\"),\n",
    "    (\"speaker1/1.wav\", \"Test one two\", \"1\"),\n",
    "    (\"speaker1/1.wav\", \"Test one two\", \"1\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9d9e8",
   "metadata": {},
   "source": [
    "# TextMelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bcafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def _orig_to_dense_speaker_id(speaker_ids):\n",
    "    speaker_ids = sorted(list(set(speaker_ids)))\n",
    "    return {orig: idx for orig, idx in zip(speaker_ids, range(len(speaker_ids)))}\n",
    "\n",
    "\n",
    "class TextMelDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audiopaths_and_text: str,\n",
    "        text_cleaners: List[str],\n",
    "        p_arpabet: float,\n",
    "        n_mel_channels: int,\n",
    "        sampling_rate: int,\n",
    "        mel_fmin: float,\n",
    "        mel_fmax: float,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        symbol_set: str,\n",
    "        padding: int = None,\n",
    "        max_wav_value: float = 32768.0,\n",
    "        include_f0: bool = False,\n",
    "        pos_weight: float = 10,\n",
    "        f0_min: int = 80,\n",
    "        f0_max: int = 880,\n",
    "        harmonic_thresh=0.25,\n",
    "        debug: bool = False,\n",
    "        debug_dataset_size: int = None,\n",
    "        oversample_weights=None,\n",
    "        intersperse_text: bool = False,\n",
    "        intersperse_token: int = 0,\n",
    "        compute_gst=None,\n",
    "        compute_durations: bool = False,\n",
    "        include_durations: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        path = audiopaths_and_text\n",
    "        oversample_weights = oversample_weights or {}\n",
    "        self.audiopaths_and_text = oversample(\n",
    "            load_filepaths_and_text(path), oversample_weights\n",
    "        )\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.p_arpabet = p_arpabet\n",
    "        self.include_durations = include_durations\n",
    "        self.compute_durations = compute_durations\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=filter_length,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            sampling_rate=sampling_rate,\n",
    "            mel_fmin=mel_fmin,\n",
    "            mel_fmax=mel_fmax,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.include_f0 = include_f0\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.harmonic_threshold = harmonic_thresh\n",
    "        # speaker id lookup table\n",
    "        speaker_ids = [i[2] for i in self.audiopaths_and_text]\n",
    "        self._speaker_id_map = _orig_to_dense_speaker_id(speaker_ids)\n",
    "        self.debug = debug\n",
    "        self.debug_dataset_size = debug_dataset_size\n",
    "        self.symbol_set = symbol_set\n",
    "        self.intersperse_text = intersperse_text\n",
    "        self.intersperse_token = intersperse_token\n",
    "        self.compute_gst = compute_gst\n",
    "        self.asr_model = (\n",
    "            EncDecCTCModel.from_pretrained(model_name=\"asr_talknet_aligner\")\n",
    "            .cpu()\n",
    "            .eval()\n",
    "        )\n",
    "        self.nemo_parser = nemo.collections.asr.data.audio_to_text.AudioToCharWithDursF0Dataset.make_vocab(\n",
    "            notation=\"phonemes\",\n",
    "            punct=True,\n",
    "            spaces=False,\n",
    "            stresses=False,\n",
    "            add_blank_at=\"last\",\n",
    "        )\n",
    "        self.blank_id_nemo = self.asr_model.decoder.num_classes_with_blank - 1\n",
    "\n",
    "    def _get_f0(self, audio):\n",
    "        f0, harmonic_rates, argmins, times = compute_yin(\n",
    "            audio,\n",
    "            self.sampling_rate,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.f0_min,\n",
    "            self.f0_max,\n",
    "            self.harmonic_threshold,\n",
    "        )\n",
    "        pad = int((self.filter_length / self.hop_length) / 2)\n",
    "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
    "        f0 = np.array(f0, dtype=np.float32)\n",
    "        return f0\n",
    "\n",
    "    def _get_gst(self, transcription):\n",
    "        return self.compute_gst(transcription)\n",
    "\n",
    "    def _get_durations(self, transcription, audio_norm):\n",
    "        log_probs, _, greedy_predictions = self.asr_model(\n",
    "            input_signal=audio_norm,\n",
    "            input_signal_length=torch.Tensor([audio_norm.shape[1]]).long(),\n",
    "        )\n",
    "        log_probs = log_probs[0].cpu().detach().numpy()\n",
    "        seq_ids = self.nemo_parser(\n",
    "            transcription\n",
    "        )  # test_sample[2][0].cpu().detach().numpy()\n",
    "        target_tokens = preprocess_tokens(seq_ids, self.blank_id_nemo)\n",
    "        f, p = forward_extractor(target_tokens, log_probs, self.blank_id_nemo)\n",
    "        durations = backward_extractor(f, p)\n",
    "        return durations\n",
    "\n",
    "    def _get_data(self, audiopath_and_text):\n",
    "        path, transcription, speaker_id = audiopath_and_text\n",
    "        speaker_id = self._speaker_id_map[speaker_id]\n",
    "        sampling_rate, wav_data = read(path)\n",
    "        text_sequence = torch.LongTensor(\n",
    "            text_to_sequence(\n",
    "                transcription,\n",
    "                self.text_cleaners,\n",
    "                p_arpabet=self.p_arpabet,\n",
    "                symbol_set=self.symbol_set,\n",
    "            )\n",
    "        )\n",
    "        if self.intersperse_text:\n",
    "            text_sequence = torch.LongTensor(\n",
    "                intersperse(text_sequence.numpy(), self.intersperse_token)\n",
    "            )  # add a blank token, whose id number is len(symbols)\n",
    "\n",
    "        audio = torch.FloatTensor(wav_data)\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
    "        melspec = torch.squeeze(melspec, 0)\n",
    "        data = {\n",
    "            \"text_sequence\": text_sequence,\n",
    "            \"mel\": melspec,\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"embedded_gst\": None,\n",
    "            \"f0\": None,\n",
    "            \"durations\": None,\n",
    "        }\n",
    "\n",
    "        if self.compute_durations:\n",
    "            durations = self._get_durations(transcription, audio_norm)\n",
    "            data[\"durations\"] = durations\n",
    "\n",
    "        if self.compute_gst:\n",
    "            embedded_gst = self._get_gst([transcription])\n",
    "            data[\"embedded_gst\"] = embedded_gst\n",
    "\n",
    "        if self.include_f0:\n",
    "            f0 = self._get_f0(audio.data.cpu().numpy())\n",
    "            f0 = torch.from_numpy(f0)[None]\n",
    "            f0 = f0[:, : melspec.size(1)]\n",
    "            data[\"f0\"] = f0\n",
    "\n",
    "        return data  # (text_sequence, melspec, speaker_id, f0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return data for a single audio file + transcription.\"\"\"\n",
    "        try:\n",
    "            data = self._get_data(self.audiopaths_and_text[idx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error while getting data: {self.audiopaths_and_text[idx]}\")\n",
    "            print(e)\n",
    "            raise\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug and self.debug_dataset_size:\n",
    "            return min(self.debug_dataset_size, len(self.audiopaths_and_text))\n",
    "        return len(self.audiopaths_and_text)\n",
    "\n",
    "    def sample_test_batch(self, size):\n",
    "        idx = np.random.choice(range(len(self)), size=size, replace=False)\n",
    "        test_batch = []\n",
    "        for index in idx:\n",
    "            test_batch.append(self.__getitem__(index))\n",
    "        return test_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cde58",
   "metadata": {},
   "source": [
    "# TextMelCollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextMelCollate:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_frames_per_step: int = 1,\n",
    "        include_f0: bool = False,\n",
    "        include_durations: bool = False,\n",
    "    ):\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "        self.include_f0 = include_f0\n",
    "        self.include_durations = include_durations\n",
    "\n",
    "    def set_frames_per_step(self, n_frames_per_step):\n",
    "        \"\"\"Set n_frames_step.\n",
    "\n",
    "        This is used to train with gradual training, where we start with a large\n",
    "        n_frames_per_step in order to learn attention quickly and decrease it\n",
    "        over the course of training in order to increase accuracy. Gradual training\n",
    "        reference:\n",
    "        https://erogol.com/gradual-training-with-tacotron-for-faster-convergence/\n",
    "        \"\"\"\n",
    "        self.n_frames_per_step = n_frames_per_step\n",
    "\n",
    "    def __call__(self, datapoints):\n",
    "        \"\"\"Collate's training batch from normalized text and mel-spectrogram\n",
    "        PARAMS\n",
    "        ------\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        batch_size = len(datapoints)\n",
    "        input_lengths = torch.LongTensor([len(x[\"text_sequence\"]) for x in datapoints])\n",
    "        max_input_len = max(input_lengths)\n",
    "\n",
    "        text_padded = torch.LongTensor(batch_size, max_input_len)\n",
    "        text_padded.zero_()\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        for i in range(batch_size):\n",
    "            print(i)\n",
    "            text = datapoints[i][\"text_sequence\"]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "\n",
    "        # Right zero-pad mel-spec\n",
    "        num_mels = datapoints[0][\"mel\"].size(0)\n",
    "        max_target_len = max([x[\"mel\"].size(1) for x in datapoints])\n",
    "        if max_target_len % self.n_frames_per_step != 0:\n",
    "            max_target_len += (\n",
    "                self.n_frames_per_step - max_target_len % self.n_frames_per_step\n",
    "            )\n",
    "            assert max_target_len % self.n_frames_per_step == 0\n",
    "\n",
    "        # include mel padded, gate padded and speaker ids\n",
    "        mel_padded = torch.FloatTensor(batch_size, num_mels, max_target_len)\n",
    "        mel_padded.zero_()\n",
    "        gate_padded = torch.FloatTensor(batch_size, max_target_len)\n",
    "        gate_padded.zero_()\n",
    "        output_lengths = torch.LongTensor(batch_size)\n",
    "        speaker_ids = torch.LongTensor(batch_size)\n",
    "        if self.include_f0:\n",
    "            f0_padded = torch.FloatTensor(batch_size, 1, max_target_len)\n",
    "            f0_padded.zero_()\n",
    "        else:\n",
    "            f0_padded = None\n",
    "        # pdb.set_trace()\n",
    "        for i in range(batch_size):\n",
    "            mel = datapoints[i][\"mel\"]\n",
    "            mel_padded[i, :, : mel.size(1)] = mel\n",
    "            gate_padded[i, mel.size(1) - 1 :] = 1\n",
    "            output_lengths[i] = mel.size(1)\n",
    "            speaker_ids[i] = datapoints[i][\"speaker_id\"]\n",
    "\n",
    "        if self.include_durations:\n",
    "            durations_padded = torch.FloatTensor(batch_size, max_input_len)\n",
    "            durations_padded.zero_()\n",
    "            for i in range(batch_size):\n",
    "                durations = datapoints[i][\"durations\"]\n",
    "                durations_padded[i, : durations_padded.size(1)]\n",
    "        else:\n",
    "            durations_padded = None\n",
    "\n",
    "        if datapoints[0][\"embedded_gst\"] is None:\n",
    "            embedded_gsts = None\n",
    "        else:\n",
    "            embedded_gsts = torch.FloatTensor(\n",
    "                np.array([sample[\"embedded_gst\"] for sample in datapoints])\n",
    "            )\n",
    "\n",
    "        model_inputs = Batch(\n",
    "            text_int_padded=text_padded,\n",
    "            input_lengths=input_lengths,\n",
    "            mel_padded=mel_padded,\n",
    "            gate_padded=gate_padded,\n",
    "            output_lengths=output_lengths,\n",
    "            speaker_ids=speaker_ids,\n",
    "            gst=embedded_gsts,\n",
    "            durations_padded=durations_padded,\n",
    "            f0_padded=f0_padded,\n",
    "        )\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed89f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 0, 3: 1, 4: 2, 9: 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_orig_to_dense_speaker_id([4, 2, 9, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-14 02:47:38 cloud:56] Found existing object /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n",
      "[NeMo I 2022-01-14 02:47:38 cloud:62] Re-using file from: /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo\n",
      "[NeMo I 2022-01-14 02:47:38 common:728] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-01-14 02:47:39 features:243] Using torch_stft is deprecated and will be removed in 1.1.0. Please set stft_conv and stft_exact_pad to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-01-14 02:47:39 features:265] PADDING: 1\n",
      "[NeMo I 2022-01-14 02:47:39 features:275] STFT using conv\n",
      "[NeMo I 2022-01-14 02:47:44 save_restore_connector:149] Model EncDecCTCModel was successfully restored from /home/s_uberduck_ai/.cache/torch/NeMo/NeMo_1.5.1/qn5x5_libri_tts_phonemes/656c7439dd3a0d614978529371be498b/qn5x5_libri_tts_phonemes.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2022-01-14 02:47:44 vocabs:324] Torch distributed needs to be initialized before you initialized <nemo.collections.asr.data.vocabs.Phonemes object at 0x7f2e9e78af50>. This class is prone to data access race conditions. Now downloading corpora from global rank 0. If other ranks pass this before rank 0, errors might result.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    padding=None,\n",
    "    win_length=1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    symbol_set=\"default\",\n",
    ")\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16563fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while getting data: ['test/fixtures/wavs/stevejobs-1.wav', '{ W EH1 L } { Y UW1 } { N OW1 } , { AE1 Z } { Y UW1 } { N OW1 } , { DH AH0 } { W EH1 B Z } { AH0 } { P R IH1 T IY0 } { M ER0 AE1 K Y AH0 L AH0 S } { TH IH1 NG } . { AH0 N D } { IH1 T } { W AA1 Z } { AH0 } { V EH1 R IY0 } { S IH1 M P AH0 L } { P EH1 R AH0 D AY2 M } { DH AE1 T } { W AA1 Z } { IH0 N V EH1 N T AH0 D } { W IH1 CH } { W AA1 Z } .', '0']\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23524/3857158776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextMelCollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23524/692662340.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;34m\"\"\"Return data for a single audio file + transcription.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudiopaths_and_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while getting data: {self.audiopaths_and_text[idx]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23524/692662340.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self, audiopath_and_text)\u001b[0m\n\u001b[1;32m    152\u001b[0m         }\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_durations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mdurations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_durations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"durations\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdurations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collate_fn = TextMelCollate()\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    padding=None,\n",
    "    win_length=1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    "    symbol_set=\"default\",\n",
    ")\n",
    "assert len(ds) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = TextMelCollate(include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "for i, batch in enumerate(dl):\n",
    "    # text_padded,\n",
    "    # input_lengths,\n",
    "    # mel_padded,\n",
    "    # gate_padded,\n",
    "    # output_lengths,\n",
    "    # speaker_ids,\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, print(\"output lengths: \", output_lengths)\n",
    "    assert gate_padded.size(1) == 566\n",
    "    assert mel_padded.size(2) == 566\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae303fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing n_frames_per_step > 1\n",
    "ds = TextMelDataset(\n",
    "    \"test/fixtures/val.txt\",\n",
    "    [\"english_cleaners\"],\n",
    "    0.0,\n",
    "    80,\n",
    "    22050,\n",
    "    0,\n",
    "    8000,\n",
    "    1024,\n",
    "    256,\n",
    "    padding=None,\n",
    "    win_length=1024,\n",
    "    debug=True,\n",
    "    debug_dataset_size=12,\n",
    "    include_f0=True,\n",
    "    symbol_set=\"default\",\n",
    ")\n",
    "assert len(ds) == 1\n",
    "collate_fn = TextMelCollate(n_frames_per_step=5, include_f0=True)\n",
    "dl = DataLoader(ds, 12, collate_fn=collate_fn)\n",
    "# text_padded,\n",
    "# input_lengths,\n",
    "# mel_padded,\n",
    "# gate_padded,\n",
    "# output_lengths,\n",
    "# speaker_ids,\n",
    "for i, batch in enumerate(dl):\n",
    "    (\n",
    "        text_padded,\n",
    "        input_lengths,\n",
    "        mel_padded,\n",
    "        gate_padded,\n",
    "        output_lengths,\n",
    "        speaker_ids,\n",
    "        *_,\n",
    "    ) = batch\n",
    "    assert output_lengths.item() == 566, output_lengths.item()\n",
    "    assert mel_padded.size(2) == 570, print(\"actual shape: \", mel_padded.shape)\n",
    "    assert gate_padded.size(1) == 570, print(\"actual shape: \", gate_padded.shape)\n",
    "    assert len(batch) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfd167",
   "metadata": {},
   "source": [
    "# TextAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextAudioSpeakerLoader(Dataset):\n",
    "    \"\"\"\n",
    "    1) loads audio, speaker_id, text pairs\n",
    "    2) normalizes text and converts them to sequences of integers\n",
    "    3) computes spectrograms from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, audiopaths_sid_text, hparams, debug=False, debug_dataset_size=None\n",
    "    ):\n",
    "        oversample_weights = hparams.oversample_weights or {}\n",
    "        self.audiopaths_sid_text = oversample(\n",
    "            load_filepaths_and_text(audiopaths_sid_text), oversample_weights\n",
    "        )\n",
    "        self.text_cleaners = hparams.text_cleaners\n",
    "        self.max_wav_value = hparams.max_wav_value\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.filter_length = hparams.filter_length\n",
    "        self.hop_length = hparams.hop_length\n",
    "        self.win_length = hparams.win_length\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        self.compute_durations = hparams.sampling_rate\n",
    "        self.debug = debug\n",
    "        self.debug_dataset_size = debug_dataset_size\n",
    "\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            n_mel_channels=hparams.n_mel_channels,\n",
    "            sampling_rate=hparams.sampling_rate,\n",
    "            mel_fmin=hparams.mel_fmin,\n",
    "            mel_fmax=hparams.mel_fmax,\n",
    "            padding=(self.filter_length - self.hop_length) // 2,\n",
    "        )\n",
    "\n",
    "        self.cleaned_text = getattr(hparams, \"cleaned_text\", False)\n",
    "        # NOTE(zach): Parametrize this later if desired.\n",
    "        self.symbol_set = IPA_SYMBOLS\n",
    "\n",
    "        self.add_blank = hparams.add_blank\n",
    "        self.min_text_len = getattr(hparams, \"min_text_len\", 1)\n",
    "        self.max_text_len = getattr(hparams, \"max_text_len\", 190)\n",
    "\n",
    "        random.seed(1234)\n",
    "        random.shuffle(self.audiopaths_sid_text)\n",
    "        self._filter()\n",
    "\n",
    "    def _filter(self):\n",
    "        \"\"\"\n",
    "        Filter text & store spec lengths\n",
    "        \"\"\"\n",
    "        # Store spectrogram lengths for Bucketing\n",
    "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n",
    "        # spec_length = wav_length // hop_length\n",
    "\n",
    "        audiopaths_sid_text_new = []\n",
    "        lengths = []\n",
    "        for audiopath, sid, text in self.audiopaths_sid_text:\n",
    "            if self.min_text_len <= len(text) and len(text) <= self.max_text_len:\n",
    "                audiopaths_sid_text_new.append([audiopath, sid, text])\n",
    "                lengths.append(os.path.getsize(audiopath) // (2 * self.hop_length))\n",
    "        self.audiopaths_sid_text = audiopaths_sid_text_new\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def get_audio_text_speaker_pair(self, audiopath_sid_text):\n",
    "        # separate filename, speaker_id and text\n",
    "        audiopath, text, sid = (\n",
    "            audiopath_sid_text[0],\n",
    "            audiopath_sid_text[1],\n",
    "            audiopath_sid_text[2],\n",
    "        )\n",
    "        text = self.get_text(text)\n",
    "        spec, wav = self.get_audio(audiopath)\n",
    "        sid = self.get_sid(sid)\n",
    "        return (text, spec, wav, sid)\n",
    "\n",
    "    def get_audio(self, filename):\n",
    "        audio, sampling_rate = load_wav_to_torch(filename)\n",
    "        if sampling_rate != self.sampling_rate:\n",
    "            raise ValueError(\n",
    "                \"{} {} SR doesn't match target {} SR\".format(\n",
    "                    sampling_rate, self.sampling_rate\n",
    "                )\n",
    "            )\n",
    "\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        audio_norm = audio_norm.unsqueeze(0)\n",
    "        spec_filename = filename.replace(\".wav\", \".uberduck.spec.pt\")\n",
    "        if os.path.exists(spec_filename):\n",
    "            spec = torch.load(spec_filename)\n",
    "        else:\n",
    "            spec = self.stft.spectrogram(audio_norm)\n",
    "            spec = torch.squeeze(spec, 0)\n",
    "            torch.save(spec, spec_filename)\n",
    "        return spec, audio_norm\n",
    "\n",
    "    def get_text(self, text):\n",
    "        if self.cleaned_text:\n",
    "            text_norm = cleaned_text_to_sequence(text, symbol_set=self.symbol_set)\n",
    "        else:\n",
    "            text_norm = text_to_sequence(\n",
    "                text, self.text_cleaners, symbol_set=self.symbol_set\n",
    "            )\n",
    "        if self.add_blank:\n",
    "            text_norm = intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def get_sid(self, sid):\n",
    "        sid = torch.LongTensor([int(sid)])\n",
    "        return sid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.get_audio_text_speaker_pair(self.audiopaths_sid_text[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.debug and self.debug_dataset_size:\n",
    "            return min(self.debug_dataset_size, len(self.audiopaths_sid_text))\n",
    "        else:\n",
    "            return len(self.audiopaths_sid_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ac590",
   "metadata": {},
   "source": [
    "# TextAudioCollate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class TextAudioSpeakerCollate:\n",
    "    \"\"\"Zero-pads model inputs and targets\"\"\"\n",
    "\n",
    "    def __init__(self, return_ids=False):\n",
    "        self.return_ids = return_ids\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Collate's training batch from normalized text, audio and speaker identities\n",
    "        PARAMS\n",
    "        ------\n",
    "        batch: [text_normalized, spec_normalized, wav_normalized, sid]\n",
    "        \"\"\"\n",
    "        # Right zero-pad all one-hot text sequences to max input length\n",
    "        _, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([x[1].size(1) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "\n",
    "        max_text_len = max([len(x[0]) for x in batch])\n",
    "        max_spec_len = max([x[1].size(1) for x in batch])\n",
    "        max_wav_len = max([x[2].size(1) for x in batch])\n",
    "\n",
    "        text_lengths = torch.LongTensor(len(batch))\n",
    "        spec_lengths = torch.LongTensor(len(batch))\n",
    "        wav_lengths = torch.LongTensor(len(batch))\n",
    "        sid = torch.LongTensor(len(batch))\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_text_len)\n",
    "        spec_padded = torch.FloatTensor(len(batch), batch[0][1].size(0), max_spec_len)\n",
    "        wav_padded = torch.FloatTensor(len(batch), 1, max_wav_len)\n",
    "        text_padded.zero_()\n",
    "        spec_padded.zero_()\n",
    "        wav_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            row = batch[ids_sorted_decreasing[i]]\n",
    "\n",
    "            text = row[0]\n",
    "            text_padded[i, : text.size(0)] = text\n",
    "            text_lengths[i] = text.size(0)\n",
    "\n",
    "            spec = row[1]\n",
    "            spec_padded[i, :, : spec.size(1)] = spec\n",
    "            spec_lengths[i] = spec.size(1)\n",
    "\n",
    "            wav = row[2]\n",
    "            wav_padded[i, :, : wav.size(1)] = wav\n",
    "            wav_lengths[i] = wav.size(1)\n",
    "\n",
    "            sid[i] = row[3]\n",
    "\n",
    "        if self.return_ids:\n",
    "            return (\n",
    "                text_padded,\n",
    "                text_lengths,\n",
    "                spec_padded,\n",
    "                spec_lengths,\n",
    "                wav_padded,\n",
    "                wav_lengths,\n",
    "                sid,\n",
    "                ids_sorted_decreasing,\n",
    "            )\n",
    "        return (\n",
    "            text_padded,\n",
    "            text_lengths,\n",
    "            spec_padded,\n",
    "            spec_lengths,\n",
    "            wav_padded,\n",
    "            wav_lengths,\n",
    "            sid,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad232f",
   "metadata": {},
   "source": [
    "# DistributedBucketSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class DistributedBucketSampler(DistributedSampler):\n",
    "    \"\"\"\n",
    "    Maintain similar input lengths in a batch.\n",
    "    Length groups are specified by boundaries.\n",
    "    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n",
    "\n",
    "    It removes samples which are not included in the boundaries.\n",
    "    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        boundaries,\n",
    "        num_replicas=None,\n",
    "        rank=None,\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n",
    "        self.lengths = dataset.lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.boundaries = boundaries\n",
    "\n",
    "        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n",
    "        self.total_size = sum(self.num_samples_per_bucket)\n",
    "        self.num_samples = self.total_size // self.num_replicas\n",
    "\n",
    "    def _create_buckets(self):\n",
    "        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n",
    "        for i in range(len(self.lengths)):\n",
    "            length = self.lengths[i]\n",
    "            idx_bucket = self._bisect(length)\n",
    "            if idx_bucket != -1:\n",
    "                buckets[idx_bucket].append(i)\n",
    "\n",
    "        for i in range(len(buckets) - 1, 0, -1):\n",
    "            if len(buckets[i]) == 0:\n",
    "                buckets.pop(i)\n",
    "                self.boundaries.pop(i + 1)\n",
    "\n",
    "        num_samples_per_bucket = []\n",
    "        for i in range(len(buckets)):\n",
    "            len_bucket = len(buckets[i])\n",
    "            total_batch_size = self.num_replicas * self.batch_size\n",
    "            rem = (\n",
    "                total_batch_size - (len_bucket % total_batch_size)\n",
    "            ) % total_batch_size\n",
    "            num_samples_per_bucket.append(len_bucket + rem)\n",
    "        return buckets, num_samples_per_bucket\n",
    "\n",
    "    def __iter__(self):\n",
    "        # deterministically shuffle based on epoch\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.epoch)\n",
    "\n",
    "        indices = []\n",
    "        if self.shuffle:\n",
    "            for bucket in self.buckets:\n",
    "                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n",
    "        else:\n",
    "            for bucket in self.buckets:\n",
    "                indices.append(list(range(len(bucket))))\n",
    "\n",
    "        batches = []\n",
    "        for i in range(len(self.buckets)):\n",
    "            bucket = self.buckets[i]\n",
    "            len_bucket = len(bucket)\n",
    "            ids_bucket = indices[i]\n",
    "            num_samples_bucket = self.num_samples_per_bucket[i]\n",
    "\n",
    "            # add extra samples to make it evenly divisible\n",
    "            rem = num_samples_bucket - len_bucket\n",
    "            ids_bucket = (\n",
    "                ids_bucket\n",
    "                + ids_bucket * (rem // len_bucket)\n",
    "                + ids_bucket[: (rem % len_bucket)]\n",
    "            )\n",
    "\n",
    "            # subsample\n",
    "            ids_bucket = ids_bucket[self.rank :: self.num_replicas]\n",
    "\n",
    "            # batching\n",
    "            for j in range(len(ids_bucket) // self.batch_size):\n",
    "                batch = [\n",
    "                    bucket[idx]\n",
    "                    for idx in ids_bucket[\n",
    "                        j * self.batch_size : (j + 1) * self.batch_size\n",
    "                    ]\n",
    "                ]\n",
    "                batches.append(batch)\n",
    "\n",
    "        if self.shuffle:\n",
    "            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n",
    "            batches = [batches[i] for i in batch_ids]\n",
    "        self.batches = batches\n",
    "\n",
    "        assert len(self.batches) * self.batch_size == self.num_samples\n",
    "        return iter(self.batches)\n",
    "\n",
    "    def _bisect(self, x, lo=0, hi=None):\n",
    "        if hi is None:\n",
    "            hi = len(self.boundaries) - 1\n",
    "\n",
    "        if hi > lo:\n",
    "            mid = (hi + lo) // 2\n",
    "            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\n",
    "                return mid\n",
    "            elif x <= self.boundaries[mid]:\n",
    "                return self._bisect(x, lo, mid)\n",
    "            else:\n",
    "                return self._bisect(x, mid + 1, hi)\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec654200",
   "metadata": {},
   "source": [
    "### TextMelBatch GradTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "# import torchaudio as ta\n",
    "\n",
    "\n",
    "# class TextMelDatasetGradTTS(torch.utils.data.Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         filelist_path,\n",
    "#         intersperse_text=True,\n",
    "#         n_fft=1024,\n",
    "#         n_mels=80,\n",
    "#         sample_rate=22050,\n",
    "#         hop_length=256,\n",
    "#         win_length=1024,\n",
    "#         f_min=0.0,\n",
    "#         f_max=8000,\n",
    "#         intersperse_token=0,\n",
    "#         symbol_set=\"grad_tts\",\n",
    "#         text_cleaners=[\"english\"],\n",
    "#     ):\n",
    "#         self.filepaths_and_text = load_filepaths_and_text(filelist_path)\n",
    "#         self.intersperse_text = intersperse_text\n",
    "#         self.intersperse_token = intersperse_token\n",
    "#         self.n_fft = n_fft\n",
    "#         self.n_mels = n_mels\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.hop_length = hop_length\n",
    "#         self.win_length = win_length\n",
    "#         self.f_min = f_min\n",
    "#         self.f_max = f_max\n",
    "#         self.symbol_set = symbol_set\n",
    "#         self.text_cleaners = text_cleaners\n",
    "#         self.p_arpabet = 1.0\n",
    "#         random.seed(1234)\n",
    "#         random.shuffle(self.filepaths_and_text)\n",
    "\n",
    "#     def get_pair(self, filepath_and_text):\n",
    "#         filepath, text = filepath_and_text[0], filepath_and_text[1]\n",
    "#         text = self.get_text(text, intersperse_text=self.intersperse_text)\n",
    "#         mel = self.get_mel(filepath)\n",
    "#         return (text, mel)\n",
    "\n",
    "#     def get_mel(self, filepath):\n",
    "#         audio, sr = ta.load(filepath)\n",
    "#         assert sr == self.sample_rate\n",
    "#         mel = mel_spectrogram(\n",
    "#             audio,\n",
    "#             self.n_fft,\n",
    "#             self.n_mels,\n",
    "#             self.sample_rate,\n",
    "#             self.hop_length,\n",
    "#             self.win_length,\n",
    "#             self.f_min,\n",
    "#             self.f_max,\n",
    "#             center=False,\n",
    "#         ).squeeze()\n",
    "#         return mel\n",
    "\n",
    "#     def get_text(self, text, intersperse_text=True):\n",
    "#         text_sequence = text_to_sequence(\n",
    "#             text,\n",
    "#             self.text_cleaners,\n",
    "#             p_arpabet=self.p_arpabet,\n",
    "#             symbol_set=self.symbol_set,\n",
    "#         )\n",
    "\n",
    "#         if self.intersperse_text:\n",
    "#             text_sequence = intersperse(\n",
    "#                 text_sequence, self.intersperse_token\n",
    "#             )  # add a blank token, whose id number is len(symbols)\n",
    "\n",
    "#         text_sequence = torch.IntTensor(text_sequence)\n",
    "#         return text_sequence\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         text, mel = self.get_pair(self.filepaths_and_text[index])\n",
    "#         item = {\"y\": mel, \"x\": text}\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.filepaths_and_text)\n",
    "\n",
    "#     def sample_test_batch(self, size):\n",
    "#         idx = np.random.choice(range(len(self)), size=size, replace=False)\n",
    "#         test_batch = []\n",
    "#         for index in idx:\n",
    "#             test_batch.append(self.__getitem__(index))\n",
    "#         return test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066940a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export\n",
    "\n",
    "# from uberduck_ml_dev.models.grad_tts import fix_len_compatibility\n",
    "\n",
    "\n",
    "# class TextMelBatchCollateGradTTS(object):\n",
    "#     def __call__(self, batch):\n",
    "#         B = len(batch)\n",
    "#         y_max_length = max([item[\"y\"].shape[-1] for item in batch])\n",
    "#         y_max_length = fix_len_compatibility(y_max_length)\n",
    "#         x_max_length = max([item[\"x\"].shape[-1] for item in batch])\n",
    "#         n_feats = batch[0][\"y\"].shape[-2]\n",
    "\n",
    "#         y = torch.zeros((B, n_feats, y_max_length), dtype=torch.float32)\n",
    "#         x = torch.zeros((B, x_max_length), dtype=torch.long)\n",
    "#         y_lengths, x_lengths = [], []\n",
    "\n",
    "#         for i, item in enumerate(batch):\n",
    "#             y_, x_ = item[\"y\"], item[\"x\"]\n",
    "#             y_lengths.append(y_.shape[-1])\n",
    "#             x_lengths.append(x_.shape[-1])\n",
    "#             y[i, :, : y_.shape[-1]] = y_\n",
    "#             x[i, : x_.shape[-1]] = x_\n",
    "\n",
    "#         y_lengths = torch.LongTensor(y_lengths)\n",
    "#         x_lengths = torch.LongTensor(x_lengths)\n",
    "#         return {\"x\": x, \"x_lengths\": x_lengths, \"y\": y, \"y_lengths\": y_lengths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "# these classes are under development\n",
    "# make a class containing e.g. texts, sequences, that can be read as a batch in either forward passes and inference\n",
    "# create lists for particular off-the-shelf models?\n",
    "\n",
    "\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audiopaths_and_text: str,\n",
    "        text_cleaners: List[str],\n",
    "        p_arpabet: float,\n",
    "        n_mel_channels: int,\n",
    "        sampling_rate: int,\n",
    "        mel_fmin: float,\n",
    "        mel_fmax: float,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        padding: int,\n",
    "        win_length: int,\n",
    "        symbol_set: str,\n",
    "        max_wav_value: float = 32768.0,\n",
    "        include_f0: bool = False,\n",
    "        pos_weight: float = 10,\n",
    "        f0_min: int = 80,\n",
    "        f0_max: int = 880,\n",
    "        harmonic_thresh=0.25,\n",
    "        debug: bool = False,\n",
    "        debug_dataset_size: int = None,\n",
    "        oversample_weights=None,\n",
    "        intersperse_text=False,\n",
    "        intersperse_token=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # oversample\n",
    "        path = audiopaths_and_text\n",
    "        oversample_weights = oversample_weights or {}\n",
    "        self.audiopaths_and_text = oversample(\n",
    "            load_filepaths_and_text(path), oversample_weights\n",
    "        )\n",
    "\n",
    "        # text to seq parameters\n",
    "        self.text_cleaners = text_cleaners\n",
    "        self.p_arpabet = p_arpabet\n",
    "        self.intersperse_text = intersperse_text\n",
    "        self.intersperse_token = intersperse_token\n",
    "\n",
    "        self.stft = MelSTFT(\n",
    "            filter_length=filter_length,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            sampling_rate=sampling_rate,\n",
    "            mel_fmin=mel_fmin,\n",
    "            mel_fmax=mel_fmax,\n",
    "            padding=padding,\n",
    "        )\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.include_f0 = include_f0\n",
    "        self.f0_min = f0_min\n",
    "        self.f0_max = f0_max\n",
    "        self.harmonic_threshold = harmonic_thresh\n",
    "        # speaker id lookup table\n",
    "        speaker_ids = [i[2] for i in self.audiopaths_and_text]\n",
    "        self.symbol_set = symbol_set\n",
    "\n",
    "    def _get_f0(self, audio):\n",
    "        f0, harmonic_rates, argmins, times = compute_yin(\n",
    "            audio,\n",
    "            self.sampling_rate,\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.f0_min,\n",
    "            self.f0_max,\n",
    "            self.harmonic_threshold,\n",
    "        )\n",
    "        pad = int((self.filter_length / self.hop_length) / 2)\n",
    "        f0 = [0.0] * pad + f0 + [0.0] * pad\n",
    "        f0 = np.array(f0, dtype=np.float32)\n",
    "        return f0\n",
    "\n",
    "    def _text_to_seq(self, audiopath_and_text):\n",
    "        path, transcription, speaker_id = audiopath_and_text\n",
    "        text_sequence = torch.LongTensor(\n",
    "            text_to_sequence(\n",
    "                transcription,\n",
    "                self.text_cleaners,\n",
    "                p_arpabet=self.p_arpabet,\n",
    "                symbol_set=self.symbol_set,\n",
    "            )\n",
    "        )\n",
    "        if self.intersperse_text:\n",
    "            text_sequence = torch.LongTensor(\n",
    "                intersperse(text_sequence.numpy(), self.intersperse_token)\n",
    "            )  # add a blank token, whose id number is len(symbols)\n",
    "        return text_to_sequence\n",
    "\n",
    "    #     def _get_f0(self,audio):\n",
    "\n",
    "    #         if self.include_f0:\n",
    "    #         else:\n",
    "    #             return None\n",
    "\n",
    "    #    def _get_mel(self,):\n",
    "\n",
    "    def _get_data(self, audiopath_and_text):\n",
    "\n",
    "        sequence = self._text_to_seq(audiopath_and_text)\n",
    "        audio = self._get_audio(audiopath_and_text)\n",
    "        melspec = self._get_mel(audio)\n",
    "        f0 = self._get_f0(audio)\n",
    "        speaker_id = self._get_sid(audiopath_and_text)\n",
    "\n",
    "        return (text_sequence, melspec, speaker_id, f0)\n",
    "\n",
    "\n",
    "class Collate:\n",
    "    \"\"\"\n",
    "    Collate assembles batches from list indexed by sample id\n",
    "    text, spectragram, etc\"\"\"\n",
    "\n",
    "    def __init__(**args):\n",
    "        pass\n",
    "\n",
    "    #         n_frames_per_step: int = 1,\n",
    "    #         include_f0: bool = False,\n",
    "    #         include_sid: bool = False,\n",
    "    #         batch_format: str\n",
    "\n",
    "    def _pad_sequence(self, batch):\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        input_lengths = [len(x[0].shape[1]) for x in batch]\n",
    "        max_input_len = input_lengths.max()\n",
    "        text_padded = torch.LongTensor(batch_size, max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(batch_size):\n",
    "            text_padded[i, : batch[0][i].shape[1]] = batch[0][i]\n",
    "\n",
    "        return text_padded\n",
    "\n",
    "    def _pad_mel(self, batch):\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        target_lengths = [len(x[0].shape[1]) for x in batch]\n",
    "        max_target_len = max(target_len)\n",
    "        textint_padded = torch.LongTensor(batch_size, max_input_len)\n",
    "        textint_padded.zero_()\n",
    "        for i in range(batch_size):\n",
    "            textint_padded[i, : batch[0][i].shape[1]] = batch[0][i]\n",
    "\n",
    "        # assert len(f0) = len(mel)\n",
    "        return text_padded\n",
    "\n",
    "    def _pad_f0(self, batch):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        text_padded, input_lengths = _pad_sequence(batch)  # idx\n",
    "        mel_padded, gate_padded, output_lengths = _pad_mel(batch)\n",
    "        f0 = None\n",
    "        batch = Batch(\n",
    "            text=text_padded,\n",
    "            input_lengths=input_lengths,\n",
    "            mel_padded=mel_padded,\n",
    "            gate_padded=gate_padded,\n",
    "            output_lengths=output_lengths,\n",
    "            f0=f0,\n",
    "            speaker_ids=speaker_ids,\n",
    "        )\n",
    "\n",
    "        if batch_format == \"taco2ss\":\n",
    "            return (text_padded, mel_padded, mel_padded, output_lengths, input_lengths)\n",
    "        # if batch_format == 'taco2ms':\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def inference(self, batch):\n",
    "\n",
    "        if batch_format == \"taco2ss\":\n",
    "            return (self.text_padded, self.input_lengths)\n",
    "        if batch_format == \"taco2ms\":\n",
    "            return (self.text_padded, self.input_lengths)\n",
    "\n",
    "    # need to have pad_sequences equivalent\n",
    "    def _to_tacotron2_singlespeaker_inference(self, batch):\n",
    "\n",
    "        text_padded, input_lengths = _pad_sequence(batch_list)  # idx\n",
    "        mel_padded, gate_padded, output_lengths = _pad_mel(batch_list)\n",
    "        return (self.text_padded, self.input_lengths)\n",
    "\n",
    "    # NOTE(zach): would model_inputs be better as a namedtuple or dataclass?\n",
    "    def _to_mellotron_train_f0():\n",
    "\n",
    "        batch = Batch\n",
    "        return (\n",
    "            text_padded,\n",
    "            input_lengths,\n",
    "            mel_padded,\n",
    "            gate_padded,\n",
    "            output_lengths,\n",
    "            speaker_ids,\n",
    "            f0_padded,\n",
    "        )\n",
    "\n",
    "    #     if self.include_f0:\n",
    "    #         model_inputs =\n",
    "    #     else:\n",
    "    #         model_inputs = (\n",
    "    #             text_padded,\n",
    "    #             input_lengths,\n",
    "    #             mel_padded,\n",
    "    #             gate_padded,\n",
    "    #             output_lengths,\n",
    "    #             speaker_ids,\n",
    "    #         )\n",
    "\n",
    "    def _to_tacotron2_multispeaker_inference(batch):\n",
    "\n",
    "        input_lengths, ids_sorted_decreasing = torch.sort(\n",
    "            torch.LongTensor([len(x) for x in batch]), dim=0, descending=True\n",
    "        )\n",
    "        max_input_len = input_lengths[0]\n",
    "\n",
    "        text_padded = torch.LongTensor(len(batch), max_input_len)\n",
    "        text_padded.zero_()\n",
    "        for i in range(len(ids_sorted_decreasing)):\n",
    "            text_padded[i, : text.size(0)] = batch[i]\n",
    "\n",
    "        return (self.text_padded, self.speakers, self.input_lengths, self.sort_indices)\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class Batch:\n",
    "#     textint_padded: torch.Tensor,\n",
    "#     input_lengths: list\n",
    "#     mel_padded: torch.Tensor\n",
    "#     gate_padded:\n",
    "#     output_length: list,\n",
    "#     speaker_ids: list,\n",
    "#     f0_padded: list,\n",
    "\n",
    "# # export\n",
    "\n",
    "# from uberduck_ml_dev.text.symbols import (\n",
    "#     DEFAULT_SYMBOLS,\n",
    "#     IPA_SYMBOLS,\n",
    "#     NVIDIA_TACO2_SYMBOLS,\n",
    "#     GRAD_TTS_SYMBOLS,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
