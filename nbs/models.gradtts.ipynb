{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fc1f99",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb666f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.gradtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import random\n",
    "import math\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from uberduck_ml_dev.models.base import TTSModel\n",
    "from uberduck_ml_dev.vendor.tfcompat.hparam import HParams\n",
    "from uberduck_ml_dev.text.symbols import SYMBOL_SETS\n",
    "from uberduck_ml_dev.text.util import text_to_sequence\n",
    "from uberduck_ml_dev.utils.utils import intersperse, intersperse_emphases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class BaseModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModule, self).__init__()\n",
    "\n",
    "    @property\n",
    "    def nparams(self):\n",
    "        \"\"\"\n",
    "        Returns number of trainable parameters of the module.\n",
    "        \"\"\"\n",
    "        num_params = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                num_params += np.prod(param.detach().cpu().numpy().shape)\n",
    "        return num_params\n",
    "\n",
    "    def relocate_input(self, x: list):\n",
    "        \"\"\"\n",
    "        Relocates provided tensors to the same device set for the module.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(len(x)):\n",
    "            if isinstance(x[i], torch.Tensor) and x[i].device != device:\n",
    "                x[i] = x[i].to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class Mish(BaseModule):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(torch.nn.functional.softplus(x))\n",
    "\n",
    "\n",
    "class Upsample(BaseModule):\n",
    "    def __init__(self, dim):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.conv = torch.nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(BaseModule):\n",
    "    def __init__(self, dim):\n",
    "        super(Downsample, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(dim, dim, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Rezero(BaseModule):\n",
    "    def __init__(self, fn):\n",
    "        super(Rezero, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.g = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) * self.g\n",
    "\n",
    "\n",
    "class Block(BaseModule):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super(Block, self).__init__()\n",
    "        self.block = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(dim, dim_out, 3, padding=1),\n",
    "            torch.nn.GroupNorm(groups, dim_out),\n",
    "            Mish(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        output = self.block(x * mask)\n",
    "        return output * mask\n",
    "\n",
    "\n",
    "class ResnetBlock(BaseModule):\n",
    "    def __init__(self, dim, dim_out, time_emb_dim, groups=8):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.mlp = torch.nn.Sequential(Mish(), torch.nn.Linear(time_emb_dim, dim_out))\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        if dim != dim_out:\n",
    "            self.res_conv = torch.nn.Conv2d(dim, dim_out, 1)\n",
    "        else:\n",
    "            self.res_conv = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask, time_emb):\n",
    "        h = self.block1(x, mask)\n",
    "        h += self.mlp(time_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = self.block2(h, mask)\n",
    "        output = h + self.res_conv(x * mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LinearAttention(BaseModule):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = torch.nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = torch.nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(\n",
    "            qkv, \"b (qkv heads c) h w -> qkv b heads c (h w)\", heads=self.heads, qkv=3\n",
    "        )\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum(\"bhdn,bhen->bhde\", k, v)\n",
    "        out = torch.einsum(\"bhde,bhdn->bhen\", context, q)\n",
    "        out = rearrange(\n",
    "            out, \"b heads c (h w) -> b (heads c) h w\", heads=self.heads, h=h, w=w\n",
    "        )\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Residual(BaseModule):\n",
    "    def __init__(self, fn):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        output = self.fn(x, *args, **kwargs) + x\n",
    "        return output\n",
    "\n",
    "\n",
    "class SinusoidalPosEmb(BaseModule):\n",
    "    def __init__(self, dim):\n",
    "        super(SinusoidalPosEmb, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x, scale=1000):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device).float() * -emb)\n",
    "        emb = scale * x.unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class GradLogPEstimator2d(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_mults=(1, 2, 4),\n",
    "        groups=8,\n",
    "        n_spks=None,\n",
    "        spk_emb_dim=64,\n",
    "        n_feats=80,\n",
    "        pe_scale=1000,\n",
    "    ):\n",
    "        super(GradLogPEstimator2d, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_mults = dim_mults\n",
    "        self.groups = groups\n",
    "        self.n_spks = n_spks if not isinstance(n_spks, type(None)) else 1\n",
    "        self.spk_emb_dim = spk_emb_dim\n",
    "        self.pe_scale = pe_scale\n",
    "\n",
    "        if n_spks > 1:\n",
    "            self.spk_mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(spk_emb_dim, spk_emb_dim * 4),\n",
    "                Mish(),\n",
    "                torch.nn.Linear(spk_emb_dim * 4, n_feats),\n",
    "            )\n",
    "        self.time_pos_emb = SinusoidalPosEmb(dim)\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dim, dim * 4), Mish(), torch.nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "\n",
    "        dims = [2 + (1 if n_spks > 1 else 0), *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        self.downs = torch.nn.ModuleList([])\n",
    "        self.ups = torch.nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "            self.downs.append(\n",
    "                torch.nn.ModuleList(\n",
    "                    [\n",
    "                        ResnetBlock(dim_in, dim_out, time_emb_dim=dim),\n",
    "                        ResnetBlock(dim_out, dim_out, time_emb_dim=dim),\n",
    "                        Residual(Rezero(LinearAttention(dim_out))),\n",
    "                        Downsample(dim_out) if not is_last else torch.nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=dim)\n",
    "        self.mid_attn = Residual(Rezero(LinearAttention(mid_dim)))\n",
    "        self.mid_block2 = ResnetBlock(mid_dim, mid_dim, time_emb_dim=dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            self.ups.append(\n",
    "                torch.nn.ModuleList(\n",
    "                    [\n",
    "                        ResnetBlock(dim_out * 2, dim_in, time_emb_dim=dim),\n",
    "                        ResnetBlock(dim_in, dim_in, time_emb_dim=dim),\n",
    "                        Residual(Rezero(LinearAttention(dim_in))),\n",
    "                        Upsample(dim_in),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        self.final_block = Block(dim, dim)\n",
    "        self.final_conv = torch.nn.Conv2d(dim, 1, 1)\n",
    "\n",
    "    def forward(self, x, mask, mu, t, spk=None):\n",
    "        if not isinstance(spk, type(None)):\n",
    "            s = self.spk_mlp(spk)\n",
    "\n",
    "        t = self.time_pos_emb(t, scale=self.pe_scale)\n",
    "        t = self.mlp(t)\n",
    "\n",
    "        if self.n_spks < 2:\n",
    "            x = torch.stack([mu, x], 1)\n",
    "        else:\n",
    "            s = s.unsqueeze(-1).repeat(1, 1, x.shape[-1])\n",
    "            x = torch.stack([mu, x, s], 1)\n",
    "        mask = mask.unsqueeze(1)\n",
    "\n",
    "        hiddens = []\n",
    "        masks = [mask]\n",
    "        for resnet1, resnet2, attn, downsample in self.downs:\n",
    "            mask_down = masks[-1]\n",
    "            x = resnet1(x, mask_down, t)\n",
    "            x = resnet2(x, mask_down, t)\n",
    "            x = attn(x)\n",
    "            hiddens.append(x)\n",
    "            x = downsample(x * mask_down)\n",
    "            masks.append(mask_down[:, :, :, ::2])\n",
    "\n",
    "        masks = masks[:-1]\n",
    "        mask_mid = masks[-1]\n",
    "        x = self.mid_block1(x, mask_mid, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, mask_mid, t)\n",
    "\n",
    "        for resnet1, resnet2, attn, upsample in self.ups:\n",
    "            mask_up = masks.pop()\n",
    "            x = torch.cat((x, hiddens.pop()), dim=1)\n",
    "            x = resnet1(x, mask_up, t)\n",
    "            x = resnet2(x, mask_up, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x * mask_up)\n",
    "\n",
    "        x = self.final_block(x, mask)\n",
    "        output = self.final_conv(x * mask)\n",
    "\n",
    "        return (output * mask).squeeze(1)\n",
    "\n",
    "\n",
    "def get_noise(t, beta_init, beta_term, cumulative=False):\n",
    "    if cumulative:\n",
    "        noise = beta_init * t + 0.5 * (beta_term - beta_init) * (t**2)\n",
    "    else:\n",
    "        noise = beta_init + (beta_term - beta_init) * t\n",
    "    return noise\n",
    "\n",
    "\n",
    "class Diffusion(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        dim,\n",
    "        n_spks=1,\n",
    "        spk_emb_dim=64,\n",
    "        beta_min=0.05,\n",
    "        beta_max=20,\n",
    "        pe_scale=1000,\n",
    "    ):\n",
    "        super(Diffusion, self).__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.dim = dim\n",
    "        self.n_spks = n_spks\n",
    "        self.spk_emb_dim = spk_emb_dim\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.pe_scale = pe_scale\n",
    "\n",
    "        self.estimator = GradLogPEstimator2d(\n",
    "            dim, n_spks=n_spks, spk_emb_dim=spk_emb_dim, pe_scale=pe_scale\n",
    "        )\n",
    "\n",
    "    def forward_diffusion(self, x0, mask, mu, t):\n",
    "        time = t.unsqueeze(-1).unsqueeze(-1)\n",
    "        cum_noise = get_noise(time, self.beta_min, self.beta_max, cumulative=True)\n",
    "        mean = x0 * torch.exp(-0.5 * cum_noise) + mu * (\n",
    "            1.0 - torch.exp(-0.5 * cum_noise)\n",
    "        )\n",
    "        variance = 1.0 - torch.exp(-cum_noise)\n",
    "        z = torch.randn(x0.shape, dtype=x0.dtype, device=x0.device, requires_grad=False)\n",
    "        xt = mean + z * torch.sqrt(variance)\n",
    "        return xt * mask, z * mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reverse_diffusion(self, z, mask, mu, n_timesteps, stoc=False, spk=None):\n",
    "        h = 1.0 / n_timesteps\n",
    "        xt = z * mask\n",
    "        for i in range(n_timesteps):\n",
    "            t = (1.0 - (i + 0.5) * h) * torch.ones(\n",
    "                z.shape[0], dtype=z.dtype, device=z.device\n",
    "            )\n",
    "            time = t.unsqueeze(-1).unsqueeze(-1)\n",
    "            noise_t = get_noise(time, self.beta_min, self.beta_max, cumulative=False)\n",
    "            if stoc:  # adds stochastic term\n",
    "                dxt_det = 0.5 * (mu - xt) - self.estimator(xt, mask, mu, t, spk)\n",
    "                dxt_det = dxt_det * noise_t * h\n",
    "                dxt_stoc = torch.randn(\n",
    "                    z.shape, dtype=z.dtype, device=z.device, requires_grad=False\n",
    "                )\n",
    "                dxt_stoc = dxt_stoc * torch.sqrt(noise_t * h)\n",
    "                dxt = dxt_det + dxt_stoc\n",
    "            else:\n",
    "                dxt = 0.5 * (mu - xt - self.estimator(xt, mask, mu, t, spk))\n",
    "                dxt = dxt * noise_t * h\n",
    "            xt = (xt - dxt) * mask\n",
    "        return xt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, z, mask, mu, n_timesteps, stoc=False, spk=None):\n",
    "        return self.reverse_diffusion(z, mask, mu, n_timesteps, stoc, spk)\n",
    "\n",
    "    def loss_t(self, x0, mask, mu, t, spk=None):\n",
    "        xt, z = self.forward_diffusion(x0, mask, mu, t)\n",
    "        time = t.unsqueeze(-1).unsqueeze(-1)\n",
    "        cum_noise = get_noise(time, self.beta_min, self.beta_max, cumulative=True)\n",
    "        noise_estimation = self.estimator(xt, mask, mu, t, spk)\n",
    "        noise_estimation *= torch.sqrt(1.0 - torch.exp(-cum_noise))\n",
    "        loss = torch.sum((noise_estimation + z) ** 2) / (torch.sum(mask) * self.n_feats)\n",
    "        return loss, xt\n",
    "\n",
    "    def compute_loss(self, x0, mask, mu, spk=None, offset=1e-5):\n",
    "        t = torch.rand(\n",
    "            x0.shape[0], dtype=x0.dtype, device=x0.device, requires_grad=False\n",
    "        )\n",
    "        t = torch.clamp(t, offset, 1.0 - offset)\n",
    "        return self.loss_t(x0, mask, mu, t, spk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64999a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\"\"\" from https://github.com/jaywalnut310/glow-tts \"\"\"\n",
    "\n",
    "\n",
    "def sequence_mask(length, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    x = torch.arange(int(max_length), dtype=length.dtype, device=length.device)\n",
    "    return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "\n",
    "def fix_len_compatibility(length, num_downsamplings_in_unet=2):\n",
    "    while True:\n",
    "        if length % (2**num_downsamplings_in_unet) == 0:\n",
    "            return length\n",
    "        length += 1\n",
    "\n",
    "\n",
    "def convert_pad_shape(pad_shape):\n",
    "    l = pad_shape[::-1]\n",
    "    pad_shape = [item for sublist in l for item in sublist]\n",
    "    return pad_shape\n",
    "\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "    device = duration.device\n",
    "\n",
    "    b, t_x, t_y = mask.shape\n",
    "    cum_duration = torch.cumsum(duration, 1)\n",
    "    path = torch.zeros(b, t_x, t_y, dtype=mask.dtype).to(device=device)\n",
    "\n",
    "    cum_duration_flat = cum_duration.view(b * t_x)\n",
    "    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n",
    "    path = path.view(b, t_x, t_y)\n",
    "    path = (\n",
    "        path\n",
    "        - torch.nn.functional.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[\n",
    "            :, :-1\n",
    "        ]\n",
    "    )\n",
    "    path = path * mask\n",
    "    return path\n",
    "\n",
    "\n",
    "def duration_loss(logw, logw_, lengths):\n",
    "    loss = torch.sum((logw - logw_) ** 2) / torch.sum(lengths)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630dc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\"\"\" from https://github.com/jaywalnut310/glow-tts \"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class LayerNorm(BaseModule):\n",
    "    def __init__(self, channels, eps=1e-4):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.eps = eps\n",
    "\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(channels))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_dims = len(x.shape)\n",
    "        mean = torch.mean(x, 1, keepdim=True)\n",
    "        variance = torch.mean((x - mean) ** 2, 1, keepdim=True)\n",
    "\n",
    "        x = (x - mean) * torch.rsqrt(variance + self.eps)\n",
    "\n",
    "        shape = [1, -1] + [1] * (n_dims - 2)\n",
    "        x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvReluNorm(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        n_layers,\n",
    "        p_dropout,\n",
    "    ):\n",
    "        super(ConvReluNorm, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.conv_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers = torch.nn.ModuleList()\n",
    "        self.conv_layers.append(\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n",
    "            )\n",
    "        )\n",
    "        self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.relu_drop = torch.nn.Sequential(\n",
    "            torch.nn.ReLU(), torch.nn.Dropout(p_dropout)\n",
    "        )\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                torch.nn.Conv1d(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    kernel_size,\n",
    "                    padding=kernel_size // 2,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers.append(LayerNorm(hidden_channels))\n",
    "        self.proj = torch.nn.Conv1d(hidden_channels, out_channels, 1)\n",
    "        self.proj.weight.data.zero_()\n",
    "        self.proj.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x_org = x\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x * x_mask)\n",
    "            x = self.norm_layers[i](x)\n",
    "            x = self.relu_drop(x)\n",
    "        x = x_org + self.proj(x)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class DurationPredictor(BaseModule):\n",
    "    def __init__(self, in_channels, filter_channels, kernel_size, p_dropout):\n",
    "        super(DurationPredictor, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "        self.conv_1 = torch.nn.Conv1d(\n",
    "            in_channels, filter_channels, kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "        self.norm_1 = LayerNorm(filter_channels)\n",
    "        self.conv_2 = torch.nn.Conv1d(\n",
    "            filter_channels, filter_channels, kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "        self.norm_2 = LayerNorm(filter_channels)\n",
    "        self.proj = torch.nn.Conv1d(filter_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.conv_1(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.norm_1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv_2(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.norm_2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.proj(x * x_mask)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class MultiHeadAttention(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        out_channels,\n",
    "        n_heads,\n",
    "        window_size=None,\n",
    "        heads_share=True,\n",
    "        p_dropout=0.0,\n",
    "        proximal_bias=False,\n",
    "        proximal_init=False,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert channels % n_heads == 0\n",
    "\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.window_size = window_size\n",
    "        self.heads_share = heads_share\n",
    "        self.proximal_bias = proximal_bias\n",
    "        self.p_dropout = p_dropout\n",
    "        self.attn = None\n",
    "\n",
    "        self.k_channels = channels // n_heads\n",
    "        self.conv_q = torch.nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_k = torch.nn.Conv1d(channels, channels, 1)\n",
    "        self.conv_v = torch.nn.Conv1d(channels, channels, 1)\n",
    "        if window_size is not None:\n",
    "            n_heads_rel = 1 if heads_share else n_heads\n",
    "            rel_stddev = self.k_channels**-0.5\n",
    "            self.emb_rel_k = torch.nn.Parameter(\n",
    "                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n",
    "                * rel_stddev\n",
    "            )\n",
    "            self.emb_rel_v = torch.nn.Parameter(\n",
    "                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n",
    "                * rel_stddev\n",
    "            )\n",
    "        self.conv_o = torch.nn.Conv1d(channels, out_channels, 1)\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.conv_q.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_k.weight)\n",
    "        if proximal_init:\n",
    "            self.conv_k.weight.data.copy_(self.conv_q.weight.data)\n",
    "            self.conv_k.bias.data.copy_(self.conv_q.bias.data)\n",
    "        torch.nn.init.xavier_uniform_(self.conv_v.weight)\n",
    "\n",
    "    def forward(self, x, c, attn_mask=None):\n",
    "        q = self.conv_q(x)\n",
    "        k = self.conv_k(c)\n",
    "        v = self.conv_v(c)\n",
    "\n",
    "        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n",
    "\n",
    "        x = self.conv_o(x)\n",
    "        return x\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        b, d, t_s, t_t = (*key.size(), query.size(2))\n",
    "        query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n",
    "        key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n",
    "        value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n",
    "\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.k_channels)\n",
    "        if self.window_size is not None:\n",
    "            assert (\n",
    "                t_s == t_t\n",
    "            ), \"Relative attention is only available for self-attention.\"\n",
    "            key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n",
    "            rel_logits = self._matmul_with_relative_keys(query, key_relative_embeddings)\n",
    "            rel_logits = self._relative_position_to_absolute_position(rel_logits)\n",
    "            scores_local = rel_logits / math.sqrt(self.k_channels)\n",
    "            scores = scores + scores_local\n",
    "        if self.proximal_bias:\n",
    "            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n",
    "            scores = scores + self._attention_bias_proximal(t_s).to(\n",
    "                device=scores.device, dtype=scores.dtype\n",
    "            )\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e4)\n",
    "        p_attn = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        p_attn = self.drop(p_attn)\n",
    "        output = torch.matmul(p_attn, value)\n",
    "        if self.window_size is not None:\n",
    "            relative_weights = self._absolute_position_to_relative_position(p_attn)\n",
    "            value_relative_embeddings = self._get_relative_embeddings(\n",
    "                self.emb_rel_v, t_s\n",
    "            )\n",
    "            output = output + self._matmul_with_relative_values(\n",
    "                relative_weights, value_relative_embeddings\n",
    "            )\n",
    "        output = output.transpose(2, 3).contiguous().view(b, d, t_t)\n",
    "        return output, p_attn\n",
    "\n",
    "    def _matmul_with_relative_values(self, x, y):\n",
    "        ret = torch.matmul(x, y.unsqueeze(0))\n",
    "        return ret\n",
    "\n",
    "    def _matmul_with_relative_keys(self, x, y):\n",
    "        ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n",
    "        return ret\n",
    "\n",
    "    def _get_relative_embeddings(self, relative_embeddings, length):\n",
    "        pad_length = max(length - (self.window_size + 1), 0)\n",
    "        slice_start_position = max((self.window_size + 1) - length, 0)\n",
    "        slice_end_position = slice_start_position + 2 * length - 1\n",
    "        if pad_length > 0:\n",
    "            padded_relative_embeddings = torch.nn.functional.pad(\n",
    "                relative_embeddings,\n",
    "                convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),\n",
    "            )\n",
    "        else:\n",
    "            padded_relative_embeddings = relative_embeddings\n",
    "        used_relative_embeddings = padded_relative_embeddings[\n",
    "            :, slice_start_position:slice_end_position\n",
    "        ]\n",
    "        return used_relative_embeddings\n",
    "\n",
    "    def _relative_position_to_absolute_position(self, x):\n",
    "        batch, heads, length, _ = x.size()\n",
    "        x = torch.nn.functional.pad(\n",
    "            x, convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]])\n",
    "        )\n",
    "        x_flat = x.view([batch, heads, length * 2 * length])\n",
    "        x_flat = torch.nn.functional.pad(\n",
    "            x_flat, convert_pad_shape([[0, 0], [0, 0], [0, length - 1]])\n",
    "        )\n",
    "        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[\n",
    "            :, :, :length, length - 1 :\n",
    "        ]\n",
    "        return x_final\n",
    "\n",
    "    def _absolute_position_to_relative_position(self, x):\n",
    "        batch, heads, length, _ = x.size()\n",
    "        x = torch.nn.functional.pad(\n",
    "            x, convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length - 1]])\n",
    "        )\n",
    "        x_flat = x.view([batch, heads, length**2 + length * (length - 1)])\n",
    "        x_flat = torch.nn.functional.pad(\n",
    "            x_flat, convert_pad_shape([[0, 0], [0, 0], [length, 0]])\n",
    "        )\n",
    "        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n",
    "        return x_final\n",
    "\n",
    "    def _attention_bias_proximal(self, length):\n",
    "        r = torch.arange(length, dtype=torch.float32)\n",
    "        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n",
    "        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n",
    "\n",
    "\n",
    "class FFN(BaseModule):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, filter_channels, kernel_size, p_dropout=0.0\n",
    "    ):\n",
    "        super(FFN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv1d(\n",
    "            in_channels, filter_channels, kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "        self.conv_2 = torch.nn.Conv1d(\n",
    "            filter_channels, out_channels, kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.conv_1(x * x_mask)\n",
    "        x = torch.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.conv_2(x * x_mask)\n",
    "        return x * x_mask\n",
    "\n",
    "\n",
    "class Encoder(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        filter_channels,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size=1,\n",
    "        p_dropout=0.0,\n",
    "        window_size=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.drop = torch.nn.Dropout(p_dropout)\n",
    "        self.attn_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers_1 = torch.nn.ModuleList()\n",
    "        self.ffn_layers = torch.nn.ModuleList()\n",
    "        self.norm_layers_2 = torch.nn.ModuleList()\n",
    "        for _ in range(self.n_layers):\n",
    "            self.attn_layers.append(\n",
    "                MultiHeadAttention(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    n_heads,\n",
    "                    window_size=window_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n",
    "            self.ffn_layers.append(\n",
    "                FFN(\n",
    "                    hidden_channels,\n",
    "                    hidden_channels,\n",
    "                    filter_channels,\n",
    "                    kernel_size,\n",
    "                    p_dropout=p_dropout,\n",
    "                )\n",
    "            )\n",
    "            self.norm_layers_2.append(LayerNorm(hidden_channels))\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n",
    "        for i in range(self.n_layers):\n",
    "            x = x * x_mask\n",
    "            y = self.attn_layers[i](x, x, attn_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_1[i](x + y)\n",
    "            y = self.ffn_layers[i](x, x_mask)\n",
    "            y = self.drop(y)\n",
    "            x = self.norm_layers_2[i](x + y)\n",
    "        x = x * x_mask\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextEncoder(BaseModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        n_feats,\n",
    "        n_channels,\n",
    "        filter_channels,\n",
    "        filter_channels_dp,\n",
    "        n_heads,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        p_dropout,\n",
    "        window_size=None,\n",
    "        spk_emb_dim=64,\n",
    "        n_spks=1,\n",
    "    ):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_feats = n_feats\n",
    "        self.n_channels = n_channels\n",
    "        self.filter_channels = filter_channels\n",
    "        self.filter_channels_dp = filter_channels_dp\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p_dropout = p_dropout\n",
    "        self.window_size = window_size\n",
    "        self.spk_emb_dim = spk_emb_dim\n",
    "        self.n_spks = n_spks\n",
    "\n",
    "        self.emb = torch.nn.Embedding(n_vocab, n_channels)\n",
    "        torch.nn.init.normal_(self.emb.weight, 0.0, n_channels**-0.5)\n",
    "\n",
    "        self.prenet = ConvReluNorm(\n",
    "            n_channels, n_channels, n_channels, kernel_size=5, n_layers=3, p_dropout=0.5\n",
    "        )\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            n_channels + (spk_emb_dim if n_spks > 1 else 0),\n",
    "            filter_channels,\n",
    "            n_heads,\n",
    "            n_layers,\n",
    "            kernel_size,\n",
    "            p_dropout,\n",
    "            window_size=window_size,\n",
    "        )\n",
    "\n",
    "        self.proj_m = torch.nn.Conv1d(\n",
    "            n_channels + (spk_emb_dim if n_spks > 1 else 0), n_feats, 1\n",
    "        )\n",
    "        self.proj_w = DurationPredictor(\n",
    "            n_channels + (spk_emb_dim if n_spks > 1 else 0),\n",
    "            filter_channels_dp,\n",
    "            kernel_size,\n",
    "            p_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lengths, spk=None):\n",
    "        x = self.emb(x) * math.sqrt(self.n_channels)\n",
    "        x = torch.transpose(x, 1, -1)\n",
    "        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)\n",
    "\n",
    "        x = self.prenet(x, x_mask)\n",
    "        if self.n_spks > 1:\n",
    "            x = torch.cat([x, spk.unsqueeze(-1).repeat(1, 1, x.shape[-1])], dim=1)\n",
    "        x = self.encoder(x, x_mask)\n",
    "        mu = self.proj_m(x) * x_mask\n",
    "\n",
    "        x_dp = torch.detach(x)\n",
    "        logw = self.proj_w(x_dp, x_mask)\n",
    "\n",
    "        return mu, logw, x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import monotonic_align\n",
    "\n",
    "\n",
    "class GradTTS(TTSModel):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        self.n_vocab = (\n",
    "            len(SYMBOL_SETS[hparams.symbol_set]) + 1\n",
    "            if hparams.intersperse_text\n",
    "            else len(SYMBOL_SETS[hparams.symbol_set])\n",
    "        )\n",
    "        self.n_spks = hparams.n_spks\n",
    "        self.spk_emb_dim = hparams.spk_emb_dim\n",
    "        self.n_enc_channels = hparams.n_enc_channels\n",
    "        self.filter_channels = hparams.filter_channels\n",
    "        self.filter_channels_dp = hparams.filter_channels_dp\n",
    "        self.n_heads = hparams.n_heads\n",
    "        self.n_enc_layers = hparams.n_enc_layers\n",
    "        self.enc_kernel = hparams.enc_kernel\n",
    "        self.enc_dropout = hparams.enc_dropout\n",
    "        self.window_size = hparams.window_size\n",
    "        self.n_feats = hparams.n_feats\n",
    "        self.dec_dim = hparams.dec_dim\n",
    "        self.beta_min = hparams.beta_min\n",
    "        self.beta_max = hparams.beta_max\n",
    "        self.pe_scale = hparams.pe_scale\n",
    "        self.hop_length = hparams.hop_length\n",
    "        self.sampling_rate = hparams.sampling_rate\n",
    "        # NOTE(zach): Parametrize this later\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if self.n_spks > 1:\n",
    "            self.spk_emb = torch.nn.Embedding(self.n_spks, self.spk_emb_dim)\n",
    "        self.encoder = TextEncoder(\n",
    "            self.n_vocab,\n",
    "            self.n_feats,\n",
    "            self.n_enc_channels,\n",
    "            self.filter_channels,\n",
    "            self.filter_channels_dp,\n",
    "            self.n_heads,\n",
    "            self.n_enc_layers,\n",
    "            self.enc_kernel,\n",
    "            self.enc_dropout,\n",
    "            self.window_size,\n",
    "        )\n",
    "        self.decoder = Diffusion(\n",
    "            self.n_feats,\n",
    "            self.dec_dim,\n",
    "            self.n_spks,\n",
    "            self.spk_emb_dim,\n",
    "            self.beta_min,\n",
    "            self.beta_max,\n",
    "            self.pe_scale,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def nparams(self):\n",
    "        \"\"\"\n",
    "        Returns number of trainable parameters of the module.\n",
    "        \"\"\"\n",
    "        num_params = 0\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                num_params += np.prod(param.detach().cpu().numpy().shape)\n",
    "        return num_params\n",
    "\n",
    "    def relocate_input(self, x: list):\n",
    "        \"\"\"\n",
    "        Relocates provided tensors to the same device set for the module.\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        for i in range(len(x)):\n",
    "            if isinstance(x[i], torch.Tensor) and x[i].device != device:\n",
    "                x[i] = x[i].to(device)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        x_lengths,\n",
    "        n_timesteps,\n",
    "        temperature=1.0,\n",
    "        stoc=False,\n",
    "        spk=None,\n",
    "        length_scale=1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generates mel-spectrogram from text. Returns:\n",
    "            1. encoder outputs\n",
    "            2. decoder outputs\n",
    "            3. generated alignment\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): batch of texts, converted to a tensor with phoneme embedding ids.\n",
    "            x_lengths (torch.Tensor): lengths of texts in batch.\n",
    "            n_timesteps (int): number of steps to use for reverse diffusion in decoder.\n",
    "            temperature (float, optional): controls variance of terminal distribution.\n",
    "            stoc (bool, optional): flag that adds stochastic term to the decoder sampler.\n",
    "                Usually, does not provide synthesis improvements.\n",
    "            length_scale (float, optional): controls speech pace.\n",
    "                Increase value to slow down generated speech and vice versa.\n",
    "        \"\"\"\n",
    "        x, x_lengths = self.relocate_input([x, x_lengths])\n",
    "\n",
    "        if self.n_spks > 1:\n",
    "            # Get speaker embedding\n",
    "            spk = self.spk_emb(spk)\n",
    "\n",
    "        # Get encoder_outputs `mu_x` and log-scaled token durations `logw`\n",
    "        mu_x, logw, x_mask = self.encoder(x, x_lengths, spk)\n",
    "        w = torch.exp(logw) * x_mask\n",
    "        w_ceil = torch.ceil(w) * length_scale\n",
    "        y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()\n",
    "        y_max_length = int(y_lengths.max())\n",
    "        y_max_length_ = fix_len_compatibility(y_max_length)\n",
    "\n",
    "        # Using obtained durations `w` construct alignment map `attn`\n",
    "        y_mask = sequence_mask(y_lengths, y_max_length_).unsqueeze(1).to(x_mask.dtype)\n",
    "        attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)\n",
    "        attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "        # Align encoded text and get mu_y\n",
    "        mu_y = torch.matmul(attn.squeeze(1).transpose(1, 2), mu_x.transpose(1, 2))\n",
    "        mu_y = mu_y.transpose(1, 2)\n",
    "        encoder_outputs = mu_y[:, :, :y_max_length]\n",
    "\n",
    "        # Sample latent representation from terminal distribution N(mu_y, I)\n",
    "        z = mu_y + torch.randn_like(mu_y, device=mu_y.device) / temperature\n",
    "        # Generate sample by performing reverse dynamics\n",
    "        decoder_outputs = self.decoder(z, y_mask, mu_y, n_timesteps, stoc, spk)\n",
    "        decoder_outputs = decoder_outputs[:, :, :y_max_length]\n",
    "\n",
    "        return encoder_outputs, decoder_outputs, attn[:, :, :y_max_length]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(\n",
    "        self,\n",
    "        text,\n",
    "        n_timesteps,\n",
    "        cleaner_names=[\"english_cleaners\"],\n",
    "        temperature=1.0,\n",
    "        stoc=False,\n",
    "        spk=None,\n",
    "        length_scale=1.0,\n",
    "        intersperse_text=True,\n",
    "        intersperse_token=148,\n",
    "    ):\n",
    "        seq = text_to_sequence(\n",
    "            text, cleaner_names=cleaner_names, p_arpabet=1.0, symbol_set=\"gradtts\"\n",
    "        )\n",
    "        if intersperse_text:\n",
    "            x = torch.LongTensor(intersperse(seq, intersperse_token))[None]\n",
    "        else:\n",
    "            x = torch.LongTensor(seq)[None]\n",
    "\n",
    "        x_lengths = torch.LongTensor([x.shape[-1]])\n",
    "        if self.device == \"cuda\":\n",
    "            x = x.cuda()\n",
    "            x_lengths = x_lengths.cuda()\n",
    "\n",
    "        y_enc, y_dec, attn = self.forward(\n",
    "            x,\n",
    "            x_lengths,\n",
    "            n_timesteps=n_timesteps,\n",
    "            temperature=temperature,\n",
    "            stoc=stoc,\n",
    "            spk=spk,\n",
    "            length_scale=length_scale,\n",
    "        )\n",
    "        return y_enc, y_dec, attn\n",
    "\n",
    "    def compute_loss(self, x, x_lengths, y, y_lengths, spk=None, out_size=None):\n",
    "        \"\"\"\n",
    "        Computes 3 losses:\n",
    "            1. duration loss: loss between predicted token durations and those extracted by Monotinic Alignment Search (MAS).\n",
    "            2. prior loss: loss between mel-spectrogram and encoder outputs.\n",
    "            3. diffusion loss: loss between gaussian noise and its reconstruction by diffusion-based decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): batch of texts, converted to a tensor with phoneme embedding ids.\n",
    "            x_lengths (torch.Tensor): lengths of texts in batch.\n",
    "            y (torch.Tensor): batch of corresponding mel-spectrograms.\n",
    "            y_lengths (torch.Tensor): lengths of mel-spectrograms in batch.\n",
    "            out_size (int, optional): length (in mel's sampling rate) of segment to cut, on which decoder will be trained.\n",
    "                Should be divisible by 2^{num of UNet downsamplings}. Needed to increase batch size.\n",
    "        \"\"\"\n",
    "        x, x_lengths, y, y_lengths = self.relocate_input([x, x_lengths, y, y_lengths])\n",
    "\n",
    "        if self.n_spks > 1:\n",
    "            # Get speaker embedding\n",
    "            spk = self.spk_emb(spk)\n",
    "\n",
    "        # Get encoder_outputs `mu_x` and log-scaled token durations `logw`\n",
    "        mu_x, logw, x_mask = self.encoder(x, x_lengths, spk)\n",
    "        y_max_length = y.shape[-1]\n",
    "\n",
    "        y_mask = sequence_mask(y_lengths, y_max_length).unsqueeze(1).to(x_mask)\n",
    "        attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)\n",
    "\n",
    "        # Use MAS to find most likely alignment `attn` between text and mel-spectrogram\n",
    "        with torch.no_grad():\n",
    "            const = -0.5 * math.log(2 * math.pi) * self.n_feats\n",
    "            factor = -0.5 * torch.ones(mu_x.shape, dtype=mu_x.dtype, device=mu_x.device)\n",
    "            y_square = torch.matmul(factor.transpose(1, 2), y**2)\n",
    "            y_mu_double = torch.matmul(2.0 * (factor * mu_x).transpose(1, 2), y)\n",
    "            mu_square = torch.sum(factor * (mu_x**2), 1).unsqueeze(-1)\n",
    "            log_prior = y_square - y_mu_double + mu_square + const\n",
    "            attn = monotonic_align.maximum_path_gradtts(log_prior, attn_mask.squeeze(1))\n",
    "            attn = attn.detach()\n",
    "\n",
    "        # Compute loss between predicted log-scaled durations and those obtained from MAS\n",
    "        logw_ = torch.log(1e-8 + torch.sum(attn.unsqueeze(1), -1)) * x_mask\n",
    "        dur_loss = duration_loss(logw, logw_, x_lengths)\n",
    "\n",
    "        # Cut a small segment of mel-spectrogram in order to increase batch size\n",
    "        if not isinstance(out_size, type(None)):\n",
    "            max_offset = (y_lengths - out_size).clamp(0)\n",
    "            offset_ranges = list(\n",
    "                zip([0] * max_offset.shape[0], max_offset.cpu().numpy())\n",
    "            )\n",
    "            out_offset = torch.LongTensor(\n",
    "                [\n",
    "                    torch.tensor(random.choice(range(start, end)) if end > start else 0)\n",
    "                    for start, end in offset_ranges\n",
    "                ]\n",
    "            ).to(y_lengths)\n",
    "\n",
    "            attn_cut = torch.zeros(\n",
    "                attn.shape[0],\n",
    "                attn.shape[1],\n",
    "                out_size,\n",
    "                dtype=attn.dtype,\n",
    "                device=attn.device,\n",
    "            )\n",
    "            y_cut = torch.zeros(\n",
    "                y.shape[0], self.n_feats, out_size, dtype=y.dtype, device=y.device\n",
    "            )\n",
    "            y_cut_lengths = []\n",
    "            for i, (y_, out_offset_) in enumerate(zip(y, out_offset)):\n",
    "                y_cut_length = out_size + (y_lengths[i] - out_size).clamp(None, 0)\n",
    "                y_cut_lengths.append(y_cut_length)\n",
    "                cut_lower, cut_upper = out_offset_, out_offset_ + y_cut_length\n",
    "                y_cut[i, :, :y_cut_length] = y_[:, cut_lower:cut_upper]\n",
    "                attn_cut[i, :, :y_cut_length] = attn[i, :, cut_lower:cut_upper]\n",
    "            y_cut_lengths = torch.LongTensor(y_cut_lengths)\n",
    "            y_cut_mask = sequence_mask(y_cut_lengths).unsqueeze(1).to(y_mask)\n",
    "\n",
    "            attn = attn_cut\n",
    "            y = y_cut\n",
    "            y_mask = F.pad(\n",
    "                input=y_cut_mask,\n",
    "                pad=(0, out_size - y_cut_mask.shape[2], 0, 0),\n",
    "                mode=\"constant\",\n",
    "                value=0,\n",
    "            )\n",
    "\n",
    "        # Align encoded text with mel-spectrogram and get mu_y segment\n",
    "        mu_y = torch.matmul(attn.squeeze(1).transpose(1, 2), mu_x.transpose(1, 2))\n",
    "        mu_y = mu_y.transpose(1, 2)\n",
    "\n",
    "        # Compute loss of score-based decoder\n",
    "        diff_loss, xt = self.decoder.compute_loss(y, y_mask, mu_y, spk)\n",
    "\n",
    "        # Compute loss between aligned encoder outputs and mel-spectrogram\n",
    "        prior_loss = torch.sum(0.5 * ((y - mu_y) ** 2 + math.log(2 * math.pi)) * y_mask)\n",
    "        prior_loss = prior_loss / (torch.sum(y_mask) * self.n_feats)\n",
    "\n",
    "        return dur_loss, prior_loss, diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEFAULTS = HParams(\n",
    "    cudnn_enabled=True,\n",
    "    log_dir=\"output\",\n",
    "    symbol_set=\"gradtts\",\n",
    "    intersperse_text=True,\n",
    "    n_spks=1,\n",
    "    spk_emb_dim=64,\n",
    "    sampling_rate=22050,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    n_enc_channels=192,\n",
    "    filter_channels=768,\n",
    "    filter_channels_dp=256,\n",
    "    n_enc_layers=6,\n",
    "    enc_kernel=3,\n",
    "    enc_dropout=0.1,\n",
    "    n_heads=2,\n",
    "    window_size=4,\n",
    "    dec_dim=64,\n",
    "    beta_min=0.05,\n",
    "    beta_max=20.0,\n",
    "    pe_scale=1000,\n",
    "    test_size=2,\n",
    "    n_epochs=10000,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    seed=37,\n",
    "    out_size=2 * 22050 // 256,\n",
    "    filter_length=1024,\n",
    "    rank=0,\n",
    "    distributed_run=False,\n",
    "    oversample_weights=None,\n",
    "    text_cleaners=[\"english_cleaners\"],\n",
    "    max_wav_value=32768.0,\n",
    "    n_feats=80,\n",
    "    mel_fmax=8000,\n",
    "    mel_fmin=0.0,\n",
    "    checkpoint=None,\n",
    "    log_interval=100,\n",
    "    save_every=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "checkpoint = \"../models/grad_1750.pt\"\n",
    "model = GradTTS(DEFAULTS)\n",
    "model.load_state_dict(torch.load(checkpoint))\n",
    "model = model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d858bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "from uberduck_ml_dev.vocoders.hifigan import HiFiGanGenerator\n",
    "\n",
    "hifigan = HiFiGanGenerator(\n",
    "    config=\"../models/hifigan-config.json\",\n",
    "    checkpoint=\"../models/gen_02640000_studio\",\n",
    "    cudnn_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "y_enc, y_dec, attn = model.infer(\n",
    "    \"The Fitness Gram Pacer Test is a multistage aerobic capacity test that progressively gets more difficult as it continues.\",\n",
    "    n_timesteps=100,\n",
    ")\n",
    "audio = hifigan.infer(y_dec)\n",
    "import IPython.display as ipd\n",
    "\n",
    "# ipd.Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "y_enc, y_dec, attn = model.infer(\n",
    "    \"The fitness gram pace test is a multi stage aerobic capacity test that { P R AH0 G R EH1 S IH0 V L IY0 } gets more difficult as it continues.\",\n",
    "    n_timesteps=50,\n",
    ")\n",
    "audio = hifigan.infer(y_dec)\n",
    "audio = audio / np.max(audio)\n",
    "# ipd.Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ef4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip\n",
    "y_enc, y_dec, attn = model.infer(\n",
    "    \"Do you say progressively or do you say { P R AH0 G R EH1 S IH0 V L IY0 }.\",\n",
    "    n_timesteps=50,\n",
    ")\n",
    "audio = hifigan.infer(y_dec)\n",
    "audio = audio / np.max(audio)\n",
    "# ipd.Audio(audio, rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c101c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
